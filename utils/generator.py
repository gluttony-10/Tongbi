"""
ç”Ÿæˆå™¨æ¨¡å—

åŒ…å«æ‰€æœ‰å›¾åƒç”Ÿæˆç›¸å…³çš„å‡½æ•°
"""

import gc
import io
import json
import time
import random
import base64
import datetime
import numpy as np
import torch
import requests
from io import BytesIO
from PIL import Image
from diffusers.utils import load_image
from diffusers.image_processor import VaeImageProcessor
import gradio as gr

import utils.state as state
from utils.model_loader import load_model
from utils.cache_utils import get_cached_prompt_embeds
from utils.image_utils import create_pnginfo, calculate_dimensions, upload_image_to_smms
from utils.config_utils import save_tab_model
from utils.camera_utils import build_camera_prompt


def modelscope_generate(
    mode,
    prompt,
    negative_prompt,
    width,
    height,
    batch_images, 
    seed_param,
    transformer_dropdown,
    image=None,
    image_urls=None,  # æ–°å¢ï¼šæ”¯æŒç›´æ¥ä¼ å…¥å›¾ç‰‡URLåˆ—è¡¨
):
    """ModelScope API äº‘ç«¯ç”Ÿæˆ"""
    num_inference_steps = 50  
    true_cfg_scale = 4.0 
    results = []
    inference_times = []  # è®°å½•æ¯å¼ å›¾çš„ç”Ÿæˆæ—¶é—´
    total_start_time = time.time()  # è®°å½•æ€»å¼€å§‹æ—¶é—´
    
    # æ˜¾ç¤ºModelScope APIç”Ÿæˆæç¤ºä¿¡æ¯
    mode_name_map = {
        "t2i_ms": "æ–‡ç”Ÿå›¾",
        "edit_ms": "å¤šå›¾ç¼–è¾‘" if image_urls else "å›¾ç”Ÿå›¾/ç¼–è¾‘"
    }
    mode_name = mode_name_map.get(mode, "ç”Ÿæˆ")
    # æ ¹æ®é€‰æ‹©çš„æ¨¡å‹æ˜¾ç¤ºå¯¹åº”çš„APIæ¨¡å‹åç§°
    if mode == "t2i_ms":
        if "MS-Z-Image-Turbo" in transformer_dropdown:
            api_model_name = "Tongyi-MAI/Z-Image-Turbo"
        elif "MS-Qwen-Image-2512" in transformer_dropdown:
            api_model_name = "Qwen/Qwen-Image-2512"
        else:
            api_model_name = "Qwen/Qwen-Image"
    else:
        # æ ¹æ®é€‰æ‹©çš„æ¨¡å‹æ˜¾ç¤ºå¯¹åº”çš„APIæ¨¡å‹åç§°
        if "MS-Qwen-Image-Edit-2511" in transformer_dropdown:
            api_model_name = "Qwen/Qwen-Image-Edit-2511"
        else:
            api_model_name = "Qwen/Qwen-Image-Edit-2509"  # é»˜è®¤ä½¿ç”¨2509
    
    if image_urls:
        msg = f"ğŸŒ ä½¿ç”¨ModelScope APIäº‘ç«¯ç”Ÿæˆ ({mode_name}) | ğŸ¤– æ¨¡å‹: {api_model_name} | ğŸ“ æç¤ºè¯: {prompt[:50]}{'...' if len(prompt) > 50 else ''} | ğŸ–¼ï¸ å›¾ç‰‡æ•°é‡: {len(image_urls)}å¼ , æ‰¹é‡: {batch_images}å¼ "
    else:
        msg = f"ğŸŒ ä½¿ç”¨ModelScope APIäº‘ç«¯ç”Ÿæˆ ({mode_name}) | ğŸ¤– æ¨¡å‹: {api_model_name} | ğŸ“ æç¤ºè¯: {prompt[:50]}{'...' if len(prompt) > 50 else ''} | ğŸ“Š åˆ†è¾¨ç‡: {width}x{height}, æ‰¹é‡: {batch_images}å¼ "
    print(msg)
    yield results, msg
    
    resolutions = [
        (928, 1664),
        (1104, 1472),
        (1328, 1328),
        (1472, 1104),
        (1664, 928)
    ]
    # å¦‚æœä½¿ç”¨image_urlsï¼ˆå¤šå›¾ç¼–è¾‘ï¼‰ï¼Œä¸éœ€è¦å¤„ç†base64ç¼–ç 
    if image_urls:
        # å¤šå›¾ç¼–è¾‘æ¨¡å¼ï¼Œä½¿ç”¨URLï¼Œä¸éœ€è¦åˆ†è¾¨ç‡å¤„ç†
        pass
    elif image:
        pil_img = image.convert("RGB")
        format = (pil_img.format or "PNG").upper()
        buffer = io.BytesIO()
        pil_img.save(buffer, format=format)
        byte_data = buffer.getvalue()
        mime_type = f"image/{format.lower()}"
        encoded_string = base64.b64encode(byte_data).decode("utf-8")
        width, height = load_image(pil_img).size
        min_distance = float('inf') # åˆå§‹åŒ–æœ€å°è·ç¦»ä¸ºæ­£æ— ç©·å¤§
        for res_width, res_height in resolutions:
            # ä½¿ç”¨æ¬§å‡ é‡Œå¾—è·ç¦»è®¡ç®—ç›¸ä¼¼åº¦
            distance = ((width - res_width) ** 2 + (height - res_height) ** 2) ** 0.5
            if distance < min_distance:
                min_distance = distance
                closest_resolution = (res_width, res_height)
        width, height = closest_resolution[0], closest_resolution[1]
    if seed_param < 0:
        seed = random.randint(0, np.iinfo(np.int32).max)
    else:
        seed = seed_param
    base_url = 'https://api-inference.modelscope.cn/'
    common_headers = {
        "Authorization": f"Bearer {state.modelscope_api_key}",
        "Content-Type": "application/json",
    }
    # ç¦ç”¨ä»£ç†ï¼Œé¿å…ä»£ç†è¿æ¥é—®é¢˜
    proxies = {'http': None, 'https': None}
    
    # ç¬¬ä¸€æ­¥ï¼šä¸€æ¬¡æ€§æäº¤æ‰€æœ‰ä»»åŠ¡
    task_ids = []
    task_info = []  # å­˜å‚¨æ¯ä¸ªä»»åŠ¡çš„ä¿¡æ¯ï¼ˆç´¢å¼•ã€ç§å­ã€å¼€å§‹æ—¶é—´ç­‰ï¼‰
    
    # ç¡®å®šAPIæ¨¡å‹IDï¼ˆåªè®¡ç®—ä¸€æ¬¡ï¼‰
    if mode == "t2i_ms":
        if "MS-Z-Image-Turbo" in transformer_dropdown:
            api_model_id = "Tongyi-MAI/Z-Image-Turbo"
        elif "MS-Qwen-Image-2512" in transformer_dropdown:
            api_model_id = "Qwen/Qwen-Image-2512"
        else:
            api_model_id = "Qwen/Qwen-Image"  # é»˜è®¤ä½¿ç”¨MS-Qwen-Image
    else:
        # æ ¹æ®é€‰æ‹©çš„æ¨¡å‹ç¡®å®šAPIæ¨¡å‹ID
        if "MS-Qwen-Image-Edit-2511" in transformer_dropdown:
            api_model_id = "Qwen/Qwen-Image-Edit-2511"
        else:
            api_model_id = "Qwen/Qwen-Image-Edit-2509"  # é»˜è®¤ä½¿ç”¨2509
    
    # æäº¤æ‰€æœ‰ä»»åŠ¡
    for i in range(batch_images):
        if state.stop_generation:
            state.stop_generation = False
            msg = f"âœ… ç”Ÿæˆå·²ä¸­æ­¢ï¼Œå·²æäº¤{len(task_ids)}ä¸ªä»»åŠ¡"
            print(msg)
            yield results, msg
            break
        
        img_start_time = time.time()
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"outputs/{timestamp}.{state.image_format}"
        
        try:
            if mode == "t2i_ms":
                response = requests.post(
                    f"{base_url}v1/images/generations",
                    headers={**common_headers, "X-ModelScope-Async-Mode": "true"},
                    proxies=proxies,
                    data=json.dumps({
                        "model": api_model_id,
                        "prompt": prompt,
                        "negative_prompt": negative_prompt,
                        "num_inference_steps": num_inference_steps,
                        "true_cfg_scale": true_cfg_scale,
                        "size": f"{width}x{height}",
                        "seed": seed + i,
                    }, ensure_ascii=False).encode('utf-8')
                )
            elif mode == "edit_ms":
                if image_urls and len(image_urls) > 0:
                    response = requests.post(
                        f"{base_url}v1/images/generations",
                        headers={**common_headers, "X-ModelScope-Async-Mode": "true"},
                        proxies=proxies,
                        data=json.dumps({
                            "model": api_model_id,
                            "prompt": prompt,
                            "image_url": image_urls,
                        }, ensure_ascii=False).encode('utf-8')
                    )
                else:
                    response = requests.post(
                        f"{base_url}v1/images/generations",
                        headers={**common_headers, "X-ModelScope-Async-Mode": "true"},
                        data=json.dumps({
                            "model": api_model_id,
                            "image": f"data:{mime_type};base64,{encoded_string}",
                            "prompt": prompt,
                            "negative_prompt": negative_prompt,
                            "num_inference_steps": num_inference_steps,
                            "true_cfg_scale": true_cfg_scale,
                            "size": f"{width}x{height}",
                            "seed": seed + i,
                        }, ensure_ascii=False).encode('utf-8')
                    )
            
            response.raise_for_status()
            task_id = response.json()["task_id"]
            task_ids.append(task_id)
            task_info.append({
                "index": i,
                "seed": seed + i,
                "start_time": img_start_time,
                "filename": filename,
                "task_id": task_id
            })
            
            msg = f"âœ… ç¬¬{i+1}/{batch_images}å¼ ä»»åŠ¡å·²æäº¤ (ä»»åŠ¡ID: {task_id[:8]}...)"
            print(msg)
            yield results, msg
        except Exception as e:
            error_msg = f"âŒ ç¬¬{i+1}/{batch_images}å¼ ä»»åŠ¡æäº¤å¤±è´¥: {str(e)}"
            print(error_msg)
            yield results, error_msg
    
    if not task_ids:
        msg = "âŒ æ²¡æœ‰æˆåŠŸæäº¤ä»»ä½•ä»»åŠ¡"
        print(msg)
        yield results, msg
        return
    
    # æ˜¾ç¤ºæ‰€æœ‰ä»»åŠ¡å·²æäº¤
    msg = f"âœ… æ‰€æœ‰{len(task_ids)}ä¸ªä»»åŠ¡å·²æäº¤å®Œæˆ | â³ æ­£åœ¨ç­‰å¾…äº‘ç«¯ç”Ÿæˆ..."
    print(msg)
    yield results, msg
    
    # ç¬¬äºŒæ­¥ï¼šå¹¶è¡Œè½®è¯¢æ‰€æœ‰ä»»åŠ¡çŠ¶æ€
    completed_tasks = {}  # {task_id: image}
    task_status_map = {}  # {task_id: status}
    task_start_time = {}  # {task_id: start_time} è®°å½•æ¯ä¸ªä»»åŠ¡çš„å¼€å§‹æ—¶é—´
    last_status_update = time.time()
    
    while len(completed_tasks) < len(task_ids):
        if state.stop_generation:
            state.stop_generation = False
            msg = f"âœ… ç”Ÿæˆå·²ä¸­æ­¢ï¼Œå·²å®Œæˆ{len(completed_tasks)}/{len(task_ids)}ä¸ªä»»åŠ¡"
            print(msg)
            yield results, msg
            break
        
        # è½®è¯¢æ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡
        for task_info_item in task_info:
            task_id = task_info_item["task_id"]
            if task_id in completed_tasks:
                continue
            
            try:
                result = requests.get(
                    f"{base_url}v1/tasks/{task_id}",
                    headers={**common_headers, "X-ModelScope-Task-Type": "image_generation"},
                    proxies=proxies,
                )
                result.raise_for_status()
                data = result.json()
                task_status = data.get("task_status", "UNKNOWN")
                task_status_map[task_id] = task_status
                
                # è®°å½•ä»»åŠ¡å¼€å§‹æ—¶é—´ï¼ˆç¬¬ä¸€æ¬¡è½®è¯¢æ—¶ï¼‰
                if task_id not in task_start_time:
                    task_start_time[task_id] = time.time()
                
                if task_status == "SUCCEED":
                    image_response = requests.get(data["output_images"][0], proxies=proxies)
                    image = Image.open(BytesIO(image_response.content))
                    completed_tasks[task_id] = image
                    
                    # å¤„ç†å®Œæˆçš„å›¾ç‰‡
                    i = task_info_item["index"]
                    img_time = time.time() - task_info_item["start_time"]
                    inference_times.append(img_time)
                    
                    # åˆ›å»º PNG å…ƒæ•°æ®
                    pnginfo = create_pnginfo(
                        mode=mode,
                        prompt=prompt,
                        negative_prompt=negative_prompt,
                        seed=task_info_item["seed"],
                        transformer_dropdown=transformer_dropdown,
                        num_inference_steps=num_inference_steps,
                        true_cfg_scale=true_cfg_scale,
                        width=width,
                        height=height,
                        image=image if mode == "edit_ms" else None,
                        generation_time=img_time
                    )
                    
                    # æ ¹æ®æ ¼å¼é€‰æ‹©ä¿å­˜æ–¹å¼
                    if state.image_format == "png":
                        image.save(task_info_item["filename"], pnginfo=pnginfo)
                    else:
                        # JPGå’ŒWEBPä¸æ”¯æŒpnginfoï¼Œéœ€è¦å•ç‹¬ä¿å­˜å…ƒæ•°æ®
                        image.save(task_info_item["filename"], format=state.image_format.upper())
                    results.append((task_info_item["filename"], None))
                    
                    msg = f"âœ… ç¬¬{i+1}/{batch_images}å¼ å®Œæˆ (ModelScope API) | ğŸŒ± ç§å­: {task_info_item['seed']}, â±ï¸ è€—æ—¶: {img_time:.2f}ç§’ | ğŸ’¾ ä¿å­˜è‡³: {task_info_item['filename']}"
                    print(msg)
                    yield results, msg
                    
                elif task_status == "FAILED":
                    i = task_info_item["index"]
                    error_msg = f"âŒ ç¬¬{i+1}/{batch_images}å¼ ç”Ÿæˆå¤±è´¥"
                    print(error_msg)
                    yield results, error_msg
                    completed_tasks[task_id] = None  # æ ‡è®°ä¸ºå·²å®Œæˆï¼ˆå¤±è´¥ï¼‰
            except Exception as e:
                i = task_info_item["index"]
                error_msg = f"âŒ ç¬¬{i+1}/{batch_images}å¼ è½®è¯¢å¤±è´¥: {str(e)}"
                print(error_msg)
                yield results, error_msg
        
        # æ¯20ç§’æ›´æ–°ä¸€æ¬¡çŠ¶æ€æ˜¾ç¤º
        current_time = time.time()
        if current_time - last_status_update >= 20:
            pending_count = len(task_ids) - len(completed_tasks)
            status_summary = []
            for task_info_item in task_info:
                task_id = task_info_item["task_id"]
                if task_id not in completed_tasks:
                    status = task_status_map.get(task_id, "UNKNOWN")
                    elapsed_time = int(current_time - task_start_time.get(task_id, current_time))
                    status_summary.append(f"ç¬¬{task_info_item['index']+1}å¼ : {status}({elapsed_time}ç§’)")
            
            if status_summary:
                msg = f"â³ ç­‰å¾…ä¸­: {pending_count}ä¸ªä»»åŠ¡æœªå®Œæˆ | " + " | ".join(status_summary[:5])  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                if len(status_summary) > 5:
                    msg += f" | ...è¿˜æœ‰{len(status_summary)-5}ä¸ªä»»åŠ¡"
                print(msg)
                yield results, msg
            last_status_update = current_time
        
        time.sleep(5)  # 5ç§’é—´éš”è½®è¯¢
    
    # ç”Ÿæˆå®Œæˆåæ˜¾ç¤ºæ€»ç»“ä¿¡æ¯
    if results:
        total_time = time.time() - total_start_time
        avg_time = total_time / len(results) if results else 0
        msg = f"ğŸ‰ ModelScope APIç”Ÿæˆå…¨éƒ¨å®Œæˆï¼ | ğŸ“Š å…±{len(results)}å¼ ï¼Œæ€»è€—æ—¶{total_time:.2f}ç§’ï¼Œå¹³å‡{avg_time:.2f}ç§’/å¼ "
        print(msg)
        yield results, msg


def _generate_common(
    mode, 
    prompt, 
    negative_prompt, 
    width, 
    height, 
    num_inference_steps, 
    batch_images, 
    true_cfg_scale, 
    seed_param, 
    transformer_dropdown, 
    lora_dropdown, 
    lora_weights, 
    image=None, 
    mask_image=None, 
    strength=None,
    size_edit2=None, 
    reserve_edit2=None,
):
    """é€šç”¨ç”Ÿæˆå‡½æ•°ï¼ˆæœ¬åœ°æ¨¡å‹ï¼‰ã€‚æ˜¾å­˜ç­‰è®¾ç½®ä»…ä½¿ç”¨å·²ä¿å­˜çš„ stateï¼Œä¸ä½¿ç”¨æœªä¿å­˜çš„ UI å€¼ã€‚"""
    results = []
    inference_times = []  # è®°å½•æ¯å¼ å›¾çš„ç”Ÿæˆæ—¶é—´
    total_start_time = time.time()  # è®°å½•æ€»å¼€å§‹æ—¶é—´
    
    if seed_param < 0:
        seed = random.randint(0, np.iinfo(np.int32).max)
    else:
        seed = seed_param
    if mode in ["editplus"]:
        CONDITION_IMAGE_SIZE = 384 * 384
        VAE_IMAGE_SIZE = 1024 * 1024
        image_processor = VaeImageProcessor(vae_scale_factor=16)
        if not isinstance(image, list):
            image = [image]
        calculated_images = []
        condition_images = []
        for img in image:
            image_width, image_height = img.size
            condition_width, condition_height = calculate_dimensions(CONDITION_IMAGE_SIZE, image_width / image_height)
            vae_width, vae_height = calculate_dimensions(VAE_IMAGE_SIZE, image_width / image_height)
            calculated_height = vae_height // 32 * 32
            calculated_width = vae_width // 32 * 32
            calculated_images.append(image_processor.resize(img, calculated_height, calculated_width))
            condition_images.append(image_processor.resize(img, condition_height, condition_width))
    if (state.mode_loaded != mode or state.prompt_cache != prompt or state.negative_prompt_cache != negative_prompt or 
        state.transformer_loaded != transformer_dropdown or state.lora_loaded != lora_dropdown or
          state.lora_loaded_weights != lora_weights or state.image_loaded!=image):
        load_model(mode, transformer_dropdown, lora_dropdown, lora_weights, state.res_vram)
        state.prompt_cache, state.negative_prompt_cache, state.image_loaded = prompt, negative_prompt, image
    # å§‹ç»ˆç¼–ç æç¤ºè¯ï¼ˆç¼“å­˜å‘½ä¸­æ—¶ä¹Ÿéœ€æœ‰ prompt_embeds ä¾› pipe ä½¿ç”¨ï¼‰
    if mode in ["editplus"]:
        prompt_embeds, prompt_embeds_mask, negative_prompt_embeds, negative_prompt_embeds_mask = get_cached_prompt_embeds(
            mode, prompt, negative_prompt, true_cfg_scale, condition_images=condition_images
        )
    else:
        prompt_embeds, prompt_embeds_mask, negative_prompt_embeds, negative_prompt_embeds_mask = get_cached_prompt_embeds(
            mode, prompt, negative_prompt, true_cfg_scale, image=image
        )
    for i in range(batch_images):
        if state.stop_generation:
            state.stop_generation = False
            msg = f"âœ… ç”Ÿæˆå·²ä¸­æ­¢ï¼Œæœ€åç§å­æ•°{seed+i-1}"
            print(msg)
            yield results, msg
            break
        
        # è®°å½•å•å¼ å›¾ç”Ÿæˆå¼€å§‹æ—¶é—´
        img_start_time = time.time()
        
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"outputs/{timestamp}.{state.image_format}"
        
        with torch.no_grad():
            if mode == "t2i":
                output = state.pipe(
                    width=width,
                    height=height,
                    num_inference_steps=num_inference_steps,
                    true_cfg_scale=true_cfg_scale,
                    prompt_embeds=prompt_embeds,
                    prompt_embeds_mask=prompt_embeds_mask,
                    negative_prompt_embeds=negative_prompt_embeds if true_cfg_scale > 1 else None,
                    negative_prompt_embeds_mask=negative_prompt_embeds_mask if true_cfg_scale > 1 else None,
                    generator=torch.Generator().manual_seed(seed + i),
                )
            elif mode == "i2i":
                output = state.pipe(
                    image=image,
                    width=width,
                    height=height,
                    num_inference_steps=num_inference_steps,
                    strength=strength,
                    true_cfg_scale=true_cfg_scale,
                    prompt_embeds=prompt_embeds,
                    prompt_embeds_mask=prompt_embeds_mask,
                    negative_prompt_embeds=negative_prompt_embeds if true_cfg_scale > 1 else None,
                    negative_prompt_embeds_mask=negative_prompt_embeds_mask if true_cfg_scale > 1 else None,
                    generator=torch.Generator().manual_seed(seed + i),
                )
            elif mode == "inp":
                output = state.pipe(
                    image=image,
                    mask_image=mask_image,
                    width=width,
                    height=height,
                    num_inference_steps=num_inference_steps,
                    strength=strength,
                    true_cfg_scale=true_cfg_scale,
                    prompt_embeds=prompt_embeds,
                    prompt_embeds_mask=prompt_embeds_mask,
                    negative_prompt_embeds=negative_prompt_embeds if true_cfg_scale > 1 else None,
                    negative_prompt_embeds_mask=negative_prompt_embeds_mask if true_cfg_scale > 1 else None,
                    generator=torch.Generator().manual_seed(seed + i),
                )
            elif mode == "editplus":
                output = state.pipe(
                    image=calculated_images,
                    width=width,
                    height=height,
                    num_inference_steps=num_inference_steps,
                    true_cfg_scale=true_cfg_scale,
                    prompt_embeds=prompt_embeds,
                    prompt_embeds_mask=prompt_embeds_mask,
                    negative_prompt_embeds=negative_prompt_embeds if true_cfg_scale > 1 else None,
                    negative_prompt_embeds_mask=negative_prompt_embeds_mask if true_cfg_scale > 1 else None,
                    generator=torch.Generator().manual_seed(seed + i),
                )
        
        # ç”Ÿæˆå®Œæˆï¼Œä¿å­˜å›¾åƒ
        image = output.images[0]
        
        # è®¡ç®—å•å¼ å›¾ç”Ÿæˆæ—¶é—´
        img_time = time.time() - img_start_time
        inference_times.append(img_time)
        
        # åˆ›å»º PNG å…ƒæ•°æ®
        pnginfo = create_pnginfo(
            mode=mode,
            prompt=prompt,
            negative_prompt=negative_prompt,
            seed=seed + i,
            transformer_dropdown=transformer_dropdown,
            num_inference_steps=num_inference_steps,
            true_cfg_scale=true_cfg_scale,
            width=width,
            height=height,
            strength=strength,
            lora_dropdown=lora_dropdown,
            lora_weights=lora_weights,
            image=image if mode in ["i2i", "inp", "editplus"] else None,
            generation_time=img_time
        )
        
        # æ ¹æ®æ ¼å¼é€‰æ‹©ä¿å­˜æ–¹å¼
        if state.image_format == "png":
            image.save(filename, pnginfo=pnginfo)
        else:
            # JPGå’ŒWEBPä¸æ”¯æŒpnginfoï¼Œéœ€è¦å•ç‹¬ä¿å­˜å…ƒæ•°æ®
            image.save(filename, format=state.image_format.upper())
        results.append((filename, None))
        
        # æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯ï¼šç§å­ã€ä¿å­˜è·¯å¾„ã€è€—æ—¶
        msg = f"âœ… ç¬¬{i+1}/{batch_images}å¼ å®Œæˆï¼Œç§å­{seed+i}ï¼Œè€—æ—¶{img_time:.2f}ç§’ | ä¿å­˜è‡³: {filename}"
        print(msg)
        yield results, msg
        
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
    
    # ç”Ÿæˆå®Œæˆåæ˜¾ç¤ºæ€»ç»“ä¿¡æ¯
    if results:
        total_time = time.time() - total_start_time
        avg_time = total_time / len(results) if results else 0
        msg = f"ğŸ‰ å…¨éƒ¨å®Œæˆï¼å…±{len(results)}å¼ ï¼Œæ€»è€—æ—¶{total_time:.2f}ç§’ï¼Œå¹³å‡{avg_time:.2f}ç§’/å¼ "
        print(msg)
        yield results, msg


def generate_t2i(prompt, negative_prompt, width, height, num_inference_steps, 
                 batch_images, true_cfg_scale, seed_param, transformer_dropdown, 
                 lora_dropdown, lora_weights):
    """æ–‡ç”Ÿå›¾ç”Ÿæˆå‡½æ•°ã€‚æ˜¾å­˜/å›¾ç‰‡æ ¼å¼ç­‰ä»…ä½¿ç”¨å·²ä¿å­˜çš„è®¾ç½®ã€‚"""
    # ä¿å­˜å½“å‰TabItemçš„æ¨¡å‹é€‰æ‹©
    save_tab_model("t2i", transformer_dropdown)
    if "MS-Qwen-Image" in transformer_dropdown or "MS-Qwen-Image-2512" in transformer_dropdown or "MS-Z-Image-Turbo" in transformer_dropdown or "ModelScope" in transformer_dropdown:
        yield from modelscope_generate(
            mode="t2i_ms",
            prompt=prompt,
            negative_prompt=negative_prompt,
            width=width,
            height=height,
            batch_images=batch_images, 
            seed_param=seed_param,
            transformer_dropdown=transformer_dropdown,
        )
    else:
        yield from _generate_common(
            mode="t2i",
            prompt=prompt,
            negative_prompt=negative_prompt,
            width=width,
            height=height,
            num_inference_steps=num_inference_steps,
            batch_images=batch_images,
            true_cfg_scale=true_cfg_scale,
            seed_param=seed_param,
            transformer_dropdown=transformer_dropdown,
            lora_dropdown=lora_dropdown,
            lora_weights=lora_weights,
        )


def generate_i2i(image, prompt, negative_prompt, width, height, num_inference_steps,
                 strength, batch_images, true_cfg_scale, seed_param, transformer_dropdown, 
                 lora_dropdown, lora_weights):
    """å›¾ç”Ÿå›¾ç”Ÿæˆå‡½æ•°ã€‚æ˜¾å­˜/å›¾ç‰‡æ ¼å¼ç­‰ä»…ä½¿ç”¨å·²ä¿å­˜çš„è®¾ç½®ã€‚"""
    # ä¿å­˜å½“å‰TabItemçš„æ¨¡å‹é€‰æ‹©
    save_tab_model("i2i", transformer_dropdown)
    # MS-Qwen-Imageåªåœ¨æ–‡ç”Ÿå›¾ä¸­å¯ç”¨ï¼Œå›¾ç”Ÿå›¾ä¸æ”¯æŒ
    image = load_image(image)
    yield from _generate_common(
        mode="i2i",
        image=image,
        prompt=prompt,
        negative_prompt=negative_prompt,
        width=width,
        height=height,
        num_inference_steps=num_inference_steps,
        strength=strength,
        batch_images=batch_images,
        true_cfg_scale=true_cfg_scale,
        seed_param=seed_param,
        transformer_dropdown=transformer_dropdown,
        lora_dropdown=lora_dropdown,
        lora_weights=lora_weights,
    )


def generate_inp(image, prompt, negative_prompt, width, height, num_inference_steps,
                 strength, batch_images, true_cfg_scale, seed_param, transformer_dropdown,
                 lora_dropdown, lora_weights):
    """å±€éƒ¨é‡ç»˜ç”Ÿæˆå‡½æ•°ã€‚æ˜¾å­˜/å›¾ç‰‡æ ¼å¼ç­‰ä»…ä½¿ç”¨å·²ä¿å­˜çš„è®¾ç½®ã€‚"""
    # ä¿å­˜å½“å‰TabItemçš„æ¨¡å‹é€‰æ‹©
    save_tab_model("inp", transformer_dropdown)
    # MS-Qwen-Imageåªåœ¨æ–‡ç”Ÿå›¾ä¸­å¯ç”¨ï¼Œå±€éƒ¨é‡ç»˜ä¸æ”¯æŒ
    # å¤„ç†è’™ç‰ˆå›¾åƒ
    mask_image = image["layers"][0]
    mask_image = mask_image .convert("RGBA")
    data = np.array(mask_image)
    # ä¿®æ”¹è’™ç‰ˆé¢œè‰²ï¼ˆé»‘è‰²->ç™½è‰²ï¼Œé€æ˜->é»‘è‰²ï¼‰
    black_pixels = (data[:, :, 0] == 0) & (data[:, :, 1] == 0) & (data[:, :, 2] == 0)
    data[black_pixels, :3] = [255, 255, 255]
    transparent_pixels = (data[:, :, 3] == 0)
    data[transparent_pixels, :3] = [0, 0, 0]
    mask_image = Image.fromarray(data)
    # æå–èƒŒæ™¯å›¾åƒ
    background_image = load_image(image["background"])
    yield from _generate_common(
        mode="inp",
        image=background_image,
        mask_image=mask_image,
        prompt=prompt,
        negative_prompt=negative_prompt,
        width=width,
        height=height,
        num_inference_steps=num_inference_steps,
        strength=strength,
        batch_images=batch_images,
        true_cfg_scale=true_cfg_scale,
        seed_param=seed_param,
        transformer_dropdown=transformer_dropdown,
        lora_dropdown=lora_dropdown,
        lora_weights=lora_weights,
    )


def generate_editplus2(image_editplus2, image_editplus3, image_editplus4, image_editplus5, prompt, negative_prompt, width, height, num_inference_steps,
                  batch_images, true_cfg_scale, seed_param, transformer_dropdown,
                  lora_dropdown, lora_weights):
    """å¤šå›¾ç¼–è¾‘ç”Ÿæˆå‡½æ•°ã€‚æ˜¾å­˜/å›¾ç‰‡æ ¼å¼ç­‰ä»…ä½¿ç”¨å·²ä¿å­˜çš„è®¾ç½®ã€‚"""
    # ä¿å­˜å½“å‰TabItemçš„æ¨¡å‹é€‰æ‹©
    save_tab_model("editplus", transformer_dropdown)
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ModelScope APIï¼ˆå¤šå›¾ç¼–è¾‘æ”¯æŒModelScopeï¼‰
    if "MS-Qwen-Image-Edit" in transformer_dropdown or "MS-Qwen-Image" in transformer_dropdown or "ModelScope" in transformer_dropdown:
        # æ”¶é›†æ‰€æœ‰éç©ºçš„å›¾ç‰‡
        image_list = [image_editplus2, image_editplus3, image_editplus4, image_editplus5]
        image_list = [img for img in image_list if img is not None]
        
        if len(image_list) == 0:
            yield [], "âŒ è¯·è‡³å°‘ä¸Šä¼ ä¸€å¼ å›¾ç‰‡"
            return
        
        # ä¸Šä¼ å›¾ç‰‡åˆ°SM.MSå›¾åºŠ
        msg = f"ğŸ“¤ æ­£åœ¨ä¸Šä¼ {len(image_list)}å¼ å›¾ç‰‡åˆ°å›¾åºŠ..."
        print(msg)
        yield [], msg
        
        image_urls = []
        for idx, img in enumerate(image_list):
            # è½¬æ¢ä¸ºRGB
            if img.mode != "RGB":
                img = img.convert("RGB")
            
            # ä¸Šä¼ åˆ°SM.MS
            url = upload_image_to_smms(img)
            if url:
                image_urls.append(url)
                print(f"  âœ… ç¬¬{idx+1}å¼ å›¾ç‰‡ä¸Šä¼ æˆåŠŸ: {url[:50]}...")
            else:
                yield [], f"âŒ ç¬¬{idx+1}å¼ å›¾ç‰‡ä¸Šä¼ å¤±è´¥ï¼Œè¯·é‡è¯•"
                return
        
        if len(image_urls) == 0:
            yield [], "âŒ æ‰€æœ‰å›¾ç‰‡ä¸Šä¼ å¤±è´¥ï¼Œè¯·é‡è¯•"
            return
        
        # ä½¿ç”¨ModelScope APIç”Ÿæˆ
        yield from modelscope_generate(
            mode="edit_ms",
            prompt=prompt,
            negative_prompt=negative_prompt,
            width=width,
            height=height,
            batch_images=batch_images, 
            seed_param=seed_param,
            transformer_dropdown=transformer_dropdown,
            image_urls=image_urls,  # ä¼ é€’å›¾ç‰‡URLåˆ—è¡¨
        )
    else:
        # ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆ
        image = [image_editplus2, image_editplus3, image_editplus4, image_editplus5]
        image = [img for img in image if img is not None]
        images = []  # ç”¨äºå­˜å‚¨æ‰€æœ‰å¤„ç†åçš„å›¾ç‰‡
        for img in image:  # éå†å›¾ç‰‡åœ°å€åˆ—è¡¨
            # è½¬æ¢ä¸ºRGBA
            img = img.convert("RGBA")
            # åˆ›å»ºç™½è‰²èƒŒæ™¯
            white_bg = Image.new("RGB", img.size, (255, 255, 255))
            # ä½¿ç”¨alphaé€šé“ä½œä¸ºæ©ç è¿›è¡Œç²˜è´´
            white_bg.paste(img, mask=img.split()[3])
            # è½¬æ¢ä¸ºRGB
            img_rgb = white_bg.convert("RGB")
            # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
            images.append(img_rgb)
        yield from _generate_common(
            mode="editplus",
            image=images,
            prompt=prompt,
            negative_prompt=negative_prompt,
            width=width,
            height=height,
            num_inference_steps=num_inference_steps,
            batch_images=batch_images,
            true_cfg_scale=true_cfg_scale,
            seed_param=seed_param,
            transformer_dropdown=transformer_dropdown, 
            lora_dropdown=lora_dropdown,
            lora_weights=lora_weights,
        )


def generate_camera_edit(image_camera, azimuth, elevation, distance, negative_prompt, width, height, num_inference_steps,
                  batch_images, true_cfg_scale, seed_param, transformer_dropdown,
                  lora_dropdown, lora_weights, additional_prompt=""):
    """
    3Dç›¸æœºæ§åˆ¶ç”Ÿæˆå‡½æ•°ã€‚æ˜¾å­˜/å›¾ç‰‡æ ¼å¼ç­‰ä»…ä½¿ç”¨å·²ä¿å­˜çš„è®¾ç½®ã€‚
    Edit the camera angle of an image using Qwen Image Edit Plus with multi-angles LoRA.
    """
    # ä¿å­˜å½“å‰TabItemçš„æ¨¡å‹é€‰æ‹©
    save_tab_model("camera", transformer_dropdown)
    if image_camera is None:
        raise gr.Error("è¯·å…ˆä¸Šä¼ å›¾ç‰‡")
    
    # Build camera prompt
    camera_prompt = build_camera_prompt(azimuth, elevation, distance)
    
    # Merge additional prompt if provided
    if additional_prompt and additional_prompt.strip():
        camera_prompt = f"{camera_prompt}, {additional_prompt.strip()}"
    
    # Convert image to RGB
    pil_image = image_camera.convert("RGB") if isinstance(image_camera, Image.Image) else Image.open(image_camera).convert("RGB")
    
    # MS-Qwen-Imageåªåœ¨æ–‡ç”Ÿå›¾ä¸­å¯ç”¨ï¼Œ3Dç›¸æœºæ§åˆ¶ä¸æ”¯æŒ
    # Use editplus mode with camera prompt
    yield from _generate_common(
        mode="editplus",
        image=[pil_image],
        prompt=camera_prompt,
        negative_prompt=negative_prompt,
        width=width,
        height=height,
        num_inference_steps=num_inference_steps,
        batch_images=batch_images,
        true_cfg_scale=true_cfg_scale,
        seed_param=seed_param,
        transformer_dropdown=transformer_dropdown, 
        lora_dropdown=lora_dropdown,
        lora_weights=lora_weights,
    )
