"""
æ¨¡å‹åŠ è½½å·¥å…·æ¨¡å—

åŒ…å«æ¨¡å‹åŠ è½½ã€LoRAåŠ è½½ç­‰åŠŸèƒ½
"""

import os
import gc
import time
import math
import torch
import safetensors.torch
from mmgp import offload
from diffusers import QwenImageTransformer2DModel, FlowMatchEulerDiscreteScheduler
from diffusers import QwenImageImg2ImgPipeline, QwenImagePipeline, QwenImageInpaintPipeline, QwenImageEditPlusPipeline
from transformers import Qwen2_5_VLForConditionalGeneration
import utils.state as state
from utils.model_downloader import check_and_download_model
from utils.lora_utils import build_lora_names, load_and_merge_lora_weight, load_and_merge_lora_weight_from_safetensors


def load_model(mode, transformer_dropdown, lora_dropdown, lora_weights, res_vram):
    """
    åŠ è½½å’Œé…ç½®æ¨¡å‹
    
    å‚æ•°:
        mode: ç”Ÿæˆæ¨¡å¼ (t2i, i2i, inp, editplus)
        transformer_dropdown: transformer æ¨¡å‹åç§°
        lora_dropdown: LoRA æ¨¡å‹åˆ—è¡¨
        lora_weights: LoRA æƒé‡å­—ç¬¦ä¸²
        res_vram: ä¿ç•™æ˜¾å­˜å¤§å° (MB)
    """
    # å¯¼å…¥å…¨å±€å˜é‡
    import utils.state as state
    
    # æ¸…ç†å†…å­˜å’Œæ˜¾å­˜
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()
    
    # è®¡ç®—æ˜¾å­˜é¢„ç®—
    res_vram = float(res_vram)
    if torch.cuda.is_available():
        free_memory, _ = torch.cuda.mem_get_info(0)
        budgets = int(free_memory / 1048576 - res_vram)
        print(f"ğŸ’¾ å¯ç”¨æ˜¾å­˜: {free_memory / 1073741824:.2f}GB, åˆ†é…é¢„ç®—: {budgets / 1024:.2f}GB")
    else:
        budgets = 0
    
    # Scheduler é…ç½®
    scheduler_config = {
        "base_image_seq_len": 256,
        "base_shift": math.log(3),
        "invert_sigmas": False,
        "max_image_seq_len": 8192,
        "max_shift": math.log(3),
        "num_train_timesteps": 1000,
        "shift": 1.0,
        "shift_terminal": None,
        "stochastic_sampling": False,
        "time_shift_type": "exponential",
        "use_beta_sigmas": False,
        "use_dynamic_shifting": True,
        "use_exponential_sigmas": False,
        "use_karras_sigmas": False,
    }
    
    # Pipeline ç±»æ˜ å°„
    PIPELINE_CLASSES = {
        "t2i": QwenImagePipeline,
        "i2i": QwenImageImg2ImgPipeline,
        "inp": QwenImageInpaintPipeline,
        "editplus": QwenImageEditPlusPipeline,
    }
    
    # åˆ¤æ–­æ˜¯å¦éœ€è¦é‡æ–°åŠ è½½æ¨¡å‹
    need_reload = (
        state.pipe is None or 
        state.mode_loaded != mode or 
        state.transformer_loaded != transformer_dropdown or 
        state.lora_loaded != lora_dropdown or 
        state.lora_loaded_weights != lora_weights
    )
    
    if not need_reload:
        print("âœ… æ¨¡å‹å·²åŠ è½½ï¼Œæ— éœ€é‡æ–°åŠ è½½")
        return
    
    print(f"ğŸ”„ å¼€å§‹åŠ è½½æ¨¡å‹ [æ¨¡å¼: {mode}, æ¨¡å‹: {transformer_dropdown}]")
    load_start_time = time.time()
    
    try:
        # æ£€æŸ¥å¹¶ä¸‹è½½æ¨¡å‹ï¼ˆå¦‚æœæ˜¯æœ¬åœ°æ¨¡å‹ï¼‰
        if not transformer_dropdown.startswith("MS-"):
            print("ğŸ” æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨...")
            success, msg = check_and_download_model(transformer_dropdown)
            if not success:
                raise ValueError(msg)
            print(msg)
        
        # å¸è½½æ—§æ¨¡å‹
        if state.pipe is not None:
            print("ğŸ—‘ï¸ å¸è½½æ—§æ¨¡å‹...")
            state.pipe.unload_lora_weights()
            state.mmgp.release()
        
        # æ›´æ–°å…¨å±€çŠ¶æ€
        state.mode_loaded, state.transformer_loaded, state.lora_loaded, state.lora_loaded_weights = (
            mode, transformer_dropdown, lora_dropdown, lora_weights
        )
        
        # 1. åŠ è½½ Text Encoder
        print("ğŸ“ åŠ è½½ Text Encoder...")
        text_encoder = offload.fast_load_transformers_model(
            f"{state.model_id}/text_encoder/text_encoder-mmgp.safetensors",
            do_quantize=False,
            modelClass=Qwen2_5_VLForConditionalGeneration,
            forcedConfigPath=f"{state.model_id}/text_encoder/config.json",
        )
        
        # 2. åŠ è½½ Transformer
        print(f"ğŸ¨ åŠ è½½ Transformer: {transformer_dropdown}")
        # å¦‚æœæ¨¡å‹åæ²¡æœ‰ .safetensors åç¼€ï¼Œè‡ªåŠ¨æ·»åŠ ï¼ˆç”¨äºæœ¬åœ°æ¨¡å‹ï¼‰
        if not transformer_dropdown.startswith("MS-") and not transformer_dropdown.endswith(".safetensors"):
            transformer_dropdown_full = f"{transformer_dropdown}.safetensors"
        else:
            transformer_dropdown_full = transformer_dropdown
        
        if "mmgp" not in transformer_dropdown_full:
            raise ValueError("âŒ è¯·ä½¿ç”¨ mmgp è½¬æ¢åä¿å­˜çš„æ¨¡å‹")
        
        transformer = offload.fast_load_transformers_model(
            f"models/transformer/{transformer_dropdown_full}",
            do_quantize=False,
            modelClass=QwenImageTransformer2DModel,
            forcedConfigPath=f"{state.model_id}/transformer/config.json",
        )
        
        # 3. åŠ è½½ Scheduler
        if "Lightning" in transformer_dropdown:
            print("âš¡ ä½¿ç”¨ Lightning Schedulerï¼ˆåŠ é€Ÿç‰ˆï¼‰")
            scheduler = FlowMatchEulerDiscreteScheduler.from_config(scheduler_config)
        else:
            print("ğŸŒŠ ä½¿ç”¨æ ‡å‡† Scheduler")
            scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(state.model_id, subfolder="scheduler")
        
        # 4. åˆå§‹åŒ– Pipeline
        pipeline_class = PIPELINE_CLASSES.get(mode)
        if pipeline_class is None:
            raise ValueError(f"âŒ ä¸æ”¯æŒçš„æ¨¡å¼: {mode}")
        
        print(f"ğŸ”§ åˆå§‹åŒ– Pipeline: {pipeline_class.__name__}")
        
        state.pipe = pipeline_class.from_pretrained(
            state.model_id,
            text_encoder=text_encoder,
            transformer=transformer,
            scheduler=scheduler,
            torch_dtype=state.dtype,
        )
        
        # 5. é…ç½®è¿›åº¦æ¡
        if mode in ["editplus"]:
            state.pipe.set_progress_bar_config(disable=None)
        
        # 6. åŠ è½½ LoRA
        if lora_dropdown:
            print(f"ğŸ¯ åŠ è½½ {len(lora_dropdown)} ä¸ª LoRA æ¨¡å‹...")
        load_lora(lora_dropdown, lora_weights)
        
        # 7. é…ç½® MMGPï¼ˆæ˜¾å­˜ç®¡ç†å’Œé‡åŒ–ï¼‰
        print("âš™ï¸ é…ç½® MMGPï¼ˆæ˜¾å­˜ç®¡ç†ï¼‰...")
        import psutil
        mem = psutil.virtual_memory()
        pinned_models = ["text_encoder", "transformer"] if mem.total/1073741824 > 60 else "transformer"
        state.mmgp = offload.all(
            state.pipe,
            pinnedMemory=pinned_models,
            budgets={'*': budgets},
            extraModelsToQuantize=["text_encoder"],
            compile=True if state.args.compile else False,
        )
        
        # 8. è®¾ç½®æ³¨æ„åŠ›åç«¯
        if state.device == "cuda":
            if torch.cuda.get_device_capability()[0] >= 8:
                state.pipe.transformer.set_attention_backend("flash")
                print("âš¡ ä½¿ç”¨ Flash Attention åŠ é€Ÿ")
            else:
                state.pipe.transformer.set_attention_backend("native")
                print("ğŸ”§ ä½¿ç”¨æ ‡å‡† Attention")
        
        # 9. å¯ç”¨ Channels Last å†…å­˜æ ¼å¼
        if state.device == "cuda" and hasattr(state.pipe, 'transformer'):
            try:
                state.pipe.transformer = state.pipe.transformer.to(memory_format=torch.channels_last)
                print("âœ… Channels Last å†…å­˜æ ¼å¼å·²å¯ç”¨")
            except Exception as e:
                print(f"âš ï¸ Channels Last å¯ç”¨å¤±è´¥: {e}")
        
        # åŠ è½½å®Œæˆ
        load_time = time.time() - load_start_time
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼è€—æ—¶ {load_time:.2f} ç§’")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        raise


def load_lora(lora_dropdown, lora_weights):
    """
    åŠ è½½å’Œé…ç½® LoRA æ¨¡å‹ï¼ˆåŠ è½½å‰è‡ªåŠ¨è½¬æ¢ï¼‰
    
    å‚æ•°:
        lora_dropdown: LoRA æ¨¡å‹åˆ—è¡¨
        lora_weights: LoRA æƒé‡å­—ç¬¦ä¸²ï¼ˆé€—å·åˆ†éš”ï¼‰
    """
    if not lora_dropdown:
        return
    
    import utils.state as state
    
    adapter_names = []
    adapter_weights = []
    
    # è§£ææƒé‡å­—ç¬¦ä¸²
    weights = [float(w) for w in lora_weights.split(',')] if lora_weights else []
    
    # åŠ è½½æ¯ä¸ª LoRA æ¨¡å‹
    for idx, lora_name in enumerate(lora_dropdown):
        try:
            lora_path = f"models/lora/Qwen-Image/{lora_name}"
            
            # åŠ è½½å‰è‡ªåŠ¨è½¬æ¢ï¼ˆå¦‚æœéœ€è¦ï¼‰
            converted_path = _convert_lora_file(lora_path)
            
            # è·å–é€‚é…å™¨åç§°ï¼ˆä½¿ç”¨è½¬æ¢åçš„æ–‡ä»¶åï¼Œä½†å»æ‰ _diffusers åç¼€ï¼‰
            base_name = os.path.splitext(os.path.basename(converted_path))[0]
            if "_diffusers" in base_name:
                adapter_name = base_name.replace("_diffusers", "")
            else:
                adapter_name = base_name
            adapter_names.append(adapter_name)
            
            weight = weights[idx] if idx < len(weights) else 1.0
            adapter_weights.append(weight)
            
            state.pipe.load_lora_weights(converted_path, adapter_name=adapter_name)
            print(f"  âœ… {lora_name} (æƒé‡: {weight})")
            
        except Exception as e:
            print(f"  âŒ {lora_name} åŠ è½½å¤±è´¥: {str(e)}")
    
    # è®¾ç½®é€‚é…å™¨
    if adapter_names:
        state.pipe.set_adapters(adapter_names, adapter_weights=adapter_weights)
        print(f"âœ… LoRA åŠ è½½å®Œæˆï¼Œå…± {len(adapter_names)} ä¸ªæ¨¡å‹")


def _convert_lora_file(lora_path):
    """
    è‡ªåŠ¨è½¬æ¢å•ä¸ª LoRA æ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
    
    å‚æ•°:
        lora_path: LoRA æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹äº models/lora/Qwen-Image/ æˆ–ç»å¯¹è·¯å¾„ï¼‰
    
    è¿”å›:
        è½¬æ¢åçš„æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœå·²è½¬æ¢æˆ–ä¸éœ€è¦è½¬æ¢ï¼Œè¿”å›åŸè·¯å¾„æˆ–å·²å­˜åœ¨çš„è½¬æ¢æ–‡ä»¶è·¯å¾„ï¼‰
    """
    # ç¡®ä¿è·¯å¾„æ˜¯ç»å¯¹è·¯å¾„
    if not os.path.isabs(lora_path):
        lora_path = os.path.abspath(lora_path)
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯è½¬æ¢åçš„æ–‡ä»¶ï¼ˆåŒ…å« _diffusers åç¼€ï¼‰
    base_name = os.path.splitext(os.path.basename(lora_path))[0]
    if "_diffusers" in base_name:
        # å·²ç»æ˜¯è½¬æ¢åçš„æ–‡ä»¶ï¼Œç›´æ¥è¿”å›
        return lora_path
    
    # æ£€æŸ¥è½¬æ¢åçš„æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    output_dir = os.path.dirname(lora_path)
    output_filename = f"{base_name}_diffusers.safetensors"
    output_path = os.path.join(output_dir, output_filename)
    if os.path.exists(output_path):
        # è½¬æ¢åçš„æ–‡ä»¶å·²å­˜åœ¨ï¼Œè¿”å›è½¬æ¢åçš„è·¯å¾„
        return output_path
    
    # è¯»å–LoRAæ–‡ä»¶å¹¶æ£€æŸ¥æ˜¯å¦éœ€è¦è½¬æ¢
    try:
        lora_data = safetensors.torch.load_file(lora_path, device="cpu")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è½¬æ¢ï¼ˆæŸ¥æ‰¾éœ€è¦è½¬æ¢çš„keyæ ¼å¼ï¼‰
        needs_conversion = False
        for key in lora_data.keys():
            if ('.lora_A.' in key or '.lora_B.' in key or 
                'diffusion_model.transformer_blocks.' in key or
                'lora_unet_transformer_blocks_' in key or
                (key.startswith('transformer_blocks.') and not key.startswith('transformer.transformer_blocks.'))):
                needs_conversion = True
                break
        
        if not needs_conversion:
            # ä¸éœ€è¦è½¬æ¢ï¼Œç›´æ¥è¿”å›åŸè·¯å¾„
            return lora_path
        
        # éœ€è¦è½¬æ¢ï¼Œæ‰§è¡Œè½¬æ¢
        print(f"ğŸ”„ æ£€æµ‹åˆ°éœ€è¦è½¬æ¢çš„ LoRA æ–‡ä»¶: {os.path.basename(lora_path)}ï¼Œå¼€å§‹è‡ªåŠ¨è½¬æ¢...")
        converted_dict = {}
        for key, value in lora_data.items():
            if 'lora' not in key:
                continue
            elif 'alpha' in key:
                continue
            fixed_key = key
            if fixed_key.endswith(".lora_A.default.weight"):
                fixed_key = fixed_key.replace(".lora_A.default.weight", ".lora.down.weight")
            elif fixed_key.endswith(".lora_B.default.weight"):
                fixed_key = fixed_key.replace(".lora_B.default.weight", ".lora.up.weight")
            elif fixed_key.endswith(".lora_A.weight"):
                fixed_key = fixed_key.replace(".lora_A.weight", ".lora.down.weight") 
            elif fixed_key.endswith(".lora_B.weight"):
                fixed_key = fixed_key.replace(".lora_B.weight", ".lora.up.weight")
            elif fixed_key.endswith(".lora_down.weight"):
                fixed_key = fixed_key.replace(".lora_down.weight", ".lora.down.weight")
            elif fixed_key.endswith(".lora_up.weight"):
                fixed_key = fixed_key.replace(".lora_up.weight", ".lora.up.weight")
    
            if fixed_key.startswith("diffusion_model.transformer_blocks."):
                fixed_key = fixed_key.replace("diffusion_model.transformer_blocks.", "transformer.transformer_blocks.")
            elif fixed_key.startswith("lora_unet_transformer_blocks_"):
                fixed_key = fixed_key.replace("lora_unet_transformer_blocks_", "transformer.transformer_blocks.")
                fixed_key = fixed_key.replace("_attn_", ".attn.")
                fixed_key = fixed_key.replace("_img_mlp_net_", ".img_mlp.net.")
                fixed_key = fixed_key.replace("_img_mod_", ".img_mod.")
                fixed_key = fixed_key.replace("_txt_mlp_net_", ".txt_mlp.net.")
                fixed_key = fixed_key.replace("_txt_mod_", ".txt_mod.")
                fixed_key = fixed_key.replace("0_", "0.")
                fixed_key = fixed_key.replace("_0", ".0")
            elif fixed_key.startswith("transformer_blocks."):
                fixed_key = "transformer." + fixed_key
            converted_dict[fixed_key] = value

        # ä¿å­˜è½¬æ¢åçš„æ–‡ä»¶
        os.makedirs(output_dir, exist_ok=True)
        safetensors.torch.save_file(converted_dict, output_path)
        print(f"âœ… {output_filename} è½¬æ¢å®Œæˆ")
        return output_path
        
    except Exception as e:
        print(f"âš ï¸ è½¬æ¢ LoRA æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}ï¼Œå°è¯•ä½¿ç”¨åŸæ–‡ä»¶")
        return lora_path
