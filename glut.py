import gc
import io
import os
from PIL import Image, PngImagePlugin
import json
import math
import time
from mmgp import offload, profile_type
import base64
import torch
import torch.nn as nn
import numpy as np
import psutil
import random
import gradio as gr
from openai import OpenAI
import requests
import argparse
import datetime
import mimetypes
from diffusers import QwenImageTransformer2DModel, FlowMatchEulerDiscreteScheduler, QwenImageImg2ImgPipeline, QwenImagePipeline, QwenImageInpaintPipeline, QwenImageEditPipeline, QwenImageControlNetPipeline, QwenImageControlNetModel, QwenImageEditInpaintPipeline, QwenImageEditPlusPipeline
from diffusers.utils import load_image
from diffusers.image_processor import VaeImageProcessor
import safetensors.torch

parser = argparse.ArgumentParser() 
parser.add_argument("--server_name", type=str, default="127.0.0.1", help="IPåœ°å€ï¼Œå±€åŸŸç½‘è®¿é—®æ”¹ä¸º0.0.0.0")
parser.add_argument("--server_port", type=int, default=7891, help="ä½¿ç”¨ç«¯å£")
parser.add_argument("--share", action="store_true", help="æ˜¯å¦å¯ç”¨gradioå…±äº«")
parser.add_argument("--mcp_server", action="store_true", help="æ˜¯å¦å¯ç”¨mcpæœåŠ¡")
parser.add_argument("--compile", action="store_true", help="æ˜¯å¦å¯ç”¨compileåŠ é€Ÿ")
args = parser.parse_args()

print(" å¯åŠ¨ä¸­ï¼Œè¯·è€å¿ƒç­‰å¾… bilibili@åå­—é±¼ https://space.bilibili.com/893892")
print(f'\033[32mPytorchç‰ˆæœ¬ï¼š{torch.__version__}\033[0m')
if torch.cuda.is_available():
    device = "cuda" 
    print(f'\033[32mæ˜¾å¡å‹å·ï¼š{torch.cuda.get_device_name()}\033[0m')
    total_vram_in_gb = torch.cuda.get_device_properties(0).total_memory / 1073741824
    print(f'\033[32mæ˜¾å­˜å¤§å°ï¼š{total_vram_in_gb:.2f}GB\033[0m')
    mem = psutil.virtual_memory()
    print(f'\033[32må†…å­˜å¤§å°ï¼š{mem.total/1073741824:.2f}GB\033[0m')
    if torch.cuda.get_device_capability()[0] >= 8:
        print(f'\033[32mæ”¯æŒBF16\033[0m')
        dtype = torch.bfloat16
    else:
        print(f'\033[32mä¸æ”¯æŒBF16ï¼Œä»…æ”¯æŒFP16\033[0m')
        dtype = torch.float16
else:
    print(f'\033[32mCUDAä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥\033[0m')
    device = "cpu"


#åˆå§‹åŒ–
config = {}
transformer_choices = []
transformer_choices2 = []
transformer_choices3 = []
transformer_loaded = None
lora_choices = []
lora_loaded = None
lora_loaded_weights = None
image_loaded = None
mode = None
mode_loaded = None
pipe = None
prompt_cache = None
negative_prompt_cache = None
model_id = "models/Qwen-Image"
stop_generation = False
mmgp = None
#ç¡®ä¿è¾“å‡ºæ–‡ä»¶å¤¹å­˜åœ¨
os.makedirs("outputs", exist_ok=True)
#è¯»å–è®¾ç½®
CONFIG_FILE = "config.json"
if os.path.exists(CONFIG_FILE):
    with open(CONFIG_FILE, "r", encoding="utf-8") as f:
        config = json.load(f)
#é»˜è®¤è®¾ç½®
transformer_ = config.get("TRANSFORMER_DROPDOWN", "Qwen-Image-Lightning-8steps-V1.1-mmgp.safetensors")
transformer_2 = config.get("TRANSFORMER_DROPDOWN2", "Qwen-Image-Edit-Lightning-8steps-V1.0-mmgp.safetensors")
transformer_3 = config.get("TRANSFORMER_DROPDOWN3", "Qwen-Image-Edit-2509-Lightning-8steps-V1.0-mmgp.safetensors")
max_vram = float(config.get("MAX_VRAM", "0.8"))
openai_base_url = config.get("OPENAI_BASE_URL", "https://open.bigmodel.cn/api/paas/v4")
openai_api_key = config.get("OPENAI_API_KEY", "")
model_name = config.get("MODEL_NAME", "GLM-4.1V-Thinking-Flash")
temperature = float(config.get("TEMPERATURE", "0.8"))
top_p = float(config.get("TOP_P", "0.6"))
max_tokens = float(config.get("MAX_TOKENS", "16384"))
modelscope_api_key = config.get("MODELSCOPE_API_KEY", "")

def refresh_model():
    global transformer_choices, transformer_choices2,  transformer_choices3, lora_choices
    transformer_dir = "models/transformer"
    lora_dir = "models/lora"
    if os.path.exists(transformer_dir):
        transformer_files = [f for f in os.listdir(transformer_dir) if f.endswith(".safetensors")]
        transformer_choices = sorted([f for f in transformer_files] + ["ModelScope-QI.safetensors"])
        transformer_choices2 = sorted([f for f in transformer_files if "edit" in f or "Edit" in f])
        #transformer_choices2 = sorted([f for f in transformer_files if "edit" in f or "Edit" in f] + ["ModelScope-QIE.safetensors"])
        transformer_choices3 = sorted([f for f in transformer_files if "2509" in f])
    else:
        print("transformeræ–‡ä»¶å¤¹ä¸å­˜åœ¨")
    if os.path.exists(lora_dir):
        lora_files = [f for f in os.listdir(lora_dir) if f.endswith(".safetensors")]
        lora_choices = sorted(lora_files)
    else:
        lora_choices = []
    return gr.Dropdown(choices=transformer_choices), gr.Dropdown(choices=transformer_choices2), gr.Dropdown(choices=transformer_choices3), gr.Dropdown(choices=lora_choices)

refresh_model()

def build_lora_names(key, lora_down_key, lora_up_key, is_native_weight):
    base = "diffusion_model." if is_native_weight else ""
    lora_down = base + key.replace(".weight", lora_down_key)
    lora_up = base + key.replace(".weight", lora_up_key)
    lora_alpha = base + key.replace(".weight", ".alpha")
    return lora_down, lora_up, lora_alpha


def load_and_merge_lora_weight(
    model: nn.Module,
    lora_state_dict: dict,
    lora_down_key: str = ".lora_down.weight",
    lora_up_key: str = ".lora_up.weight",
):
    is_native_weight = any("diffusion_model." in key for key in lora_state_dict)
    for key, value in model.named_parameters():
        lora_down_name, lora_up_name, lora_alpha_name = build_lora_names(
            key, lora_down_key, lora_up_key, is_native_weight
        )
        if lora_down_name in lora_state_dict:
            lora_down = lora_state_dict[lora_down_name]
            lora_up = lora_state_dict[lora_up_name]
            lora_alpha = float(lora_state_dict[lora_alpha_name])
            rank = lora_down.shape[0]
            scaling_factor = lora_alpha / rank
            assert lora_up.dtype == torch.float32
            assert lora_down.dtype == torch.float32
            delta_W = scaling_factor * torch.matmul(lora_up, lora_down)
            value.data = (value.data + delta_W).type_as(value.data)
    return model


def load_and_merge_lora_weight_from_safetensors(
    model: nn.Module,
    lora_weight_path: str,
    lora_down_key: str = ".lora_down.weight",
    lora_up_key: str = ".lora_up.weight",
):
    lora_state_dict = {}
    with safetensors.torch.safe_open(lora_weight_path, framework="pt", device="cpu") as f:
        for key in f.keys():
            lora_state_dict[key] = f.get_tensor(key)
    model = load_and_merge_lora_weight(
        model, lora_state_dict, lora_down_key, lora_up_key
    )
    return model


def load_model(mode, transformer_dropdown, lora_dropdown, lora_weights, max_vram):
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()
    global pipe, mode_loaded, transformer_loaded, lora_loaded, lora_loaded_weights, mmgp
    max_vram = float(max_vram)
    budgets = int(torch.cuda.get_device_properties(0).total_memory/1048576 * max_vram)
    scheduler_config = {
            "base_image_seq_len": 256,
            "base_shift": math.log(3),  # We use shift=3 in distillation
            "invert_sigmas": False,
            "max_image_seq_len": 8192,
            "max_shift": math.log(3),  # We use shift=3 in distillation
            "num_train_timesteps": 1000,
            "shift": 1.0,
            "shift_terminal": None,  # set shift_terminal to None
            "stochastic_sampling": False,
            "time_shift_type": "exponential",
            "use_beta_sigmas": False,
            "use_dynamic_shifting": True,
            "use_exponential_sigmas": False,
            "use_karras_sigmas": False,
        }
    # åˆ¤æ–­æ˜¯å¦éœ€è¦é‡æ–°åŠ è½½æ¨¡å‹
    if (pipe is None or mode_loaded != mode or transformer_loaded != transformer_dropdown or 
        lora_loaded != lora_dropdown or lora_loaded_weights != lora_weights):
        if pipe is not None:
            pipe.unload_lora_weights()
            mmgp.release()
        # æ›´æ–°å…¨å±€çŠ¶æ€
        mode_loaded, transformer_loaded, lora_loaded, lora_loaded_weights = (
            mode, transformer_dropdown, lora_dropdown, lora_weights
        )
        # åŠ è½½transformer
        #transformer = QwenImageTransformer2DModel.from_pretrained(model_id, subfolder="transformer", torch_dtype=dtype)
        #transformer = QwenImageTransformer2DModel.from_single_file("Real-Qwen-Image-V1.safetensors", config=f"{model_id}/transformer/config.json", torch_dtype=dtype)
        #transformer = load_and_merge_lora_weight_from_safetensors(transformer, "Qwen-Image-Edit-Lightning-8steps-V1.0.safetensors")
        if "mmgp" in transformer_dropdown:
            transformer = offload.fast_load_transformers_model(
                f"models/transformer/{transformer_dropdown}",
                do_quantize=False,
                modelClass=QwenImageTransformer2DModel,
                forcedConfigPath=f"{model_id}/transformer/config.json",
            )
        else:
            raise ValueError("è¯·ä½¿ç”¨mmgpè½¬åŒ–åä¿å­˜çš„æ¨¡å‹")
        # åŠ è½½scheduler
        if "Lightning" in transformer_dropdown:
            print("ä½¿ç”¨Lightning scheduler")
            scheduler = FlowMatchEulerDiscreteScheduler.from_config(scheduler_config)
        else:
            print("ä½¿ç”¨åŸå§‹scheduler")
            scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(model_id, subfolder="scheduler")
        # æ ¹æ®æ¨¡å¼åˆå§‹åŒ–pipeline
        pipeline_class = {
            "t2i": QwenImagePipeline,
            "i2i": QwenImageImg2ImgPipeline,
            "inp": QwenImageInpaintPipeline,
            "con": QwenImageControlNetPipeline,
            "edit": QwenImageEditPipeline,
            "edit2": QwenImageEditPipeline,
            "editinp":QwenImageEditInpaintPipeline,
            "editplus":QwenImageEditPlusPipeline,
        }.get(mode)
        if pipeline_class is None:
            raise ValueError(f"Unsupported mode: {mode}")
        if mode != "con":
            pipe = pipeline_class.from_pretrained(
                model_id, 
                transformer=transformer,
                scheduler=scheduler,
                torch_dtype=dtype,
            )
        elif mode == "con":
            controlnet = QwenImageControlNetModel.from_pretrained("models/Qwen-Image-ControlNet-Union", torch_dtype=dtype)
            pipe = pipeline_class.from_pretrained(
                model_id, 
                controlnet=controlnet,
                transformer=transformer,
                scheduler=scheduler,
                torch_dtype=dtype,
            )
        if mode in ["edit", "edit2", "editplus"]:
            pipe.set_progress_bar_config(disable=None)
        # åŠ è½½LoRAå¹¶é…ç½®æ˜¾å­˜
        load_lora(lora_dropdown, lora_weights)
        """mmgp = offload.all(
            pipe, 
            pinnedMemory= "transformer",
            budgets={'*': budgets}, 
            compile=True if args.compile else False,
        )"""
        mmgp = offload.profile(
            pipe, 
            profile_type.LowRAM_HighVRAM, 
            budgets={'*': budgets}, 
            compile=True if args.compile else False,
        )
        #offload.save_model(pipe.transformer, "models/transformer-mmgp.safetensors")


def load_lora(lora_dropdown, lora_weights):
    if lora_dropdown != []:
        global pipe
        adapter_names = []
        weightss = []
        weights = [float(w) for w in lora_weights.split(',')] if lora_weights else []
        for idx, lora_name in enumerate(lora_dropdown):
            try:
                adapter_name = os.path.splitext(os.path.basename(lora_name))[0]
                adapter_names.append(adapter_name)
                weight = weights[idx] if idx < len(weights) else 1.0
                weightss.append(weight)
                pipe.load_lora_weights(f"models/lora/{lora_name}", adapter_name=adapter_name)
                print(f"âœ… å·²åŠ è½½LoRAæ¨¡å‹: {lora_name} (æƒé‡: {weight})")
            except Exception as e:
                print(f"âŒ åŠ è½½{adapter_name}å¤±è´¥: {str(e)}")
        pipe.set_adapters(adapter_names, adapter_weights=weightss)
        print("LoRAåŠ è½½å®Œæˆ")


def enhance_prompt(prompt, image=None, retry_times=3):
    if isinstance(image, dict):
        image = image["background"]
    if openai_api_key == "":
        return prompt, "è¯·åœ¨è®¾ç½®ä¸­ï¼Œå¡«å†™APIç›¸å…³ä¿¡æ¯å¹¶ä¿å­˜"
    try:
        client = OpenAI(
            base_url = openai_base_url,
            api_key = openai_api_key,
        )
        text = prompt.strip()
        if image:
            pil_img = image.convert("RGB")
            img_byte_arr = io.BytesIO()
            pil_img.save(img_byte_arr, format='PNG')
            img_base = base64.b64encode(img_byte_arr.getvalue()).decode('utf-8')
            for i in range(retry_times):
                response = client.chat.completions.create(
                    messages=[{"role": "system", "content": """
æ‚¨æ˜¯ä¸€åä¸“ä¸šçš„å›¾åƒæ³¨é‡Šå‘˜ã€‚è¯·æ ¹æ®è¾“å…¥å›¾åƒå®Œæˆä»¥ä¸‹ä»»åŠ¡ã€‚
1.è¯¦ç»†æè¿°å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹ã€‚é¦–å…ˆä¿è¯å†…å®¹å®Œæ•´ï¼Œä¸è¦ç¼ºå­—ã€‚æè¿°æ–‡å­—çš„ä½ç½®ï¼Œå¦‚ï¼šå·¦ä¸Šè§’ã€å³ä¸‹è§’æˆ–è€…ç¬¬ä¸€è¡Œã€ç¬¬äºŒè¡Œç­‰ã€‚æè¿°æ–‡å­—çš„é£æ ¼æˆ–å­—ä½“ã€‚
2.è¯¦ç»†æè¿°å›¾ç‰‡ä¸­çš„å…¶ä»–å†…å®¹ã€‚
"""
                    },
                    {
                        "role": "user",
                        "content":  [
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": img_base
                                }
                            },
                            {
                                "type": "text",
                                "text": "åæ¨",
                            }
                        ]
                    },
                    ],
                    model=model_name,
                    temperature=temperature,
                    top_p=top_p,
                    stream=False,
                    max_tokens=max_tokens,
                )
                if response.choices:
                    return response.choices[0].message.content.replace('"', '').replace('<|begin_of_box|>', '').replace('<|end_of_box|>', ''), "âœ… åæ¨æç¤ºè¯å®Œæ¯•"
        else:
            for i in range(retry_times):
                response = client.chat.completions.create(
                    messages=[{"role": "system", "content": """
ä½ æ˜¯ä¸€ä½Promptä¼˜åŒ–å¸ˆï¼Œæ—¨åœ¨å°†ç”¨æˆ·è¾“å…¥æ”¹å†™ä¸ºä¼˜è´¨Promptï¼Œä½¿å…¶æ›´å®Œæ•´ã€æ›´å…·è¡¨ç°åŠ›ï¼ŒåŒæ—¶ä¸æ”¹å˜åŸæ„ã€‚

ä»»åŠ¡è¦æ±‚ï¼š
1. å¯¹äºè¿‡äºç®€çŸ­çš„ç”¨æˆ·è¾“å…¥ï¼Œåœ¨ä¸æ”¹å˜åŸæ„å‰æä¸‹ï¼Œåˆç†æ¨æ–­å¹¶è¡¥å……ç»†èŠ‚ï¼Œä½¿å¾—ç”»é¢æ›´åŠ å®Œæ•´å¥½çœ‹ï¼Œä½†æ˜¯éœ€è¦ä¿ç•™ç”»é¢çš„ä¸»è¦å†…å®¹ï¼ˆåŒ…æ‹¬ä¸»ä½“ï¼Œç»†èŠ‚ï¼ŒèƒŒæ™¯ç­‰ï¼‰ï¼›
2. å®Œå–„ç”¨æˆ·æè¿°ä¸­å‡ºç°çš„ä¸»ä½“ç‰¹å¾ï¼ˆå¦‚å¤–è²Œã€è¡¨æƒ…ï¼Œæ•°é‡ã€ç§æ—ã€å§¿æ€ç­‰ï¼‰ã€ç”»é¢é£æ ¼ã€ç©ºé—´å…³ç³»ã€é•œå¤´æ™¯åˆ«ï¼›
3. å¦‚æœç”¨æˆ·è¾“å…¥ä¸­éœ€è¦åœ¨å›¾åƒä¸­ç”Ÿæˆæ–‡å­—å†…å®¹ï¼Œè¯·æŠŠå…·ä½“çš„æ–‡å­—éƒ¨åˆ†ç”¨å¼•å·è§„èŒƒçš„è¡¨ç¤ºï¼ŒåŒæ—¶éœ€è¦æŒ‡æ˜æ–‡å­—çš„ä½ç½®ï¼ˆå¦‚ï¼šå·¦ä¸Šè§’ã€å³ä¸‹è§’ç­‰ï¼‰å’Œé£æ ¼ï¼Œè¿™éƒ¨åˆ†çš„æ–‡å­—ä¸éœ€è¦æ”¹å†™ï¼›
4. å¦‚æœéœ€è¦åœ¨å›¾åƒä¸­ç”Ÿæˆçš„æ–‡å­—æ¨¡æ£±ä¸¤å¯ï¼Œåº”è¯¥æ”¹æˆå…·ä½“çš„å†…å®¹ï¼Œå¦‚ï¼šç”¨æˆ·è¾“å…¥ï¼šé‚€è¯·å‡½ä¸Šå†™ç€åå­—å’Œæ—¥æœŸç­‰ä¿¡æ¯ï¼Œåº”è¯¥æ”¹ä¸ºå…·ä½“çš„æ–‡å­—å†…å®¹ï¼š é‚€è¯·å‡½çš„ä¸‹æ–¹å†™ç€â€œå§“åï¼šå¼ ä¸‰ï¼Œæ—¥æœŸï¼š 2025å¹´7æœˆâ€ï¼›
5. å¦‚æœç”¨æˆ·è¾“å…¥ä¸­è¦æ±‚ç”Ÿæˆç‰¹å®šçš„é£æ ¼ï¼Œåº”å°†é£æ ¼ä¿ç•™ã€‚è‹¥ç”¨æˆ·æ²¡æœ‰æŒ‡å®šï¼Œä½†ç”»é¢å†…å®¹é€‚åˆç”¨æŸç§è‰ºæœ¯é£æ ¼è¡¨ç°ï¼Œåˆ™åº”é€‰æ‹©æœ€ä¸ºåˆé€‚çš„é£æ ¼ã€‚å¦‚ï¼šç”¨æˆ·è¾“å…¥æ˜¯å¤è¯—ï¼Œåˆ™åº”é€‰æ‹©ä¸­å›½æ°´å¢¨æˆ–è€…æ°´å½©ç±»ä¼¼çš„é£æ ¼ã€‚å¦‚æœå¸Œæœ›ç”ŸæˆçœŸå®çš„ç…§ç‰‡ï¼Œåˆ™åº”é€‰æ‹©çºªå®æ‘„å½±é£æ ¼æˆ–è€…çœŸå®æ‘„å½±é£æ ¼ï¼›
6. å¦‚æœPromptæ˜¯å¤è¯—è¯ï¼Œåº”è¯¥åœ¨ç”Ÿæˆçš„Promptä¸­å¼ºè°ƒä¸­å›½å¤å…¸å…ƒç´ ï¼Œé¿å…å‡ºç°è¥¿æ–¹ã€ç°ä»£ã€å¤–å›½åœºæ™¯ï¼›
7. å¦‚æœç”¨æˆ·è¾“å…¥ä¸­åŒ…å«é€»è¾‘å…³ç³»ï¼Œåˆ™åº”è¯¥åœ¨æ”¹å†™ä¹‹åçš„promptä¸­ä¿ç•™é€»è¾‘å…³ç³»ã€‚å¦‚ï¼šç”¨æˆ·è¾“å…¥ä¸ºâ€œç”»ä¸€ä¸ªè‰åŸä¸Šçš„é£Ÿç‰©é“¾â€ï¼Œåˆ™æ”¹å†™ä¹‹ååº”è¯¥æœ‰ä¸€äº›ç®­å¤´æ¥è¡¨ç¤ºé£Ÿç‰©é“¾çš„å…³ç³»ã€‚
8. æ”¹å†™ä¹‹åçš„promptä¸­ä¸åº”è¯¥å‡ºç°ä»»ä½•å¦å®šè¯ã€‚å¦‚ï¼šç”¨æˆ·è¾“å…¥ä¸ºâ€œä¸è¦æœ‰ç­·å­â€ï¼Œåˆ™æ”¹å†™ä¹‹åçš„promptä¸­ä¸åº”è¯¥å‡ºç°ç­·å­ã€‚
9. é™¤äº†ç”¨æˆ·æ˜ç¡®è¦æ±‚ä¹¦å†™çš„æ–‡å­—å†…å®¹å¤–ï¼Œ**ç¦æ­¢å¢åŠ ä»»ä½•é¢å¤–çš„æ–‡å­—å†…å®¹**ã€‚

æ”¹å†™ç¤ºä¾‹ï¼š
1. ç”¨æˆ·è¾“å…¥ï¼š"ä¸€å¼ å­¦ç”Ÿæ‰‹ç»˜ä¼ å•ï¼Œä¸Šé¢å†™ç€ï¼šwe sell waffles: 4 for _5, benefiting a youth sports fundã€‚"
    æ”¹å†™è¾“å‡ºï¼š"æ‰‹ç»˜é£æ ¼çš„å­¦ç”Ÿä¼ å•ï¼Œä¸Šé¢ç”¨ç¨šå«©çš„æ‰‹å†™å­—ä½“å†™ç€ï¼šâ€œWe sell waffles: 4 for $5â€ï¼Œå³ä¸‹è§’æœ‰å°å­—æ³¨æ˜"benefiting a youth sports fund"ã€‚ç”»é¢ä¸­ï¼Œä¸»ä½“æ˜¯ä¸€å¼ è‰²å½©é²œè‰³çš„åå¤«é¥¼å›¾æ¡ˆï¼Œæ—è¾¹ç‚¹ç¼€ç€ä¸€äº›ç®€å•çš„è£…é¥°å…ƒç´ ï¼Œå¦‚æ˜Ÿæ˜Ÿã€å¿ƒå½¢å’Œå°èŠ±ã€‚èƒŒæ™¯æ˜¯æµ…è‰²çš„çº¸å¼ è´¨æ„Ÿï¼Œå¸¦æœ‰è½»å¾®çš„æ‰‹ç»˜ç¬”è§¦ç—•è¿¹ï¼Œè¥é€ å‡ºæ¸©é¦¨å¯çˆ±çš„æ°›å›´ã€‚ç”»é¢é£æ ¼ä¸ºå¡é€šæ‰‹ç»˜é£ï¼Œè‰²å½©æ˜äº®ä¸”å¯¹æ¯”é²œæ˜ã€‚"
2. ç”¨æˆ·è¾“å…¥ï¼š"ä¸€å¼ çº¢é‡‘è¯·æŸ¬è®¾è®¡ï¼Œä¸Šé¢æ˜¯éœ¸ç‹é¾™å›¾æ¡ˆå’Œå¦‚æ„äº‘ç­‰ä¼ ç»Ÿä¸­å›½å…ƒç´ ï¼Œç™½è‰²èƒŒæ™¯ã€‚é¡¶éƒ¨ç”¨é»‘è‰²æ–‡å­—å†™ç€â€œInvitationâ€ï¼Œåº•éƒ¨å†™ç€æ—¥æœŸã€åœ°ç‚¹å’Œé‚€è¯·äººã€‚"
    æ”¹å†™è¾“å‡ºï¼š"ä¸­å›½é£çº¢é‡‘è¯·æŸ¬è®¾è®¡ï¼Œä»¥éœ¸ç‹é¾™å›¾æ¡ˆå’Œå¦‚æ„äº‘ç­‰ä¼ ç»Ÿä¸­å›½å…ƒç´ ä¸ºä¸»è£…é¥°ã€‚èƒŒæ™¯ä¸ºçº¯ç™½è‰²ï¼Œé¡¶éƒ¨ç”¨é»‘è‰²å®‹ä½“å­—å†™ç€â€œInvitationâ€ï¼Œåº•éƒ¨åˆ™ç”¨åŒæ ·çš„å­—ä½“é£æ ¼å†™æœ‰å…·ä½“çš„æ—¥æœŸã€åœ°ç‚¹å’Œé‚€è¯·äººä¿¡æ¯ï¼šâ€œæ—¥æœŸï¼š2023å¹´10æœˆ1æ—¥ï¼Œåœ°ç‚¹ï¼šåŒ—äº¬æ•…å®«åšç‰©é™¢ï¼Œé‚€è¯·äººï¼šæåâ€ã€‚éœ¸ç‹é¾™å›¾æ¡ˆç”ŸåŠ¨è€Œå¨æ­¦ï¼Œå¦‚æ„äº‘ç¯ç»•åœ¨å…¶å‘¨å›´ï¼Œè±¡å¾å‰ç¥¥å¦‚æ„ã€‚æ•´ä½“è®¾è®¡èåˆäº†ç°ä»£ä¸ä¼ ç»Ÿçš„ç¾æ„Ÿï¼Œè‰²å½©å¯¹æ¯”é²œæ˜ï¼Œçº¿æ¡æµç•…ä¸”å¯Œæœ‰ç»†èŠ‚ã€‚ç”»é¢ä¸­è¿˜ç‚¹ç¼€ç€ä¸€äº›ç²¾è‡´çš„ä¸­å›½ä¼ ç»Ÿçº¹æ ·ï¼Œå¦‚è²èŠ±ã€ç¥¥äº‘ç­‰ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†å…¶æ–‡åŒ–åº•è•´ã€‚"
3. ç”¨æˆ·è¾“å…¥ï¼š"ä¸€å®¶ç¹å¿™çš„å’–å•¡åº—ï¼Œæ‹›ç‰Œä¸Šç”¨ä¸­æ£•è‰²è‰ä¹¦å†™ç€â€œCAFEâ€ï¼Œé»‘æ¿ä¸Šåˆ™ç”¨å¤§å·ç»¿è‰²ç²—ä½“å­—å†™ç€â€œSPECIALâ€"
    æ”¹å†™è¾“å‡ºï¼š"ç¹åéƒ½å¸‚ä¸­çš„ä¸€å®¶ç¹å¿™å’–å•¡åº—ï¼Œåº—å†…äººæ¥äººå¾€ã€‚æ‹›ç‰Œä¸Šç”¨ä¸­æ£•è‰²è‰ä¹¦å†™ç€â€œCAFEâ€ï¼Œå­—ä½“æµç•…è€Œå¯Œæœ‰è‰ºæœ¯æ„Ÿï¼Œæ‚¬æŒ‚åœ¨åº—é—¨å£çš„æ­£ä¸Šæ–¹ã€‚é»‘æ¿ä¸Šåˆ™ç”¨å¤§å·ç»¿è‰²ç²—ä½“å­—å†™ç€â€œSPECIALâ€ï¼Œå­—ä½“é†’ç›®ä¸”å…·æœ‰å¼ºçƒˆçš„è§†è§‰å†²å‡»åŠ›ï¼Œæ”¾ç½®åœ¨åº—å†…çš„æ˜¾çœ¼ä½ç½®ã€‚åº—å†…è£…é¥°æ¸©é¦¨èˆ’é€‚ï¼Œæœ¨è´¨æ¡Œæ¤…å’Œå¤å¤åŠç¯è¥é€ å‡ºä¸€ç§æ¸©æš–è€Œæ€€æ—§çš„æ°›å›´ã€‚èƒŒæ™¯ä¸­å¯ä»¥çœ‹åˆ°å¿™ç¢Œçš„å’–å•¡å¸ˆæ­£åœ¨ä¸“æ³¨åœ°åˆ¶ä½œå’–å•¡ï¼Œé¡¾å®¢ä»¬æˆ–åæˆ–ç«™ï¼Œäº«å—ç€å’–å•¡å¸¦æ¥çš„æ„‰æ‚¦æ—¶å…‰ã€‚æ•´ä½“ç”»é¢é‡‡ç”¨çºªå®æ‘„å½±é£æ ¼ï¼Œè‰²å½©é¥±å’Œåº¦é€‚ä¸­ï¼Œå…‰çº¿æŸ”å’Œè‡ªç„¶ã€‚"
4. ç”¨æˆ·è¾“å…¥ï¼š"æ‰‹æœºæŒ‚ç»³å±•ç¤ºï¼Œå››ä¸ªæ¨¡ç‰¹ç”¨æŒ‚ç»³æŠŠæ‰‹æœºæŒ‚åœ¨è„–å­ä¸Šï¼Œä¸ŠåŠèº«å›¾ã€‚"
    æ”¹å†™è¾“å‡ºï¼š"æ—¶å°šæ‘„å½±é£æ ¼ï¼Œå››ä½å¹´è½»æ¨¡ç‰¹å±•ç¤ºæ‰‹æœºæŒ‚ç»³çš„ä½¿ç”¨æ–¹å¼ï¼Œä»–ä»¬å°†æ‰‹æœºé€šè¿‡æŒ‚ç»³æŒ‚åœ¨è„–å­ä¸Šã€‚æ¨¡ç‰¹ä»¬å§¿æ€å„å¼‚ä½†éƒ½æ˜¾å¾—è½»æ¾è‡ªç„¶ï¼Œå…¶ä¸­ä¸¤ä½æ¨¡ç‰¹æ­£é¢æœå‘é•œå¤´å¾®ç¬‘ï¼Œå¦å¤–ä¸¤ä½åˆ™ä¾§èº«ç«™ç«‹ï¼Œé¢å‘å½¼æ­¤äº¤è°ˆã€‚æ¨¡ç‰¹ä»¬çš„æœè£…é£æ ¼å¤šæ ·ä½†ç»Ÿä¸€ä¸ºä¼‘é—²é£ï¼Œé¢œè‰²ä»¥æµ…è‰²ç³»ä¸ºä¸»ï¼Œä¸æŒ‚ç»³å½¢æˆé²œæ˜å¯¹æ¯”ã€‚æŒ‚ç»³æœ¬èº«è®¾è®¡ç®€æ´å¤§æ–¹ï¼Œè‰²å½©é²œè‰³ä¸”å…·æœ‰å“ç‰Œæ ‡è¯†ã€‚èƒŒæ™¯ä¸ºç®€çº¦çš„ç™½è‰²æˆ–ç°è‰²è°ƒï¼Œè¥é€ å‡ºç°ä»£è€Œå¹²å‡€çš„æ„Ÿè§‰ã€‚é•œå¤´èšç„¦äºæ¨¡ç‰¹ä»¬çš„ä¸ŠåŠèº«ï¼Œçªå‡ºæŒ‚ç»³å’Œæ‰‹æœºçš„ç»†èŠ‚ã€‚"
5. ç”¨æˆ·è¾“å…¥ï¼š"ä¸€åªå°å¥³å­©å£ä¸­å«ç€é’è›™ã€‚"
    æ”¹å†™è¾“å‡ºï¼š"ä¸€åªç©¿ç€ç²‰è‰²è¿è¡£è£™çš„å°å¥³å­©ï¼Œçš®è‚¤ç™½çš™ï¼Œæœ‰ç€å¤§å¤§çš„çœ¼ç›å’Œä¿çš®çš„é½è€³çŸ­å‘ï¼Œå¥¹å£ä¸­å«ç€ä¸€åªç»¿è‰²çš„å°é’è›™ã€‚å°å¥³å­©çš„è¡¨æƒ…æ—¢å¥½å¥‡åˆæœ‰äº›æƒŠæã€‚èƒŒæ™¯æ˜¯ä¸€ç‰‡å……æ»¡ç”Ÿæœºçš„æ£®æ—ï¼Œå¯ä»¥çœ‹åˆ°æ ‘æœ¨ã€èŠ±è‰ä»¥åŠè¿œå¤„è‹¥éšè‹¥ç°çš„å°åŠ¨ç‰©ã€‚å†™å®æ‘„å½±é£æ ¼ã€‚"
6. ç”¨æˆ·è¾“å…¥ï¼š"å­¦æœ¯é£æ ¼ï¼Œä¸€ä¸ªLarge VL Modelï¼Œå…ˆé€šè¿‡promptå¯¹ä¸€ä¸ªå›¾ç‰‡é›†åˆï¼ˆå›¾ç‰‡é›†åˆæ˜¯ä¸€äº›æ¯”å¦‚é’é“œå™¨ã€é’èŠ±ç“·ç“¶ç­‰ï¼‰è‡ªç”±çš„æ‰“æ ‡ç­¾å¾—åˆ°æ ‡ç­¾é›†åˆï¼ˆæ¯”å¦‚é“­æ–‡è§£è¯»ã€çº¹é¥°åˆ†æç­‰ï¼‰ï¼Œç„¶åå¯¹æ ‡ç­¾é›†åˆè¿›è¡Œå»é‡ç­‰æ“ä½œåï¼Œç”¨è¿‡æ»¤åçš„æ•°æ®è®­ä¸€ä¸ªå°çš„Qwen-VL-Instagæ¨¡å‹ï¼Œè¦ç”»å‡ºæ­¥éª¤é—´çš„æµç¨‹ï¼Œä¸éœ€è¦slidesé£æ ¼"
    æ”¹å†™è¾“å‡ºï¼š"å­¦æœ¯é£æ ¼æ’å›¾ï¼Œå·¦ä¸Šè§’å†™ç€æ ‡é¢˜â€œLarge VL Modelâ€ã€‚å·¦ä¾§å±•ç¤ºVLæ¨¡å‹å¯¹æ–‡ç‰©å›¾åƒé›†åˆçš„åˆ†æè¿‡ç¨‹ï¼Œå›¾åƒé›†åˆåŒ…å«ä¸­å›½å¤ä»£æ–‡ç‰©ï¼Œä¾‹å¦‚é’é“œå™¨å’Œé’èŠ±ç“·ç“¶ç­‰ã€‚æ¨¡å‹å¯¹è¿™äº›å›¾åƒè¿›è¡Œè‡ªåŠ¨æ ‡æ³¨ï¼Œç”Ÿæˆæ ‡ç­¾é›†åˆï¼Œä¸‹é¢å†™ç€â€œé“­æ–‡è§£è¯»â€å’Œâ€œçº¹é¥°åˆ†æâ€ï¼›ä¸­é—´å†™ç€â€œæ ‡ç­¾å»é‡â€ï¼›å³è¾¹ï¼Œè¿‡æ»¤åçš„æ•°æ®è¢«ç”¨äºè®­ç»ƒ Qwen-VL-Instagï¼Œå†™ç€â€œ Qwen-VL-Instagâ€ã€‚ ç”»é¢é£æ ¼ä¸ºä¿¡æ¯å›¾é£æ ¼ï¼Œçº¿æ¡ç®€æ´æ¸…æ™°ï¼Œé…è‰²ä»¥è“ç°ä¸ºä¸»ï¼Œä½“ç°ç§‘æŠ€æ„Ÿä¸å­¦æœ¯æ„Ÿã€‚æ•´ä½“æ„å›¾é€»è¾‘ä¸¥è°¨ï¼Œä¿¡æ¯ä¼ è¾¾æ˜ç¡®ï¼Œç¬¦åˆå­¦æœ¯è®ºæ–‡æ’å›¾çš„è§†è§‰æ ‡å‡†ã€‚"
7. ç”¨æˆ·è¾“å…¥ï¼š"æ‰‹ç»˜å°æŠ„ï¼Œæ°´å¾ªç¯ç¤ºæ„å›¾"
    æ”¹å†™è¾“å‡ºï¼š"æ‰‹ç»˜é£æ ¼çš„æ°´å¾ªç¯ç¤ºæ„å›¾ï¼Œæ•´ä½“ç”»é¢å‘ˆç°å‡ºä¸€å¹…ç”ŸåŠ¨å½¢è±¡çš„æ°´å¾ªç¯è¿‡ç¨‹å›¾è§£ã€‚ç”»é¢ä¸­å¤®æ˜¯ä¸€ç‰‡èµ·ä¼çš„å±±è„‰å’Œå±±è°·ï¼Œå±±è°·ä¸­æµæ·Œç€ä¸€æ¡æ¸…æ¾ˆçš„æ²³æµï¼Œæ²³æµæœ€ç»ˆæ±‡å…¥ä¸€ç‰‡å¹¿é˜”çš„æµ·æ´‹ã€‚å±±ä½“å’Œé™†åœ°ä¸Šç»˜åˆ¶æœ‰ç»¿è‰²æ¤è¢«ã€‚ç”»é¢ä¸‹æ–¹ä¸ºåœ°ä¸‹æ°´å±‚ï¼Œç”¨è“è‰²æ¸å˜è‰²å—è¡¨ç°ï¼Œä¸åœ°è¡¨æ°´å½¢æˆå±‚æ¬¡åˆ†æ˜çš„ç©ºé—´å…³ç³»ã€‚ å¤ªé˜³ä½äºç”»é¢å³ä¸Šè§’ï¼Œä¿ƒä½¿åœ°è¡¨æ°´è’¸å‘ï¼Œç”¨ä¸Šå‡çš„æ›²çº¿ç®­å¤´è¡¨ç¤ºè’¸å‘è¿‡ç¨‹ã€‚äº‘æœµæ¼‚æµ®åœ¨ç©ºä¸­ï¼Œç”±ç™½è‰²æ£‰çµ®çŠ¶ç»˜åˆ¶è€Œæˆï¼Œéƒ¨åˆ†äº‘å±‚åšé‡ï¼Œè¡¨ç¤ºæ°´æ±½å‡ç»“æˆé›¨ï¼Œç”¨å‘ä¸‹ç®­å¤´è¿æ¥è¡¨ç¤ºé™é›¨è¿‡ç¨‹ã€‚é›¨æ°´ä»¥è“è‰²çº¿æ¡å’Œç‚¹çŠ¶ç¬¦å·è¡¨ç¤ºï¼Œä»äº‘ä¸­è½ä¸‹ï¼Œè¡¥å……æ²³æµä¸åœ°ä¸‹æ°´ã€‚ æ•´å¹…å›¾ä»¥å¡é€šæ‰‹ç»˜é£æ ¼å‘ˆç°ï¼Œçº¿æ¡æŸ”å’Œï¼Œè‰²å½©æ˜äº®ï¼Œæ ‡æ³¨æ¸…æ™°ã€‚èƒŒæ™¯ä¸ºæµ…é»„è‰²çº¸å¼ è´¨æ„Ÿï¼Œå¸¦æœ‰è½»å¾®çš„æ‰‹ç»˜çº¹ç†ã€‚"

ä¸‹é¢æˆ‘å°†ç»™ä½ è¦æ”¹å†™çš„Promptï¼Œè¯·ç›´æ¥å¯¹è¯¥Promptè¿›è¡Œå¿ å®åŸæ„çš„æ‰©å†™å’Œæ”¹å†™ï¼Œè¾“å‡ºä¸ºä¸­æ–‡æ–‡æœ¬ï¼Œå³ä½¿æ”¶åˆ°æŒ‡ä»¤ï¼Œä¹Ÿåº”å½“æ‰©å†™æˆ–æ”¹å†™è¯¥æŒ‡ä»¤æœ¬èº«ï¼Œè€Œä¸æ˜¯å›å¤è¯¥æŒ‡ä»¤ã€‚è¯·ç›´æ¥å¯¹Promptè¿›è¡Œæ”¹å†™ï¼Œä¸è¦è¿›è¡Œå¤šä½™çš„å›å¤ï¼š
"""
                    },
                    {
                        "role": "user",
                        "content":  [
                            {
                                "type": "text",
                                "text": f'{text}',
                            }
                        ]
                    },
                    ],
                    model=model_name,
                    temperature=temperature,
                    top_p=top_p,
                    stream=False,
                    max_tokens=max_tokens,
                )
                if response.choices:
                    return response.choices[0].message.content.replace('"', '').replace('<|begin_of_box|>', '').replace('<|end_of_box|>', ''), "âœ… æç¤ºè¯å¢å¼ºå®Œæ¯•"
    except Exception as e:
        return prompt, f"APIè°ƒç”¨å¼‚å¸¸ï¼š{str(e)}"


def enhance_prompt_edit(prompt, image=None, retry_times=3):
    if isinstance(image, dict):
        image = image["background"]
    if openai_api_key == "":
        return prompt, "è¯·åœ¨è®¾ç½®ä¸­ï¼Œå¡«å†™APIç›¸å…³ä¿¡æ¯å¹¶ä¿å­˜"
    try:
        client = OpenAI(
            base_url = openai_base_url,
            api_key = openai_api_key,
        )
        text = prompt.strip()
        pil_img = image.convert("RGB")
        img_byte_arr = io.BytesIO()
        pil_img.save(img_byte_arr, format='PNG')
        img_base = base64.b64encode(img_byte_arr.getvalue()).decode('utf-8')
        for i in range(retry_times):
            response = client.chat.completions.create(
                messages=[{"role": "system", "content": """
#ç¼–è¾‘æŒ‡ä»¤é‡å†™å™¨
ä½ æ˜¯ä¸€åä¸“ä¸šçš„ç¼–è¾‘æŒ‡ä»¤é‡å†™è€…ã€‚æ‚¨çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·æä¾›çš„æŒ‡ä»¤å’Œè¦ç¼–è¾‘çš„å›¾åƒç”Ÿæˆç²¾ç¡®ã€ç®€æ´ã€è§†è§‰ä¸Šå¯å®ç°çš„ä¸“ä¸šçº§ç¼–è¾‘æŒ‡ä»¤ã€‚  
è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹é‡å†™è§„åˆ™ï¼š
1.æ€»åˆ™
-ä¿æŒé‡å†™åçš„æç¤º**ç®€æ´**ã€‚é¿å…è¿‡é•¿çš„å¥å­ï¼Œå‡å°‘ä¸å¿…è¦çš„æè¿°æ€§è¯­è¨€ã€‚  
-å¦‚æœæŒ‡ç¤ºæ˜¯çŸ›ç›¾çš„ã€æ¨¡ç³Šçš„æˆ–æ— æ³•å®ç°çš„ï¼Œä¼˜å…ˆè€ƒè™‘åˆç†çš„æ¨ç†å’Œçº æ­£ï¼Œå¹¶åœ¨å¿…è¦æ—¶è¡¥å……ç»†èŠ‚ã€‚  
-ä¿æŒåŸè¯´æ˜ä¹¦çš„æ ¸å¿ƒæ„å›¾ä¸å˜ï¼Œåªä¼šå¢å¼ºå…¶æ¸…æ™°åº¦ã€åˆç†æ€§å’Œè§†è§‰å¯è¡Œæ€§ã€‚  
-æ‰€æœ‰æ·»åŠ çš„å¯¹è±¡æˆ–ä¿®æ”¹å¿…é¡»ä¸ç¼–è¾‘åçš„è¾“å…¥å›¾åƒçš„æ•´ä½“åœºæ™¯çš„é€»è¾‘å’Œé£æ ¼ä¿æŒä¸€è‡´ã€‚  
2.ä»»åŠ¡ç±»å‹å¤„ç†è§„åˆ™
1.æ·»åŠ ã€åˆ é™¤ã€æ›¿æ¢ä»»åŠ¡
-å¦‚æœæŒ‡ä»¤å¾ˆæ˜ç¡®ï¼ˆå·²ç»åŒ…æ‹¬ä»»åŠ¡ç±»å‹ã€ç›®æ ‡å®ä½“ã€ä½ç½®ã€æ•°é‡ã€å±æ€§ï¼‰ï¼Œè¯·ä¿ç•™åŸå§‹æ„å›¾ï¼Œåªç»†åŒ–è¯­æ³•ã€‚  
-å¦‚æœæè¿°æ¨¡ç³Šï¼Œè¯·è¡¥å……æœ€å°‘ä½†è¶³å¤Ÿçš„ç»†èŠ‚ï¼ˆç±»åˆ«ã€é¢œè‰²ã€å¤§å°ã€æ–¹å‘ã€ä½ç½®ç­‰ï¼‰ã€‚ä¾‹å¦‚ï¼š
>åŸæ–‡ï¼šâ€œæ·»åŠ åŠ¨ç‰©â€
>é‡å†™ï¼šâ€œåœ¨å³ä¸‹è§’æ·»åŠ ä¸€åªæµ…ç°è‰²çš„çŒ«ï¼Œåç€é¢å¯¹é•œå¤´â€
-åˆ é™¤æ— æ„ä¹‰çš„æŒ‡ä»¤ï¼šä¾‹å¦‚ï¼Œâ€œæ·»åŠ 0ä¸ªå¯¹è±¡â€åº”è¢«å¿½ç•¥æˆ–æ ‡è®°ä¸ºæ— æ•ˆã€‚  
-å¯¹äºæ›¿æ¢ä»»åŠ¡ï¼Œè¯·æŒ‡å®šâ€œç”¨Xæ›¿æ¢Yâ€ï¼Œå¹¶ç®€è¦æè¿°Xçš„ä¸»è¦è§†è§‰ç‰¹å¾ã€‚
2.æ–‡æœ¬ç¼–è¾‘ä»»åŠ¡
-æ‰€æœ‰æ–‡æœ¬å†…å®¹å¿…é¡»ç”¨è‹±æ–‡åŒå¼•å·â€œâ€æ‹¬èµ·æ¥ã€‚ä¸è¦ç¿»è¯‘æˆ–æ›´æ”¹æ–‡æœ¬çš„åŸå§‹è¯­è¨€ï¼Œä¹Ÿä¸è¦æ›´æ”¹å¤§å†™å­—æ¯ã€‚  
-**å¯¹äºæ–‡æœ¬æ›¿æ¢ä»»åŠ¡ï¼Œè¯·å§‹ç»ˆä½¿ç”¨å›ºå®šæ¨¡æ¿ï¼š**
-`å°†â€œxxâ€æ›¿æ¢ä¸ºâ€œyyâ€`ã€‚  
-`å°†xxè¾¹ç•Œæ¡†æ›¿æ¢ä¸ºâ€œyyâ€`ã€‚  
-å¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®šæ–‡æœ¬å†…å®¹ï¼Œåˆ™æ ¹æ®æŒ‡ä»¤å’Œè¾“å…¥å›¾åƒçš„ä¸Šä¸‹æ–‡æ¨æ–­å¹¶æ·»åŠ ç®€æ´çš„æ–‡æœ¬ã€‚ä¾‹å¦‚ï¼š
>åŸæ–‡ï¼šâ€œæ·»åŠ ä¸€è¡Œæ–‡å­—â€ï¼ˆæµ·æŠ¥ï¼‰
>é‡å†™ï¼šåœ¨é¡¶éƒ¨ä¸­å¿ƒæ·»åŠ æ–‡æœ¬â€œé™é‡ç‰ˆâ€ï¼Œå¹¶å¸¦æœ‰è½»å¾®é˜´å½±
-ä»¥ç®€æ´çš„æ–¹å¼æŒ‡å®šæ–‡æœ¬ä½ç½®ã€é¢œè‰²å’Œå¸ƒå±€ã€‚  
3.äººå·¥ç¼–è¾‘ä»»åŠ¡
-ä¿æŒäººçš„æ ¸å¿ƒè§†è§‰ä¸€è‡´æ€§ï¼ˆç§æ—ã€æ€§åˆ«ã€å¹´é¾„ã€å‘å‹ã€è¡¨æƒ…ã€æœè£…ç­‰ï¼‰ã€‚  
-å¦‚æœä¿®æ”¹å¤–è§‚ï¼ˆå¦‚è¡£æœã€å‘å‹ï¼‰ï¼Œè¯·ç¡®ä¿æ–°å…ƒç´ ä¸åŸå§‹é£æ ¼ä¸€è‡´ã€‚  
-**å¯¹äºè¡¨æƒ…å˜åŒ–ï¼Œå®ƒä»¬å¿…é¡»æ˜¯è‡ªç„¶å’Œå¾®å¦™çš„ï¼Œæ°¸è¿œä¸è¦å¤¸å¼ ã€‚**
-å¦‚æœä¸ç‰¹åˆ«å¼ºè°ƒåˆ é™¤ï¼Œåˆ™åº”ä¿ç•™åŸå§‹å›¾åƒä¸­æœ€é‡è¦çš„ä¸»é¢˜ï¼ˆä¾‹å¦‚ï¼Œäººã€åŠ¨ç‰©ï¼‰ã€‚
-å¯¹äºèƒŒæ™¯æ›´æ”¹ä»»åŠ¡ï¼Œé¦–å…ˆè¦å¼ºè°ƒä¿æŒä¸»é¢˜çš„ä¸€è‡´æ€§ã€‚  
-ç¤ºä¾‹ï¼š
>åŸæ–‡ï¼šâ€œæ›´æ¢äººçš„å¸½å­â€
>æ”¹å†™ï¼šâ€œç”¨æ·±æ£•è‰²è´é›·å¸½ä»£æ›¿ç”·å£«çš„å¸½å­ï¼›ä¿æŒå¾®ç¬‘ã€çŸ­å‘å’Œç°è‰²å¤¹å…‹ä¸å˜â€
4.é£æ ¼è½¬æ¢æˆ–å¢å¼ºä»»åŠ¡
-å¦‚æœæŒ‡å®šäº†ä¸€ç§é£æ ¼ï¼Œè¯·ç”¨å…³é”®çš„è§†è§‰ç‰¹å¾ç®€æ´åœ°æè¿°å®ƒã€‚ä¾‹å¦‚ï¼š
>åŸåˆ›ï¼šâ€œè¿ªæ–¯ç§‘é£æ ¼â€
>æ”¹å†™ï¼šâ€œ20ä¸–çºª70å¹´ä»£çš„è¿ªæ–¯ç§‘ï¼šé—ªçƒçš„ç¯å…‰ã€è¿ªæ–¯ç§‘çƒã€é•œé¢å¢™ã€å¤šå½©çš„è‰²è°ƒâ€
-å¦‚æœæŒ‡ä»¤è¯´â€œä½¿ç”¨å‚è€ƒé£æ ¼â€æˆ–â€œä¿æŒå½“å‰é£æ ¼â€ï¼Œåˆ™åˆ†æè¾“å…¥å›¾åƒï¼Œæå–ä¸»è¦ç‰¹å¾ï¼ˆé¢œè‰²ã€æ„å›¾ã€çº¹ç†ã€ç…§æ˜ã€è‰ºæœ¯é£æ ¼ï¼‰ï¼Œå¹¶ç®€æ´åœ°æ•´åˆå®ƒä»¬ã€‚  
-**å¯¹äºç€è‰²ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ¢å¤æ—§ç…§ç‰‡ï¼Œå§‹ç»ˆä½¿ç”¨å›ºå®šæ¨¡æ¿ï¼š**â€œæ¢å¤æ—§ç…§ç‰‡ã€å»é™¤åˆ’ç—•ã€å‡å°‘å™ªéŸ³ã€å¢å¼ºç»†èŠ‚ã€é«˜åˆ†è¾¨ç‡ã€é€¼çœŸã€è‡ªç„¶çš„è‚¤è‰²ã€æ¸…æ™°çš„é¢éƒ¨ç‰¹å¾ã€æ— å¤±çœŸã€å¤å¤ç…§ç‰‡æ¢å¤â€
-å¦‚æœè¿˜æœ‰å…¶ä»–æ›´æ”¹ï¼Œè¯·å°†æ ·å¼æè¿°æ”¾åœ¨æœ«å°¾ã€‚
3.åˆç†æ€§å’Œé€»è¾‘æ£€æŸ¥
-è§£å†³ç›¸äº’çŸ›ç›¾çš„æŒ‡ç¤ºï¼šä¾‹å¦‚ï¼Œâ€œåˆ é™¤æ‰€æœ‰æ ‘ä½†ä¿ç•™æ‰€æœ‰æ ‘â€åº”åœ¨é€»è¾‘ä¸Šå¾—åˆ°çº æ­£ã€‚  
-æ·»åŠ ç¼ºå¤±çš„å…³é”®ä¿¡æ¯ï¼šå¦‚æœä½ç½®æœªæŒ‡å®šï¼Œè¯·æ ¹æ®æ„å›¾é€‰æ‹©åˆç†çš„åŒºåŸŸï¼ˆé è¿‘ä¸»ä½“ã€ç©ºç™½ã€ä¸­å¿ƒ/è¾¹ç¼˜ï¼‰ã€‚  
"""
                },
                {
                    "role": "user",
                    "content":  [
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": img_base
                            }
                        },
                        {
                            "type": "text",
                            "text": f"{text}",
                        }
                    ]
                },
                ],
                model=model_name,
                temperature=temperature,
                top_p=top_p,
                stream=False,
                max_tokens=max_tokens,
            )
            if response.choices:
                return response.choices[0].message.content.replace('"', '').replace('<|begin_of_box|>', '').replace('<|end_of_box|>', ''), "âœ… åæ¨æç¤ºè¯å®Œæ¯•"
    except Exception as e:
        return prompt, f"APIè°ƒç”¨å¼‚å¸¸ï¼š{str(e)}"
    

def enhance_prompt_edit2(prompt, image=None, image2=None, retry_times=3):
    if isinstance(image, dict):
        image = image["background"]
    if openai_api_key == "":
        return prompt, "è¯·åœ¨è®¾ç½®ä¸­ï¼Œå¡«å†™APIç›¸å…³ä¿¡æ¯å¹¶ä¿å­˜"
    try:
        client = OpenAI(
            base_url = openai_base_url,
            api_key = openai_api_key,
        )
        text = prompt.strip()
        pil_img = image.convert("RGB")
        img_byte_arr = io.BytesIO()
        pil_img.save(img_byte_arr, format='PNG')
        img_base = base64.b64encode(img_byte_arr.getvalue()).decode('utf-8')
        pil_img2 = image2.convert("RGB")
        img_byte_arr2 = io.BytesIO()
        pil_img2.save(img_byte_arr2, format='PNG')
        img_base2 = base64.b64encode(img_byte_arr2.getvalue()).decode('utf-8')
        for i in range(retry_times):
            response = client.chat.completions.create(
                messages=[{"role": "system", "content": """
#ç¼–è¾‘æŒ‡ä»¤é‡å†™å™¨
ä½ æ˜¯ä¸€åä¸“ä¸šçš„ç¼–è¾‘æŒ‡ä»¤é‡å†™è€…ã€‚æ‚¨çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·æä¾›çš„æŒ‡ä»¤å’Œè¦ç¼–è¾‘çš„å›¾åƒç”Ÿæˆç²¾ç¡®ã€ç®€æ´ã€è§†è§‰ä¸Šå¯å®ç°çš„ä¸“ä¸šçº§ç¼–è¾‘æŒ‡ä»¤ã€‚  
è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹é‡å†™è§„åˆ™ï¼š
1.æ€»åˆ™
-ä¿æŒé‡å†™åçš„æç¤º**ç®€æ´**ã€‚é¿å…è¿‡é•¿çš„å¥å­ï¼Œå‡å°‘ä¸å¿…è¦çš„æè¿°æ€§è¯­è¨€ã€‚  
-å¦‚æœæŒ‡ç¤ºæ˜¯çŸ›ç›¾çš„ã€æ¨¡ç³Šçš„æˆ–æ— æ³•å®ç°çš„ï¼Œä¼˜å…ˆè€ƒè™‘åˆç†çš„æ¨ç†å’Œçº æ­£ï¼Œå¹¶åœ¨å¿…è¦æ—¶è¡¥å……ç»†èŠ‚ã€‚  
-ä¿æŒåŸè¯´æ˜ä¹¦çš„æ ¸å¿ƒæ„å›¾ä¸å˜ï¼Œåªä¼šå¢å¼ºå…¶æ¸…æ™°åº¦ã€åˆç†æ€§å’Œè§†è§‰å¯è¡Œæ€§ã€‚  
-æ‰€æœ‰æ·»åŠ çš„å¯¹è±¡æˆ–ä¿®æ”¹å¿…é¡»ä¸ç¼–è¾‘åçš„è¾“å…¥å›¾åƒçš„æ•´ä½“åœºæ™¯çš„é€»è¾‘å’Œé£æ ¼ä¿æŒä¸€è‡´ã€‚  
2.ä»»åŠ¡ç±»å‹å¤„ç†è§„åˆ™
1.æ·»åŠ ã€åˆ é™¤ã€æ›¿æ¢ä»»åŠ¡
-å¦‚æœæŒ‡ä»¤å¾ˆæ˜ç¡®ï¼ˆå·²ç»åŒ…æ‹¬ä»»åŠ¡ç±»å‹ã€ç›®æ ‡å®ä½“ã€ä½ç½®ã€æ•°é‡ã€å±æ€§ï¼‰ï¼Œè¯·ä¿ç•™åŸå§‹æ„å›¾ï¼Œåªç»†åŒ–è¯­æ³•ã€‚  
-å¦‚æœæè¿°æ¨¡ç³Šï¼Œè¯·è¡¥å……æœ€å°‘ä½†è¶³å¤Ÿçš„ç»†èŠ‚ï¼ˆç±»åˆ«ã€é¢œè‰²ã€å¤§å°ã€æ–¹å‘ã€ä½ç½®ç­‰ï¼‰ã€‚ä¾‹å¦‚ï¼š
>åŸæ–‡ï¼šâ€œæ·»åŠ åŠ¨ç‰©â€
>é‡å†™ï¼šâ€œåœ¨å³ä¸‹è§’æ·»åŠ ä¸€åªæµ…ç°è‰²çš„çŒ«ï¼Œåç€é¢å¯¹é•œå¤´â€
-åˆ é™¤æ— æ„ä¹‰çš„æŒ‡ä»¤ï¼šä¾‹å¦‚ï¼Œâ€œæ·»åŠ 0ä¸ªå¯¹è±¡â€åº”è¢«å¿½ç•¥æˆ–æ ‡è®°ä¸ºæ— æ•ˆã€‚  
-å¯¹äºæ›¿æ¢ä»»åŠ¡ï¼Œè¯·æŒ‡å®šâ€œç”¨Xæ›¿æ¢Yâ€ï¼Œå¹¶ç®€è¦æè¿°Xçš„ä¸»è¦è§†è§‰ç‰¹å¾ã€‚
2.æ–‡æœ¬ç¼–è¾‘ä»»åŠ¡
-æ‰€æœ‰æ–‡æœ¬å†…å®¹å¿…é¡»ç”¨è‹±æ–‡åŒå¼•å·â€œâ€æ‹¬èµ·æ¥ã€‚ä¸è¦ç¿»è¯‘æˆ–æ›´æ”¹æ–‡æœ¬çš„åŸå§‹è¯­è¨€ï¼Œä¹Ÿä¸è¦æ›´æ”¹å¤§å†™å­—æ¯ã€‚  
-**å¯¹äºæ–‡æœ¬æ›¿æ¢ä»»åŠ¡ï¼Œè¯·å§‹ç»ˆä½¿ç”¨å›ºå®šæ¨¡æ¿ï¼š**
-`å°†â€œxxâ€æ›¿æ¢ä¸ºâ€œyyâ€`ã€‚  
-`å°†xxè¾¹ç•Œæ¡†æ›¿æ¢ä¸ºâ€œyyâ€`ã€‚  
-å¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®šæ–‡æœ¬å†…å®¹ï¼Œåˆ™æ ¹æ®æŒ‡ä»¤å’Œè¾“å…¥å›¾åƒçš„ä¸Šä¸‹æ–‡æ¨æ–­å¹¶æ·»åŠ ç®€æ´çš„æ–‡æœ¬ã€‚ä¾‹å¦‚ï¼š
>åŸæ–‡ï¼šâ€œæ·»åŠ ä¸€è¡Œæ–‡å­—â€ï¼ˆæµ·æŠ¥ï¼‰
>é‡å†™ï¼šåœ¨é¡¶éƒ¨ä¸­å¿ƒæ·»åŠ æ–‡æœ¬â€œé™é‡ç‰ˆâ€ï¼Œå¹¶å¸¦æœ‰è½»å¾®é˜´å½±
-ä»¥ç®€æ´çš„æ–¹å¼æŒ‡å®šæ–‡æœ¬ä½ç½®ã€é¢œè‰²å’Œå¸ƒå±€ã€‚  
3.äººå·¥ç¼–è¾‘ä»»åŠ¡
-ä¿æŒäººçš„æ ¸å¿ƒè§†è§‰ä¸€è‡´æ€§ï¼ˆç§æ—ã€æ€§åˆ«ã€å¹´é¾„ã€å‘å‹ã€è¡¨æƒ…ã€æœè£…ç­‰ï¼‰ã€‚  
-å¦‚æœä¿®æ”¹å¤–è§‚ï¼ˆå¦‚è¡£æœã€å‘å‹ï¼‰ï¼Œè¯·ç¡®ä¿æ–°å…ƒç´ ä¸åŸå§‹é£æ ¼ä¸€è‡´ã€‚  
-**å¯¹äºè¡¨æƒ…å˜åŒ–ï¼Œå®ƒä»¬å¿…é¡»æ˜¯è‡ªç„¶å’Œå¾®å¦™çš„ï¼Œæ°¸è¿œä¸è¦å¤¸å¼ ã€‚**
-å¦‚æœä¸ç‰¹åˆ«å¼ºè°ƒåˆ é™¤ï¼Œåˆ™åº”ä¿ç•™åŸå§‹å›¾åƒä¸­æœ€é‡è¦çš„ä¸»é¢˜ï¼ˆä¾‹å¦‚ï¼Œäººã€åŠ¨ç‰©ï¼‰ã€‚
-å¯¹äºèƒŒæ™¯æ›´æ”¹ä»»åŠ¡ï¼Œé¦–å…ˆè¦å¼ºè°ƒä¿æŒä¸»é¢˜çš„ä¸€è‡´æ€§ã€‚  
-ç¤ºä¾‹ï¼š
>åŸæ–‡ï¼šâ€œæ›´æ¢äººçš„å¸½å­â€
>æ”¹å†™ï¼šâ€œç”¨æ·±æ£•è‰²è´é›·å¸½ä»£æ›¿ç”·å£«çš„å¸½å­ï¼›ä¿æŒå¾®ç¬‘ã€çŸ­å‘å’Œç°è‰²å¤¹å…‹ä¸å˜â€
4.é£æ ¼è½¬æ¢æˆ–å¢å¼ºä»»åŠ¡
-å¦‚æœæŒ‡å®šäº†ä¸€ç§é£æ ¼ï¼Œè¯·ç”¨å…³é”®çš„è§†è§‰ç‰¹å¾ç®€æ´åœ°æè¿°å®ƒã€‚ä¾‹å¦‚ï¼š
>åŸåˆ›ï¼šâ€œè¿ªæ–¯ç§‘é£æ ¼â€
>æ”¹å†™ï¼šâ€œ20ä¸–çºª70å¹´ä»£çš„è¿ªæ–¯ç§‘ï¼šé—ªçƒçš„ç¯å…‰ã€è¿ªæ–¯ç§‘çƒã€é•œé¢å¢™ã€å¤šå½©çš„è‰²è°ƒâ€
-å¦‚æœæŒ‡ä»¤è¯´â€œä½¿ç”¨å‚è€ƒé£æ ¼â€æˆ–â€œä¿æŒå½“å‰é£æ ¼â€ï¼Œåˆ™åˆ†æè¾“å…¥å›¾åƒï¼Œæå–ä¸»è¦ç‰¹å¾ï¼ˆé¢œè‰²ã€æ„å›¾ã€çº¹ç†ã€ç…§æ˜ã€è‰ºæœ¯é£æ ¼ï¼‰ï¼Œå¹¶ç®€æ´åœ°æ•´åˆå®ƒä»¬ã€‚  
-**å¯¹äºç€è‰²ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ¢å¤æ—§ç…§ç‰‡ï¼Œå§‹ç»ˆä½¿ç”¨å›ºå®šæ¨¡æ¿ï¼š**â€œæ¢å¤æ—§ç…§ç‰‡ã€å»é™¤åˆ’ç—•ã€å‡å°‘å™ªéŸ³ã€å¢å¼ºç»†èŠ‚ã€é«˜åˆ†è¾¨ç‡ã€é€¼çœŸã€è‡ªç„¶çš„è‚¤è‰²ã€æ¸…æ™°çš„é¢éƒ¨ç‰¹å¾ã€æ— å¤±çœŸã€å¤å¤ç…§ç‰‡æ¢å¤â€
-å¦‚æœè¿˜æœ‰å…¶ä»–æ›´æ”¹ï¼Œè¯·å°†æ ·å¼æè¿°æ”¾åœ¨æœ«å°¾ã€‚
3.åˆç†æ€§å’Œé€»è¾‘æ£€æŸ¥
-è§£å†³ç›¸äº’çŸ›ç›¾çš„æŒ‡ç¤ºï¼šä¾‹å¦‚ï¼Œâ€œåˆ é™¤æ‰€æœ‰æ ‘ä½†ä¿ç•™æ‰€æœ‰æ ‘â€åº”åœ¨é€»è¾‘ä¸Šå¾—åˆ°çº æ­£ã€‚  
-æ·»åŠ ç¼ºå¤±çš„å…³é”®ä¿¡æ¯ï¼šå¦‚æœä½ç½®æœªæŒ‡å®šï¼Œè¯·æ ¹æ®æ„å›¾é€‰æ‹©åˆç†çš„åŒºåŸŸï¼ˆé è¿‘ä¸»ä½“ã€ç©ºç™½ã€ä¸­å¿ƒ/è¾¹ç¼˜ï¼‰ã€‚  
"""
                },
                {
                    "role": "user",
                    "content":  [
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": img_base
                            }
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": img_base2
                            }
                        },
                        {
                            "type": "text",
                            "text": f"{text}",
                        }
                    ]
                },
                ],
                model=model_name,
                temperature=temperature,
                top_p=top_p,
                stream=False,
                max_tokens=max_tokens,
            )
            if response.choices:
                return response.choices[0].message.content.replace('"', '').replace('<|begin_of_box|>', '').replace('<|end_of_box|>', ''), "âœ… åæ¨æç¤ºè¯å®Œæ¯•"
    except Exception as e:
        return prompt, f"APIè°ƒç”¨å¼‚å¸¸ï¼š{str(e)}"
    

def modelscope_generate(
    mode,
    prompt,
    negative_prompt,
    width,
    height,
    batch_images, 
    seed_param,
    transformer_dropdown,
    image=None, 
    mask_image=None, 
    strength=None,
    size_edit2=None, 
    reserve_edit2=None,
):
    global stop_generation
    num_inference_steps = 50  
    true_cfg_scale = 4.0 
    results = []
    resolutions = [
        (928, 1664),
        (1104, 1472),
        (1328, 1328),
        (1472, 1104),
        (1664, 928)
    ]
    if image:
        pil_img = image.convert("RGB")
        filename = f"outputs/temp.png"
        pil_img.save(filename, format='PNG')
        mime_type, _ = mimetypes.guess_type(filename)
        if not mime_type or not mime_type.startswith("image/"):
            raise ValueError("ä¸æ”¯æŒæˆ–æ— æ³•è¯†åˆ«çš„å›¾åƒæ ¼å¼")
        with open(filename, "rb") as image_file:
            encoded_string = base64.b64encode(image_file.read()).decode('utf-8')
        width, height = load_image(pil_img).size
    min_distance = float('inf') # åˆå§‹åŒ–æœ€å°è·ç¦»ä¸ºæ­£æ— ç©·å¤§
    for res_width, res_height in resolutions:
        # ä½¿ç”¨æ¬§å‡ é‡Œå¾—è·ç¦»è®¡ç®—ç›¸ä¼¼åº¦
        distance = ((width - res_width) ** 2 + (height - res_height) ** 2) ** 0.5
        if distance < min_distance:
            min_distance = distance
            closest_resolution = (res_width, res_height)
    width, height = closest_resolution[0], closest_resolution[1]
    if seed_param < 0:
        seed = random.randint(0, np.iinfo(np.int32).max)
    else:
        seed = seed_param
    base_url = 'https://api-inference.modelscope.cn/'
    common_headers = {
        "Authorization": f"Bearer {modelscope_api_key}",
        "Content-Type": "application/json",
    }
    for i in range(batch_images):
        if stop_generation:
            stop_generation = False
            yield results, seed + i, "âœ… ç”Ÿæˆå·²ä¸­æ­¢"
            break
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"outputs/{timestamp}.png"
        pnginfo = PngImagePlugin.PngInfo()
        pnginfo.add_text("mode", f"{mode}\n")
        if image:
            pnginfo.add_text("image", f"{str(image)}\n")
        pnginfo.add_text("prompt", f"{prompt}\n")
        pnginfo.add_text("negative_prompt", f"{negative_prompt}\n")
        pnginfo.add_text("num_inference_steps", f"{str(num_inference_steps)}\n")
        pnginfo.add_text("true_cfg_scale", f"{str(true_cfg_scale)}\n")
        pnginfo.add_text("seed", f"{str(seed + i)}\n")
        pnginfo.add_text("models", f"{transformer_dropdown}\n")
        """lora_str = ", ".join(lora_dropdown) if lora_dropdown else ""
        pnginfo.add_text("lora", f"{lora_str}\n")
        lora_weights_str = ", ".join(lora_weights) if lora_weights else ""
        pnginfo.add_text("lora_weights", f"{lora_weights_str}\n")
        if strength:
            pnginfo.add_text("strength", f"{str(strength)}\n")"""
        if mode == "t2i_ms":
            response = requests.post(
                f"{base_url}v1/images/generations",
                headers={**common_headers, "X-ModelScope-Async-Mode": "true"},
                data=json.dumps({
                    "model": "Qwen/Qwen-Image", # ModelScope Model-Id, required
                    "prompt": prompt,
                    "negative_prompt": negative_prompt,
                    "num_inference_steps": num_inference_steps,
                    "true_cfg_scale": true_cfg_scale,
                    "size": f"{width}x{height}",
                    "seed": seed + i,
                }, ensure_ascii=False).encode('utf-8')
            )
        elif mode == "edit_ms":
            response = requests.post(
                f"{base_url}v1/images/generations",
                headers={**common_headers, "X-ModelScope-Async-Mode": "true"},
                data=json.dumps({
                    "model": "Qwen/Qwen-Image-Edit", # ModelScope Model-Id, required
                    "image_url": f"{encoded_string}",
                    "prompt": prompt,
                    "negative_prompt": negative_prompt,
                    "num_inference_steps": num_inference_steps,
                    "true_cfg_scale": true_cfg_scale,
                    "size": f"{width}x{height}",
                    "seed": seed + i,
                }, ensure_ascii=False).encode('utf-8')
            )
        response.raise_for_status()
        task_id = response.json()["task_id"]
        while True:
            result = requests.get(
                f"{base_url}v1/tasks/{task_id}",
                headers={**common_headers, "X-ModelScope-Task-Type": "image_generation"},
            )
            result.raise_for_status()
            data = result.json()
            if data["task_status"] == "SUCCEED":
                image = Image.open(io.BytesIO(requests.get(data["output_images"][0]).content))
                image.save("result_image.jpg")
                break
            elif data["task_status"] == "FAILED":
                print("Image Generation Failed.")
                break
            time.sleep(1)
        image.save(filename, pnginfo=pnginfo)
        results.append(image)
        yield results, f"ç§å­æ•°{seed+i}ï¼Œä¿å­˜åœ°å€{filename}"

def exchange_width_height(width, height):
    return height, width, "âœ… å®½é«˜äº¤æ¢å®Œæ¯•"


def adjust_width_height(image):
    if isinstance(image, dict):
        image = image["background"]
    image = load_image(image)
    width, height = image.size
    original_area = width * height
    default_area = 1328*1328
    ratio = math.sqrt(original_area / default_area)
    width = width / ratio // 16 * 16
    height = height / ratio // 16 * 16
    return int(width), int(height), "âœ… æ ¹æ®å›¾ç‰‡è°ƒæ•´å®½é«˜"


def stop_generate():
    global stop_generation
    stop_generation = True
    return "ğŸ›‘ ç­‰å¾…ç”Ÿæˆä¸­æ­¢"


def _generate_common(
    mode, 
    prompt, 
    negative_prompt, 
    width, 
    height, 
    num_inference_steps, 
    batch_images, 
    true_cfg_scale, 
    seed_param, 
    transformer_dropdown, 
    lora_dropdown, 
    lora_weights, 
    max_vram, 
    image=None, 
    mask_image=None, 
    strength=None,
    size_edit2=None, 
    reserve_edit2=None,
):
    global mode_loaded, prompt_cache, negative_prompt_cache, stop_generation, prompt_embeds, prompt_embeds_mask, negative_prompt_embeds, negative_prompt_embeds_mask, image_loaded
    results = []
    if seed_param < 0:
        seed = random.randint(0, np.iinfo(np.int32).max)
    else:
        seed = seed_param
    if mode in ["edit", "edit2", "editinp", "editplus"]:
        if width:
            image_width = round(image.size[0] / image.size[1] * height / 32) * 32
        ratio = image.size[0] / image.size[1]
        calculated_width = math.sqrt(1024*1024 * ratio)
        calculated_height = calculated_width / ratio
        calculated_width = round(calculated_width / 32) * 32
        calculated_height = round(calculated_height / 32) * 32
        image_processor = VaeImageProcessor(vae_scale_factor=16)
        calculated_image = image_processor.resize(image, calculated_height, calculated_width)
    if (mode != mode_loaded or prompt_cache != prompt or negative_prompt_cache != negative_prompt or 
        transformer_loaded != transformer_dropdown or lora_loaded != lora_dropdown or
          lora_loaded_weights != lora_weights or image_loaded!=image):
        load_model(mode, transformer_dropdown, lora_dropdown, lora_weights, max_vram)
        prompt_cache, negative_prompt_cache, image_loaded = prompt, negative_prompt, image
        if mode == "t2i" or mode == "i2i" or mode == "inp" or mode == "con":
            prompt_embeds, prompt_embeds_mask = pipe.encode_prompt(prompt)
            negative_prompt_embeds, negative_prompt_embeds_mask = pipe.encode_prompt(negative_prompt)
        elif mode in ["edit", "edit2", "editinp", "editplus"]:
            prompt_embeds, prompt_embeds_mask = pipe.encode_prompt(prompt, calculated_image)
            negative_prompt_embeds, negative_prompt_embeds_mask = pipe.encode_prompt(negative_prompt, calculated_image)
    for i in range(batch_images):
        if stop_generation:
            stop_generation = False
            yield results, f"âœ… ç”Ÿæˆå·²ä¸­æ­¢ï¼Œæœ€åç§å­æ•°{seed+i-1}"
            break
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"outputs/{timestamp}.png"
        pnginfo = PngImagePlugin.PngInfo()
        pnginfo.add_text("mode", f"{mode}\n")
        if image:
            pnginfo.add_text("image", f"{str(image)}\n")
        pnginfo.add_text("prompt", f"{prompt}\n")
        pnginfo.add_text("negative_prompt", f"{negative_prompt}\n")
        if width and mode != "edit2":
            pnginfo.add_text("width", f"{str(width)}\n")
        if height:
            pnginfo.add_text("height", f"{str(height)}\n")
        pnginfo.add_text("num_inference_steps", f"{str(num_inference_steps)}\n")
        pnginfo.add_text("true_cfg_scale", f"{str(true_cfg_scale)}\n")
        pnginfo.add_text("seed", f"{str(seed + i)}\n")
        pnginfo.add_text("models", f"{transformer_dropdown}\n")
        lora_str = ", ".join(lora_dropdown) if lora_dropdown else ""
        pnginfo.add_text("lora", f"{lora_str}\n")
        lora_weights_str = ", ".join(lora_weights) if lora_weights else ""
        pnginfo.add_text("lora_weights", f"{lora_weights_str}\n")
        if strength:
            pnginfo.add_text("strength", f"{str(strength)}\n")
        with torch.no_grad():
            if mode == "t2i":
                output = pipe(
                    width=width,
                    height=height,
                    num_inference_steps=num_inference_steps,
                    true_cfg_scale=true_cfg_scale,
                    prompt_embeds=prompt_embeds,
                    prompt_embeds_mask=prompt_embeds_mask,
                    negative_prompt_embeds=negative_prompt_embeds,
                    negative_prompt_embeds_mask=negative_prompt_embeds_mask,
                    generator=torch.Generator().manual_seed(seed + i),
                )
            elif mode == "i2i":
                output = pipe(
                    image=image,
                    width=width,
                    height=height,
                    num_inference_steps=num_inference_steps,
                    strength=strength,
                    true_cfg_scale=true_cfg_scale,
                    prompt_embeds=prompt_embeds,
                    prompt_embeds_mask=prompt_embeds_mask,
                    negative_prompt_embeds=negative_prompt_embeds,
                    negative_prompt_embeds_mask=negative_prompt_embeds_mask,
                    generator=torch.Generator().manual_seed(seed + i),
                )
            elif mode == "inp":
                output = pipe(
                    image=image,
                    mask_image=mask_image,
                    width=width,
                    height=height,
                    num_inference_steps=num_inference_steps,
                    strength=strength,
                    true_cfg_scale=true_cfg_scale,
                    prompt_embeds=prompt_embeds,
                    prompt_embeds_mask=prompt_embeds_mask,
                    negative_prompt_embeds=negative_prompt_embeds,
                    negative_prompt_embeds_mask=negative_prompt_embeds_mask,
                    generator=torch.Generator().manual_seed(seed + i),
                )
            elif mode == "con":
                output = pipe(
                    control_image=image,
                    width=width,
                    height=height,
                    num_inference_steps=num_inference_steps,
                    controlnet_conditioning_scale=strength,
                    true_cfg_scale=true_cfg_scale,
                    prompt_embeds=prompt_embeds,
                    prompt_embeds_mask=prompt_embeds_mask,
                    negative_prompt_embeds=negative_prompt_embeds,
                    negative_prompt_embeds_mask=negative_prompt_embeds_mask,
                    generator=torch.Generator().manual_seed(seed + i),
                )
            elif mode == "edit":
                output = pipe(
                    image=calculated_image,
                    num_inference_steps=num_inference_steps,
                    true_cfg_scale=true_cfg_scale,
                    prompt_embeds=prompt_embeds,
                    prompt_embeds_mask=prompt_embeds_mask,
                    negative_prompt_embeds=negative_prompt_embeds,
                    negative_prompt_embeds_mask=negative_prompt_embeds_mask,
                    generator=torch.Generator().manual_seed(seed + i),
                )
            elif mode == "edit2":
                output = pipe(
                    image=calculated_image,
                    width=image_width if size_edit2!="å°å°ºå¯¸" else None,
                    height=height if size_edit2!="å°å°ºå¯¸" else None,
                    num_inference_steps=num_inference_steps,
                    true_cfg_scale=true_cfg_scale,
                    prompt_embeds=prompt_embeds,
                    prompt_embeds_mask=prompt_embeds_mask,
                    negative_prompt_embeds=negative_prompt_embeds,
                    negative_prompt_embeds_mask=negative_prompt_embeds_mask,
                    generator=torch.Generator().manual_seed(seed + i),
                )
                if reserve_edit2=="ä¿ç•™ä¸»ä½“" and size_edit2=="å¤§å°ºå¯¸":
                    output.images[0] = output.images[0].crop((0, 0, width, output.images[0].height))
                elif reserve_edit2=="ä¿ç•™ä¸»ä½“" and size_edit2=="å°å°ºå¯¸":
                    reserve_width = round(width * calculated_height / height)
                    output.images[0] = output.images[0].crop((0, 0, reserve_width, output.images[0].height))
            elif mode == "editinp":
                output = pipe(
                    image=image,
                    mask_image=mask_image,
                    num_inference_steps=num_inference_steps,
                    strength=strength,
                    true_cfg_scale=true_cfg_scale,
                    prompt_embeds=prompt_embeds,
                    prompt_embeds_mask=prompt_embeds_mask,
                    negative_prompt_embeds=negative_prompt_embeds,
                    negative_prompt_embeds_mask=negative_prompt_embeds_mask,
                    generator=torch.Generator().manual_seed(seed + i),
                )
            elif mode == "editplus":
                output = pipe(
                    image=image,
                    num_inference_steps=num_inference_steps,
                    true_cfg_scale=true_cfg_scale,
                    guidance_scale=1.0,
                    prompt=prompt,
                    negative_prompt=negative_prompt,
                    generator=torch.Generator().manual_seed(seed + i),
                )
        image = output.images[0]
        image.save(filename, pnginfo=pnginfo)
        results.append(image)
        yield results, f"ç§å­æ•°{seed+i}ï¼Œä¿å­˜åœ°å€{filename}"
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()


def generate_t2i(prompt, negative_prompt, width, height, num_inference_steps, 
                 batch_images, true_cfg_scale, seed_param, transformer_dropdown, 
                 lora_dropdown, lora_weights, max_vram):
    if "ModelScope" in transformer_dropdown:
        yield from modelscope_generate(
            mode="t2i_ms",
            prompt=prompt,
            negative_prompt=negative_prompt,
            width=width,
            height=height,
            batch_images=batch_images, 
            seed_param=seed_param,
            transformer_dropdown=transformer_dropdown,
        )
    else:
        yield from _generate_common(
            mode="t2i",
            prompt=prompt,
            negative_prompt=negative_prompt,
            width=width,
            height=height,
            num_inference_steps=num_inference_steps,
            batch_images=batch_images,
            true_cfg_scale=true_cfg_scale,
            seed_param=seed_param,
            transformer_dropdown=transformer_dropdown,
            lora_dropdown=lora_dropdown,
            lora_weights=lora_weights,
            max_vram=max_vram
        )


def generate_i2i(image, prompt, negative_prompt, width, height, num_inference_steps,
                 strength, batch_images, true_cfg_scale, seed_param, transformer_dropdown, 
                 lora_dropdown, lora_weights, max_vram):
    image = load_image(image)
    yield from _generate_common(
        mode="i2i",
        image=image,
        prompt=prompt,
        negative_prompt=negative_prompt,
        width=width,
        height=height,
        num_inference_steps=num_inference_steps,
        strength=strength,
        batch_images=batch_images,
        true_cfg_scale=true_cfg_scale,
        seed_param=seed_param,
        transformer_dropdown=transformer_dropdown,
        lora_dropdown=lora_dropdown,
        lora_weights=lora_weights,
        max_vram=max_vram
    )


def generate_inp(image, prompt, negative_prompt, width, height, num_inference_steps,
                 strength, batch_images, true_cfg_scale, seed_param, transformer_dropdown,
                 lora_dropdown, lora_weights, max_vram):
    # å¤„ç†è’™ç‰ˆå›¾åƒ
    mask_image = image["layers"][0]
    mask_image = mask_image .convert("RGBA")
    data = np.array(mask_image)
    # ä¿®æ”¹è’™ç‰ˆé¢œè‰²ï¼ˆé»‘è‰²->ç™½è‰²ï¼Œé€æ˜->é»‘è‰²ï¼‰
    black_pixels = (data[:, :, 0] == 0) & (data[:, :, 1] == 0) & (data[:, :, 2] == 0)
    data[black_pixels, :3] = [255, 255, 255]
    transparent_pixels = (data[:, :, 3] == 0)
    data[transparent_pixels, :3] = [0, 0, 0]
    mask_image = Image.fromarray(data)
    # æå–èƒŒæ™¯å›¾åƒ
    background_image = load_image(image["background"])
    yield from _generate_common(
        mode="inp",
        image=background_image,
        mask_image=mask_image,
        prompt=prompt,
        negative_prompt=negative_prompt,
        width=width,
        height=height,
        num_inference_steps=num_inference_steps,
        strength=strength,
        batch_images=batch_images,
        true_cfg_scale=true_cfg_scale,
        seed_param=seed_param,
        transformer_dropdown=transformer_dropdown,
        lora_dropdown=lora_dropdown,
        lora_weights=lora_weights,
        max_vram=max_vram
    )


def generate_con(image, prompt, negative_prompt, width, height, num_inference_steps,
                 strength, batch_images, true_cfg_scale, seed_param, transformer_dropdown, 
                 lora_dropdown, lora_weights, max_vram):
    image = load_image(image)
    yield from _generate_common(
        mode="con",
        image=image,
        prompt=prompt,
        negative_prompt=negative_prompt,
        width=width,
        height=height,
        num_inference_steps=num_inference_steps,
        strength=strength,
        batch_images=batch_images,
        true_cfg_scale=true_cfg_scale,
        seed_param=seed_param,
        transformer_dropdown=transformer_dropdown,
        lora_dropdown=lora_dropdown,
        lora_weights=lora_weights,
        max_vram=max_vram
    )


def generate_edit(image, prompt, negative_prompt, num_inference_steps,
                  batch_images, true_cfg_scale, seed_param, transformer_dropdown,
                  lora_dropdown, lora_weights, max_vram):
    if "ModelScope" in transformer_dropdown:
        yield from modelscope_generate(
            mode="edit_ms",
            prompt=prompt,
            negative_prompt=negative_prompt,
            width=None,
            height=None,
            batch_images=batch_images, 
            seed_param=seed_param,
            transformer_dropdown=transformer_dropdown,
            image=image,
        )
    else:
        image = image.convert("RGBA")
        white_bg = Image.new("RGB", image.size, (255, 255, 255))
        # ä½¿ç”¨alphaé€šé“ä½œä¸ºæ©ç è¿›è¡Œç²˜è´´
        white_bg.paste(image, mask=image.split()[3])
        # å¦‚æœéœ€è¦ä¿å­˜ä¸ºRGBæ¨¡å¼çš„å›¾åƒ
        image = white_bg.convert("RGB")
        yield from _generate_common(
            mode="edit",
            image=image,
            prompt=prompt,
            negative_prompt=negative_prompt,
            width=None,
            height=None,
            num_inference_steps=num_inference_steps,
            batch_images=batch_images,
            true_cfg_scale=true_cfg_scale,
            seed_param=seed_param,
            transformer_dropdown=transformer_dropdown, 
            lora_dropdown=lora_dropdown,
            lora_weights=lora_weights,
            max_vram=max_vram
        )


def generate_edit2(image, image2, prompt, negative_prompt, num_inference_steps,
                  batch_images, true_cfg_scale, seed_param, transformer_dropdown,
                  lora_dropdown, lora_weights, max_vram, size_edit2, reserve_edit2):
    ratio = image.size[0] / image.size[1]
    calculated_width = math.sqrt(1024*1024 * ratio)
    calculated_height = calculated_width / ratio
    calculated_width = round(calculated_width / 32) * 32
    calculated_height = round(calculated_height / 32) * 32
    image = image.convert("RGBA")
    image2 = image2.convert("RGBA")
    new_height = max(image.height, image2.height)
    # æŒ‰æ¯”ä¾‹è°ƒæ•´å®½åº¦
    image = image.resize((int(image.width * new_height / image.height), new_height))
    image2 = image2.resize((int(image2.width * new_height / image2.height), new_height))
    # åˆ›å»ºæ–°ç”»å¸ƒ
    result = Image.new("RGBA", (image.width + image2.width, new_height))
    # æ‹¼æ¥å›¾ç‰‡
    result.paste(image, (0, 0))
    result.paste(image2, (image.width, 0))
    white_bg = Image.new("RGB", result.size, (255, 255, 255))
    # ä½¿ç”¨alphaé€šé“ä½œä¸ºæ©ç è¿›è¡Œç²˜è´´
    white_bg.paste(result, mask=result.split()[3])
    # å¦‚æœéœ€è¦ä¿å­˜ä¸ºRGBæ¨¡å¼çš„å›¾åƒ
    image = white_bg.convert("RGB")
    yield from _generate_common(
        mode="edit2",
        image=image,
        prompt=prompt,
        negative_prompt=negative_prompt,
        width=calculated_width,
        height=calculated_height,
        num_inference_steps=num_inference_steps,
        batch_images=batch_images,
        true_cfg_scale=true_cfg_scale,
        seed_param=seed_param,
        transformer_dropdown=transformer_dropdown, 
        lora_dropdown=lora_dropdown,
        lora_weights=lora_weights,
        max_vram=max_vram,
        size_edit2=size_edit2, 
        reserve_edit2=reserve_edit2,
    )


def generate_editinp(image, prompt, negative_prompt, num_inference_steps,
                  strength, batch_images, true_cfg_scale, seed_param, transformer_dropdown,
                  lora_dropdown, lora_weights, max_vram):
    # å¤„ç†è’™ç‰ˆå›¾åƒ
    mask_image = image["layers"][0]
    mask_image = mask_image .convert("RGBA")
    data = np.array(mask_image)
    # ä¿®æ”¹è’™ç‰ˆé¢œè‰²ï¼ˆé»‘è‰²->ç™½è‰²ï¼Œé€æ˜->é»‘è‰²ï¼‰
    black_pixels = (data[:, :, 0] == 0) & (data[:, :, 1] == 0) & (data[:, :, 2] == 0)
    data[black_pixels, :3] = [255, 255, 255]
    transparent_pixels = (data[:, :, 3] == 0)
    data[transparent_pixels, :3] = [0, 0, 0]
    mask_image = Image.fromarray(data)
    # æå–èƒŒæ™¯å›¾åƒ
    background_image = load_image(image["background"])
    yield from _generate_common(
        mode="editinp",
        image=background_image,
        mask_image=mask_image,
        prompt=prompt,
        negative_prompt=negative_prompt,
        width=None,
        height=None,
        num_inference_steps=num_inference_steps,
        strength=strength, 
        batch_images=batch_images,
        true_cfg_scale=true_cfg_scale,
        seed_param=seed_param,
        transformer_dropdown=transformer_dropdown, 
        lora_dropdown=lora_dropdown,
        lora_weights=lora_weights,
        max_vram=max_vram
    )


def generate_editplus(image, prompt, negative_prompt, num_inference_steps,
                  batch_images, true_cfg_scale, seed_param, transformer_dropdown,
                  lora_dropdown, lora_weights, max_vram):
    image = image.convert("RGBA")
    white_bg = Image.new("RGB", image.size, (255, 255, 255))
    # ä½¿ç”¨alphaé€šé“ä½œä¸ºæ©ç è¿›è¡Œç²˜è´´
    white_bg.paste(image, mask=image.split()[3])
    # å¦‚æœéœ€è¦ä¿å­˜ä¸ºRGBæ¨¡å¼çš„å›¾åƒ
    image = white_bg.convert("RGB")
    yield from _generate_common(
        mode="editplus",
        image=image,
        prompt=prompt,
        negative_prompt=negative_prompt,
        width=None,
        height=None,
        num_inference_steps=num_inference_steps,
        batch_images=batch_images,
        true_cfg_scale=true_cfg_scale,
        seed_param=seed_param,
        transformer_dropdown=transformer_dropdown, 
        lora_dropdown=lora_dropdown,
        lora_weights=lora_weights,
        max_vram=max_vram
    )


def convert_lora(lora_in):
    global pipe
    results = []
    for lora_path in lora_in:
        # è¯»å–LoRAæ–‡ä»¶
        pipe = safetensors.torch.load_file(lora_path, device="cpu")
        # æ‰“å°æ‰€æœ‰keyå€¼
        """print("LoRAæ–‡ä»¶åŒ…å«çš„keyå€¼:")
        for key in pipe.keys():
            print(f"  {key}")"""
        converted_dict = {}
        for key, value in pipe.items():
            if 'lora' not in key:
                continue
            elif 'alpha' in key:
                continue
            fixed_key = key
            if fixed_key.endswith(".lora_A.default.weight"):
                fixed_key = fixed_key.replace(".lora_A.default.weight", ".lora.down.weight")
            elif fixed_key.endswith(".lora_B.default.weight"):
                fixed_key = fixed_key.replace(".lora_B.default.weight", ".lora.up.weight")
            elif fixed_key.endswith(".lora_A.weight"):
                fixed_key = fixed_key.replace(".lora_A.weight", ".lora.down.weight") 
            elif fixed_key.endswith(".lora_B.weight"):
                fixed_key = fixed_key.replace(".lora_B.weight", ".lora.up.weight")
            elif fixed_key.endswith(".lora_down.weight"):
                fixed_key = fixed_key.replace(".lora_down.weight", ".lora.down.weight")
            elif fixed_key.endswith(".lora_up.weight"):
                fixed_key = fixed_key.replace(".lora_up.weight", ".lora.up.weight")
    
            if fixed_key.startswith("diffusion_model.transformer_blocks."):
                fixed_key = fixed_key.replace("diffusion_model.transformer_blocks.", "transformer.transformer_blocks.")
            elif fixed_key.startswith("lora_unet_transformer_blocks_"):
                fixed_key = fixed_key.replace("lora_unet_transformer_blocks_", "transformer.transformer_blocks.")
                fixed_key = fixed_key.replace("_attn_", ".attn.")
                fixed_key = fixed_key.replace("_img_mlp_net_", ".img_mlp.net.")
                fixed_key = fixed_key.replace("_img_mod_", ".img_mod.")
                fixed_key = fixed_key.replace("_txt_mlp_net_", ".txt_mlp.net.")
                fixed_key = fixed_key.replace("_txt_mod_", ".txt_mod.")
                fixed_key = fixed_key.replace("0_", "0.")
                fixed_key = fixed_key.replace("_0", ".0")
            elif fixed_key.startswith("transformer_blocks."):
                fixed_key = "transformer." + fixed_key
            converted_dict[fixed_key] = value

        base_name = os.path.splitext(os.path.basename(lora_path))[0]
        output_filename = f"{base_name}_diffusers.safetensors"
        output_path = os.path.join("models/lora", output_filename)
        safetensors.torch.save_file(converted_dict, output_path)
        results.append(output_path)
        yield results, f"âœ… {output_filename}è½¬æ¢å®Œæˆ"
    pipe = None
    yield results, f"âœ… å…¨éƒ¨è½¬æ¢å®Œæˆï¼Œè¯·ç‚¹å‡»åˆ·æ–°æ¨¡å‹"


def load_image_info(image):
    img = Image.open(image)
    # è¯»å–PNGæ–‡æœ¬ä¿¡æ¯å—
    if img.format == 'PNG' and hasattr(img, 'text'):
        info = "".join([f"{k}: {v}" for k, v in img.text.items()])
    else:
        info = "è¯¥æ–‡ä»¶ä¸åŒ…å«PNGæ–‡æœ¬å…ƒæ•°æ®"
    return info 


def save_openai_config(transformer_dropdown, transformer_dropdown2, transformer_dropdown3, max_vram_tb, base_url_tb, api_key_tb, model_name_tb, temperature_tb, top_p_tb, max_tokens_tb, modelscope_api_key_tb):
    global max_vram, base_url, api_key, model_name, temperature, top_p, max_tokens, modelscope_api_key
    max_vram, base_url, api_key, model_name, temperature, top_p, max_tokens, modelscope_api_key = max_vram_tb, base_url_tb, api_key_tb, model_name_tb, temperature_tb, top_p_tb, max_tokens_tb, modelscope_api_key_tb
    config = {
        "TRANSFORMER_DROPDOWN": transformer_dropdown,
        "TRANSFORMER_DROPDOWN2": transformer_dropdown2,
        "TRANSFORMER_DROPDOWN3": transformer_dropdown3,
        "MAX_VRAM": max_vram_tb,
        "OPENAI_BASE_URL": base_url_tb,
        "OPENAI_API_KEY": api_key_tb,
        "MODEL_NAME": model_name_tb,
        "TEMPERATURE": temperature_tb,
        "TOP_P": top_p_tb,
        "MAX_TOKENS": max_tokens_tb,
        "MODELSCOPE_API_KEY": modelscope_api_key_tb,
    }
    with open(CONFIG_FILE, "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    return "âœ… é…ç½®å·²ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶"


with gr.Blocks(theme=gr.themes.Base()) as demo:
    gr.Markdown("""
            <div>
                <h2 style="font-size: 30px;text-align: center;">Qwen-Image</h2>
            </div>
            <div style="text-align: center;">
                åå­—é±¼
                <a href="https://space.bilibili.com/893892">ğŸŒbilibili</a> 
                |Qwen-Image
                <a href="https://github.com/QwenLM/Qwen-Image">ğŸŒgithub</a> 
            </div>
            <div style="text-align: center; font-weight: bold; color: red;">
                âš ï¸ è¯¥æ¼”ç¤ºä»…ä¾›å­¦æœ¯ç ”ç©¶å’Œä½“éªŒä½¿ç”¨ã€‚
            </div>
            """)
    with gr.Accordion("æ¨¡å‹è®¾ç½®", open=False):
        with gr.Column():
            with gr.Row():
                refresh_button = gr.Button("ğŸ”„ åˆ·æ–°æ¨¡å‹", scale=0)
            with gr.Row():
                transformer_dropdown = gr.Dropdown(label="QIæ¨¡å‹", info="å­˜æ”¾åŸºç¡€æ¨¡å‹åˆ°models/transformerï¼Œä»…æ”¯æŒmmgpè½¬åŒ–ç‰ˆæœ¬", choices=transformer_choices, value=transformer_)
                transformer_dropdown2 = gr.Dropdown(label="QIEæ¨¡å‹", info="å­˜æ”¾ç¼–è¾‘æ¨¡å‹åˆ°models/transformerï¼Œä»…æ”¯æŒmmgpè½¬åŒ–ç‰ˆæœ¬", choices=transformer_choices2, value=transformer_2)
                transformer_dropdown3 = gr.Dropdown(label="QIEPæ¨¡å‹", info="å­˜æ”¾ç¼–è¾‘æ¨¡å‹åˆ°models/transformerï¼Œä»…æ”¯æŒmmgpè½¬åŒ–ç‰ˆæœ¬", choices=transformer_choices3, value=transformer_3)
                lora_dropdown = gr.Dropdown(label="LoRAæ¨¡å‹", info="å­˜æ”¾LoRAæ¨¡å‹åˆ°models/lora", choices=lora_choices, multiselect=True)
                lora_weights = gr.Textbox(label="LoRAæƒé‡", info="Loraæƒé‡ï¼Œå¤šä¸ªæƒé‡è¯·ç”¨è‹±æ–‡é€—å·éš”å¼€ã€‚ä¾‹å¦‚ï¼š0.8,0.5,0.2", value="")
    with gr.TabItem("æ–‡ç”Ÿå›¾"):
        with gr.Row():
            with gr.Column():
                prompt = gr.Textbox(label="æç¤ºè¯", value="è¶…æ¸…ï¼Œ4Kï¼Œç”µå½±çº§æ„å›¾ï¼Œ")
                negative_prompt = gr.Textbox(label="è´Ÿé¢æç¤ºè¯", value="")
                with gr.Row():
                    generate_button = gr.Button("ğŸ¬ å¼€å§‹ç”Ÿæˆ", variant='primary', scale=5)
                    enhance_button = gr.Button("æç¤ºè¯å¢å¼º", scale=1)
                with gr.Accordion("å‚æ•°è®¾ç½®", open=True):
                    gr.Markdown("æ¨èåˆ†è¾¨ç‡ï¼š1328x1328ã€1664x928ã€1472x1104")
                    with gr.Row():
                        width = gr.Slider(label="å®½åº¦", minimum=256, maximum=2656, step=16, value=1328)
                        height = gr.Slider(label="é«˜åº¦", minimum=256, maximum=2656, step=16, value=1328)
                    exchange_button = gr.Button("ğŸ”„ äº¤æ¢å®½é«˜")
                    batch_images = gr.Slider(label="æ‰¹é‡ç”Ÿæˆ", minimum=1, maximum=100, step=1, value=1)
                    num_inference_steps = gr.Slider(label="é‡‡æ ·æ­¥æ•°ï¼ˆæ¨è8æ­¥ï¼‰", minimum=1, maximum=100, step=1, value=8)
                    true_cfg_scale = gr.Slider(label="true cfg scale", minimum=1, maximum=10, step=0.1, value=1.0)
                    seed_param = gr.Number(label="ç§å­ï¼Œè¯·è¾“å…¥è‡ªç„¶æ•°ï¼Œ-1ä¸ºéšæœº", value=-1)
            with gr.Column():
                info = gr.Textbox(label="æç¤ºä¿¡æ¯", interactive=False)
                image_output = gr.Gallery(label="ç”Ÿæˆç»“æœ", interactive=False)
                stop_button = gr.Button("ä¸­æ­¢ç”Ÿæˆ", variant="stop")
    with gr.TabItem("å›¾ç”Ÿå›¾"):
        with gr.Row():
            with gr.Column():
                image_i2i = gr.Image(label="è¾“å…¥å›¾ç‰‡", type="pil", height=400)
                prompt_i2i = gr.Textbox(label="æç¤ºè¯", value="è¶…æ¸…ï¼Œ4Kï¼Œç”µå½±çº§æ„å›¾ï¼Œ")
                negative_prompt_i2i = gr.Textbox(label="è´Ÿé¢æç¤ºè¯", value="")
                with gr.Row():
                    generate_button_i2i = gr.Button("ğŸ¬ å¼€å§‹ç”Ÿæˆ", variant='primary', scale=4)
                    enhance_button_i2i = gr.Button("æç¤ºè¯å¢å¼º", scale=1)
                    reverse_button_i2i = gr.Button("åæ¨æç¤ºè¯", scale=1)
                with gr.Accordion("å‚æ•°è®¾ç½®", open=True):
                    gr.Markdown("æ¨èåˆ†è¾¨ç‡ï¼š1328x1328ã€1664x928ã€1472x1104")
                    with gr.Row():
                        width_i2i = gr.Slider(label="å®½åº¦", minimum=256, maximum=2656, step=16, value=1328)
                        height_i2i = gr.Slider(label="é«˜åº¦", minimum=256, maximum=2656, step=16, value=1328)
                    with gr.Row():
                        exchange_button_i2i = gr.Button("ğŸ”„ äº¤æ¢å®½é«˜")
                        adjust_button_i2i = gr.Button("æ ¹æ®å›¾ç‰‡è°ƒæ•´å®½é«˜")
                    strength_i2i = gr.Slider(label="strength", minimum=0, maximum=1, step=0.01, value=0.5)
                    batch_images_i2i = gr.Slider(label="æ‰¹é‡ç”Ÿæˆ", minimum=1, maximum=100, step=1, value=1)
                    num_inference_steps_i2i = gr.Slider(label="é‡‡æ ·æ­¥æ•°ï¼ˆæ¨è8æ­¥ï¼‰", minimum=1, maximum=100, step=1, value=8)
                    true_cfg_scale_i2i = gr.Slider(label="true cfg scale", minimum=1, maximum=10, step=0.1, value=1.0)
                    seed_param_i2i = gr.Number(label="ç§å­ï¼Œè¯·è¾“å…¥è‡ªç„¶æ•°ï¼Œ-1ä¸ºéšæœº", value=-1)
            with gr.Column():
                info_i2i = gr.Textbox(label="æç¤ºä¿¡æ¯", interactive=False)
                image_output_i2i = gr.Gallery(label="ç”Ÿæˆç»“æœ", interactive=False)
                stop_button_i2i = gr.Button("ä¸­æ­¢ç”Ÿæˆ", variant="stop")
    with gr.TabItem("å±€éƒ¨é‡ç»˜"):
        with gr.Row():
            with gr.Column():
                image_inp = gr.ImageMask(label="è¾“å…¥è’™ç‰ˆ", type="pil", height=400)
                prompt_inp = gr.Textbox(label="æç¤ºè¯", value="è¶…æ¸…ï¼Œ4Kï¼Œç”µå½±çº§æ„å›¾ï¼Œ")
                negative_prompt_inp = gr.Textbox(label="è´Ÿé¢æç¤ºè¯", value="")
                with gr.Row():
                    generate_button_inp = gr.Button("ğŸ¬ å¼€å§‹ç”Ÿæˆ", variant='primary', scale=4)
                    enhance_button_inp = gr.Button("æç¤ºè¯å¢å¼º", scale=1)
                    reverse_button_inp = gr.Button("åæ¨æç¤ºè¯", scale=1)
                with gr.Accordion("å‚æ•°è®¾ç½®", open=True):
                    gr.Markdown("æ¨èåˆ†è¾¨ç‡ï¼š1328x1328ã€1664x928ã€1472x1104")
                    with gr.Row():
                        width_inp = gr.Slider(label="å®½åº¦", minimum=256, maximum=2656, step=16, value=1328)
                        height_inp = gr.Slider(label="é«˜åº¦", minimum=256, maximum=2656, step=16, value=1328)
                    with gr.Row():
                        exchange_button_inp = gr.Button("ğŸ”„ äº¤æ¢å®½é«˜")
                        adjust_button_inp = gr.Button("æ ¹æ®å›¾ç‰‡è°ƒæ•´å®½é«˜")
                    strength_inp = gr.Slider(label="strength", minimum=0, maximum=1, step=0.01, value=0.8)
                    batch_images_inp = gr.Slider(label="æ‰¹é‡ç”Ÿæˆ", minimum=1, maximum=100, step=1, value=1)
                    num_inference_steps_inp = gr.Slider(label="é‡‡æ ·æ­¥æ•°ï¼ˆæ¨è8æ­¥ï¼‰", minimum=1, maximum=100, step=1, value=8)
                    true_cfg_scale_inp = gr.Slider(label="true cfg scale", minimum=1, maximum=10, step=0.1, value=1.0)
                    seed_param_inp = gr.Number(label="ç§å­ï¼Œè¯·è¾“å…¥è‡ªç„¶æ•°ï¼Œ-1ä¸ºéšæœº", value=-1)
            with gr.Column():
                info_inp = gr.Textbox(label="æç¤ºä¿¡æ¯", interactive=False)
                image_output_inp = gr.Gallery(label="ç”Ÿæˆç»“æœ", interactive=False)
                stop_button_inp = gr.Button("ä¸­æ­¢ç”Ÿæˆ", variant="stop")
    with gr.TabItem("ControlNet"):
        with gr.Row():
            with gr.Column():
                image_con = gr.Image(label="è¾“å…¥å›¾ç‰‡", type="pil", height=400)
                prompt_con = gr.Textbox(label="æç¤ºè¯", value="è¶…æ¸…ï¼Œ4Kï¼Œç”µå½±çº§æ„å›¾ï¼Œ")
                negative_prompt_con = gr.Textbox(label="è´Ÿé¢æç¤ºè¯", value="")
                with gr.Row():
                    generate_button_con = gr.Button("ğŸ¬ å¼€å§‹ç”Ÿæˆ", variant='primary', scale=4)
                    enhance_button_con = gr.Button("æç¤ºè¯å¢å¼º", scale=1)
                    reverse_button_con = gr.Button("åæ¨æç¤ºè¯", scale=1)
                with gr.Accordion("å‚æ•°è®¾ç½®", open=True):
                    gr.Markdown("åˆ†è¾¨ç‡è¯·ç‚¹å‡»è°ƒæ•´å®½é«˜")
                    with gr.Row():
                        width_con = gr.Slider(label="å®½åº¦", minimum=256, maximum=2656, step=16, value=1328)
                        height_con = gr.Slider(label="é«˜åº¦", minimum=256, maximum=2656, step=16, value=1328)
                    with gr.Row():
                        exchange_button_con = gr.Button("ğŸ”„ äº¤æ¢å®½é«˜")
                        adjust_button_con = gr.Button("æ ¹æ®å›¾ç‰‡è°ƒæ•´å®½é«˜")
                    strength_con = gr.Slider(label="strengthï¼ˆæ¨è0.8~1ï¼‰", minimum=0, maximum=1, step=0.01, value=1.0)
                    batch_images_con = gr.Slider(label="æ‰¹é‡ç”Ÿæˆ", minimum=1, maximum=100, step=1, value=1)
                    num_inference_steps_con = gr.Slider(label="é‡‡æ ·æ­¥æ•°ï¼ˆæ¨è8æ­¥ï¼‰", minimum=1, maximum=100, step=1, value=8)
                    true_cfg_scale_con = gr.Slider(label="true cfg scale", minimum=1, maximum=10, step=0.1, value=1.0)
                    seed_param_con = gr.Number(label="ç§å­ï¼Œè¯·è¾“å…¥è‡ªç„¶æ•°ï¼Œ-1ä¸ºéšæœº", value=-1)
            with gr.Column():
                info_con = gr.Textbox(label="æç¤ºä¿¡æ¯", interactive=False)
                image_output_con = gr.Gallery(label="ç”Ÿæˆç»“æœ", interactive=False)
                stop_button_con = gr.Button("ä¸­æ­¢ç”Ÿæˆ", variant="stop")
    with gr.TabItem("å›¾åƒç¼–è¾‘"):
        with gr.Row():
            with gr.Column():
                image_edit = gr.Image(label="è¾“å…¥å›¾ç‰‡", type="pil", height=400, image_mode="RGBA")
                prompt_edit = gr.Textbox(label="æç¤ºè¯", value="ç»™å·¦è¾¹çš„å¥³å­©æ¢ä¸Šå³è¾¹çš„è¡£æœ")
                negative_prompt_edit = gr.Textbox(label="è´Ÿé¢æç¤ºè¯", value="")
                with gr.Row():
                    generate_button_edit = gr.Button("ğŸ¬ å¼€å§‹ç”Ÿæˆ", variant='primary', scale=4)
                    enhance_button_edit = gr.Button("æç¤ºè¯å¢å¼º", scale=1)
                    reverse_button_edit = gr.Button("åæ¨æç¤ºè¯", scale=1)
                with gr.Accordion("å‚æ•°è®¾ç½®", open=True):
                    gr.Markdown("åˆ†è¾¨ç‡è‡ªåŠ¨è®¡ç®—")
                    batch_images_edit = gr.Slider(label="æ‰¹é‡ç”Ÿæˆ", minimum=1, maximum=100, step=1, value=1)
                    num_inference_steps_edit = gr.Slider(label="é‡‡æ ·æ­¥æ•°ï¼ˆæ¨è8æ­¥ï¼‰", minimum=1, maximum=100, step=1, value=8)
                    true_cfg_scale_edit = gr.Slider(label="true cfg scale", minimum=1, maximum=10, step=0.1, value=1.0)
                    seed_param_edit = gr.Number(label="ç§å­ï¼Œè¯·è¾“å…¥è‡ªç„¶æ•°ï¼Œ-1ä¸ºéšæœº", value=0)
            with gr.Column():
                info_edit = gr.Textbox(label="æç¤ºä¿¡æ¯", interactive=False)
                image_output_edit = gr.Gallery(label="ç”Ÿæˆç»“æœ", interactive=False)
                stop_button_edit = gr.Button("ä¸­æ­¢ç”Ÿæˆ", variant="stop")
    with gr.TabItem("å›¾åƒç¼–è¾‘ï¼ˆåŒå›¾ï¼‰"):
        with gr.Row():
            with gr.Column():
                with gr.Row():
                    image_edit2 = gr.Image(label="è¾“å…¥ä¸»ä½“å›¾ç‰‡", type="pil", height=400, image_mode="RGBA")
                    image_edit3 = gr.Image(label="è¾“å…¥å‚è€ƒå›¾ç‰‡", type="pil", height=400, image_mode="RGBA")
                prompt_edit2 = gr.Textbox(label="æç¤ºè¯", value="ç»™å·¦è¾¹çš„å¥³å­©æ¢ä¸Šå³è¾¹çš„è¡£æœ")
                negative_prompt_edit2 = gr.Textbox(label="è´Ÿé¢æç¤ºè¯", value="")
                with gr.Row():
                    generate_button_edit2 = gr.Button("ğŸ¬ å¼€å§‹ç”Ÿæˆ", variant='primary', scale=4)
                    enhance_button_edit2 = gr.Button("æç¤ºè¯å¢å¼º", scale=1)
                    reverse_button_edit2 = gr.Button("åæ¨æç¤ºè¯", scale=1)
                with gr.Accordion("å‚æ•°è®¾ç½®", open=True):
                    with gr.Row():
                        size_edit2 = gr.Radio(label="åˆ†è¾¨ç‡", choices=["å°å°ºå¯¸", "å¤§å°ºå¯¸"], value="å°å°ºå¯¸")
                    with gr.Row():
                        reserve_edit2 = gr.Radio(label="ä¿ç•™éƒ¨åˆ†", choices=["ä¿ç•™å…¨éƒ¨", "ä¿ç•™ä¸»ä½“"], value="ä¿ç•™ä¸»ä½“")
                    batch_images_edit2 = gr.Slider(label="æ‰¹é‡ç”Ÿæˆ", minimum=1, maximum=100, step=1, value=1)
                    num_inference_steps_edit2 = gr.Slider(label="é‡‡æ ·æ­¥æ•°ï¼ˆæ¨è8æ­¥ï¼‰", minimum=1, maximum=100, step=1, value=8)
                    true_cfg_scale_edit2 = gr.Slider(label="true cfg scale", minimum=1, maximum=10, step=0.1, value=1.0)
                    seed_param_edit2 = gr.Number(label="ç§å­ï¼Œè¯·è¾“å…¥è‡ªç„¶æ•°ï¼Œ-1ä¸ºéšæœº", value=0)
            with gr.Column():
                info_edit2 = gr.Textbox(label="æç¤ºä¿¡æ¯", interactive=False)
                image_output_edit2 = gr.Gallery(label="ç”Ÿæˆç»“æœ", interactive=False)
                stop_button_edit2 = gr.Button("ä¸­æ­¢ç”Ÿæˆ", variant="stop")
    with gr.TabItem("å±€éƒ¨ç¼–è¾‘"):
        with gr.Row():
            with gr.Column():
                image_editinp = gr.ImageMask(label="è¾“å…¥è’™ç‰ˆ", type="pil", height=400)
                prompt_editinp = gr.Textbox(label="æç¤ºè¯", value="ç»™å·¦è¾¹çš„å¥³å­©æ¢ä¸Šå³è¾¹çš„è¡£æœ")
                negative_prompt_editinp = gr.Textbox(label="è´Ÿé¢æç¤ºè¯", value="")
                with gr.Row():
                    generate_button_editinp = gr.Button("ğŸ¬ å¼€å§‹ç”Ÿæˆ", variant='primary', scale=4)
                    enhance_button_editinp = gr.Button("æç¤ºè¯å¢å¼º", scale=1)
                    reverse_button_editinp = gr.Button("åæ¨æç¤ºè¯", scale=1)
                with gr.Accordion("å‚æ•°è®¾ç½®", open=True):
                    gr.Markdown("åˆ†è¾¨ç‡è‡ªåŠ¨è®¡ç®—")
                    strength_editinp = gr.Slider(label="strength", minimum=0, maximum=1, step=0.01, value=1.0)
                    batch_images_editinp = gr.Slider(label="æ‰¹é‡ç”Ÿæˆ", minimum=1, maximum=100, step=1, value=1)
                    num_inference_steps_editinp = gr.Slider(label="é‡‡æ ·æ­¥æ•°ï¼ˆæ¨è8æ­¥ï¼‰", minimum=1, maximum=100, step=1, value=8)
                    true_cfg_scale_editinp = gr.Slider(label="true cfg scale", minimum=1, maximum=10, step=0.1, value=1.0)
                    seed_param_editinp = gr.Number(label="ç§å­ï¼Œè¯·è¾“å…¥è‡ªç„¶æ•°ï¼Œ-1ä¸ºéšæœº", value=0)
            with gr.Column():
                info_editinp = gr.Textbox(label="æç¤ºä¿¡æ¯", interactive=False)
                image_output_editinp = gr.Gallery(label="ç”Ÿæˆç»“æœ", interactive=False)
                stop_button_editinp = gr.Button("ä¸­æ­¢ç”Ÿæˆ", variant="stop")
    with gr.TabItem("å•å›¾ç¼–è¾‘"):
        with gr.Row():
            with gr.Column():
                image_editplus = gr.Image(label="è¾“å…¥å›¾ç‰‡", type="pil", height=400, image_mode="RGBA")
                prompt_editplus = gr.Textbox(label="æç¤ºè¯", value="ç»™å·¦è¾¹çš„å¥³å­©æ¢ä¸Šå³è¾¹çš„è¡£æœ")
                negative_prompt_editplus = gr.Textbox(label="è´Ÿé¢æç¤ºè¯", value="")
                with gr.Row():
                    generate_button_editplus = gr.Button("ğŸ¬ å¼€å§‹ç”Ÿæˆ", variant='primary', scale=4)
                    enhance_button_editplus = gr.Button("æç¤ºè¯å¢å¼º", scale=1)
                    reverse_button_editplus = gr.Button("åæ¨æç¤ºè¯", scale=1)
                with gr.Accordion("å‚æ•°è®¾ç½®", open=True):
                    gr.Markdown("åˆ†è¾¨ç‡è‡ªåŠ¨è®¡ç®—")
                    batch_images_editplus = gr.Slider(label="æ‰¹é‡ç”Ÿæˆ", minimum=1, maximum=100, step=1, value=1)
                    num_inference_steps_editplus = gr.Slider(label="é‡‡æ ·æ­¥æ•°ï¼ˆæ¨è8æ­¥ï¼‰", minimum=1, maximum=100, step=1, value=8)
                    true_cfg_scale_editplus = gr.Slider(label="true cfg scale", minimum=1, maximum=10, step=0.1, value=1.0)
                    seed_param_editplus = gr.Number(label="ç§å­ï¼Œè¯·è¾“å…¥è‡ªç„¶æ•°ï¼Œ-1ä¸ºéšæœº", value=0)
            with gr.Column():
                info_editplus = gr.Textbox(label="æç¤ºä¿¡æ¯", interactive=False)
                image_output_editplus = gr.Gallery(label="ç”Ÿæˆç»“æœ", interactive=False)
                stop_button_editplus = gr.Button("ä¸­æ­¢ç”Ÿæˆ", variant="stop")
    with gr.TabItem("å¤šå›¾ç¼–è¾‘"):
        with gr.Row():
            with gr.Column():
                image_editplus = gr.Image(label="è¾“å…¥å›¾ç‰‡", type="pil", height=400, image_mode="RGBA")
                prompt_editplus = gr.Textbox(label="æç¤ºè¯", value="ç»™å·¦è¾¹çš„å¥³å­©æ¢ä¸Šå³è¾¹çš„è¡£æœ")
                negative_prompt_editplus = gr.Textbox(label="è´Ÿé¢æç¤ºè¯", value="")
                with gr.Row():
                    generate_button_editplus = gr.Button("ğŸ¬ å¼€å§‹ç”Ÿæˆ", variant='primary', scale=4)
                    enhance_button_editplus = gr.Button("æç¤ºè¯å¢å¼º", scale=1)
                    reverse_button_editplus = gr.Button("åæ¨æç¤ºè¯", scale=1)
                with gr.Accordion("å‚æ•°è®¾ç½®", open=True):
                    gr.Markdown("åˆ†è¾¨ç‡è‡ªåŠ¨è®¡ç®—")
                    batch_images_editplus = gr.Slider(label="æ‰¹é‡ç”Ÿæˆ", minimum=1, maximum=100, step=1, value=1)
                    num_inference_steps_editplus = gr.Slider(label="é‡‡æ ·æ­¥æ•°ï¼ˆæ¨è8æ­¥ï¼‰", minimum=1, maximum=100, step=1, value=8)
                    true_cfg_scale_editplus = gr.Slider(label="true cfg scale", minimum=1, maximum=10, step=0.1, value=1.0)
                    seed_param_editplus = gr.Number(label="ç§å­ï¼Œè¯·è¾“å…¥è‡ªç„¶æ•°ï¼Œ-1ä¸ºéšæœº", value=0)
            with gr.Column():
                info_editplus = gr.Textbox(label="æç¤ºä¿¡æ¯", interactive=False)
                image_output_editplus = gr.Gallery(label="ç”Ÿæˆç»“æœ", interactive=False)
                stop_button_editplus = gr.Button("ä¸­æ­¢ç”Ÿæˆ", variant="stop")
    with gr.TabItem("è½¬æ¢lora"):
        with gr.Row():
            with gr.Column():
                lora_in = gr.File(label="ä¸Šä¼ loraæ–‡ä»¶ï¼Œå¯å¤šé€‰", type="filepath", file_count="multiple")
                convert_button = gr.Button("å¼€å§‹è½¬æ¢", variant='primary')
            with gr.Column():
                info_lora = gr.Textbox(label="æç¤ºä¿¡æ¯", interactive=False)
                lora_out = gr.File(label="è¾“å‡ºæ–‡ä»¶", type="filepath", interactive=False)
                gr.Markdown("å¯è½¬åŒ–loraä¸ºdiffuserså¯ä»¥ä½¿ç”¨çš„loraï¼Œæ¯”å¦‚è½¬åŒ–[é­”æ­](https://modelscope.cn/aigc/modelTraining)è®­ç»ƒçš„loraã€‚")
    with gr.TabItem("å›¾ç‰‡ä¿¡æ¯"):
        with gr.Row():
            with gr.Column():
                image_info = gr.Image(label="è¾“å…¥å›¾ç‰‡", type="filepath")
            with gr.Column():
                info_info = gr.Textbox(label="å›¾ç‰‡ä¿¡æ¯", interactive=False)
                gr.Markdown("ä¸Šä¼ å›¾ç‰‡å³å¯æŸ¥çœ‹å›¾ç‰‡å†…ä¿å­˜çš„ä¿¡æ¯")
    with gr.TabItem("è®¾ç½®"):
        with gr.Row():
            with gr.Column():
                max_vram_tb = gr.Slider(label="æœ€å¤§æ˜¾å­˜ä½¿ç”¨æ¯”ä¾‹", minimum=0.1, maximum=1, step=0.01, value=max_vram)
                with gr.Accordion("å¤šæ¨¡æ€APIè®¾ç½®", open=True):
                    openai_base_url_tb = gr.Textbox(label="BASE URL", info="è¯·è¾“å…¥BASE URLï¼Œä¾‹å¦‚ï¼šhttps://open.bigmodel.cn/api/paas/v4", value=openai_base_url)
                    openai_api_key_tb = gr.Textbox(label="API KEY", info="è¯·è¾“å…¥API KEYï¼Œæš—æ–‡æ˜¾ç¤º", value=openai_api_key, type="password")
                    with gr.Row():
                        model_name_tb = gr.Textbox(label="MODEL NAME", info="è¯·è¾“å…¥æ¨¡å‹åç§°ï¼Œéœ€è¦æ”¯æŒå›¾ç‰‡è¾“å…¥çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œä¾‹å¦‚ï¼šGLM-4.5V", value=model_name)
                        temperature_tb = gr.Slider(label="temperature", info="é‡‡æ ·æ¸©åº¦ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºæ€§å’Œåˆ›é€ æ€§", minimum=0, maximum=1, step=0.1, value=temperature)
                    with gr.Row():
                        top_p_tb = gr.Slider(label="top_p", info="æ ¸é‡‡æ ·ï¼ˆnucleus samplingï¼‰å‚æ•°ï¼Œæ˜¯temperatureé‡‡æ ·çš„æ›¿ä»£æ–¹æ³•", minimum=0, maximum=1, step=0.1, value=top_p)
                        max_tokens_tb = gr.Slider(label="max_tokens", info="æ¨¡å‹è¾“å‡ºçš„æœ€å¤§ä»¤ç‰Œï¼ˆtokenï¼‰æ•°é‡é™åˆ¶", minimum=1024, maximum=65536, step=1024, value=max_tokens)
                with gr.Accordion("åœ¨çº¿ç”Ÿå›¾APIè®¾ç½®", open=True):
                    modelscope_api_key_tb = gr.Textbox(label="é­”æ­çš„API KEY", info="ä½¿ç”¨é­”æ­åœ¨çº¿æ¨¡å‹æ—¶éœ€è¦ï¼Œè·å–åœ°å€https://modelscope.cn/my/myaccesstoken", value=modelscope_api_key, type="password")
            with gr.Column():
                info_config = gr.Textbox(label="æç¤ºä¿¡æ¯", value="ä¿®æ”¹åè¯·ç‚¹å‡»ä¿å­˜è®¾ç½®ç”Ÿæ•ˆã€‚", interactive=False)
                save_button = gr.Button("ä¿å­˜è®¾ç½®", variant='primary')
                gr.Markdown("""å¤šæ¨¡æ€APIè®¾ç½®æ”¯æŒé€šç”¨ç±»OPENAIçš„APIï¼Œè¯·ä½¿ç”¨å¤šæ¨¡æ€æ¨¡å‹ï¼Œå¦‚ï¼šGLM-4.5Vã€GLM-4.1V-Thinking-Flashç­‰ï¼ˆéœ€è¦æ”¯æŒbase64ï¼‰ã€‚
                            å¯ç”³è¯·[æ™ºè°±API](https://www.bigmodel.cn/invite?icode=eKq1YoHsX6y4VhGIPJuOPGczbXFgPRGIalpycrEwJ28%3D)ã€‚
                            temperatureã€top_på’Œmax_tokensä¸‰ä¸ªå€¼ï¼Œé»˜è®¤æ˜¯GLM-4.5Vçš„æ¨èå€¼ã€‚
                            å¦‚æœæ›´æ¢æ¨¡å‹ï¼Œè¯·è‡ªè¡Œä¿®æ”¹ã€‚
                            ä¿å­˜è®¾ç½®é™¤äº†ä¿å­˜æ­¤é¡µé¢çš„è®¾ç½®ï¼Œè¿˜ä¼šä¿å­˜QIåŸºç¡€æ¨¡å‹å’ŒQIç¼–è¾‘æ¨¡å‹çš„è®¾ç½®ã€‚
                            """)
    # æ¨¡å‹è®¾ç½®
    refresh_button.click(
        fn=refresh_model,
        inputs=[],
        outputs=[transformer_dropdown, transformer_dropdown2, lora_dropdown]
    )
    # æ–‡ç”Ÿå›¾
    gr.on(
        triggers=[generate_button.click, prompt.submit, negative_prompt.submit],
        fn = generate_t2i,
        inputs = [
            prompt,
            negative_prompt,
            width,
            height,
            num_inference_steps,
            batch_images,
            true_cfg_scale, 
            seed_param,
            transformer_dropdown,
            lora_dropdown, 
            lora_weights,
            max_vram_tb,
        ],
        outputs = [image_output, info]
    )
    enhance_button.click(
        fn=enhance_prompt, 
        inputs=[prompt], 
        outputs=[prompt, info]
    )
    exchange_button.click(
        fn=exchange_width_height, 
        inputs=[width, height], 
        outputs=[width, height, info]
    )
    stop_button.click(
        fn=stop_generate, 
        inputs=[], 
        outputs=[info]
    )
    # å›¾ç”Ÿå›¾
    gr.on(
        triggers=[generate_button_i2i.click, prompt_i2i.submit, negative_prompt_i2i.submit],
        fn = generate_i2i,
        inputs = [
            image_i2i,
            prompt_i2i,
            negative_prompt_i2i,
            width_i2i,
            height_i2i,
            num_inference_steps_i2i,
            strength_i2i,
            batch_images_i2i,
            true_cfg_scale_i2i, 
            seed_param_i2i,
            transformer_dropdown,
            lora_dropdown, 
            lora_weights,
            max_vram_tb,
        ],
        outputs = [image_output_i2i, info_i2i]
    )
    enhance_button_i2i.click(
        fn=enhance_prompt, 
        inputs=[prompt_i2i], 
        outputs=[prompt_i2i, info_i2i]
    )
    reverse_button_i2i.click(
        fn=enhance_prompt, 
        inputs=[prompt_i2i, image_i2i], 
        outputs=[prompt_i2i, info_i2i]
    )
    exchange_button_i2i.click(
        fn=exchange_width_height, 
        inputs=[width_i2i, height_i2i], 
        outputs=[width_i2i, height_i2i, info_i2i]
    )
    adjust_button_i2i.click(
        fn=adjust_width_height, 
        inputs=[image_i2i], 
        outputs=[width_i2i, height_i2i, info_i2i]
    )
    stop_button_i2i.click(
        fn=stop_generate, 
        inputs=[], 
        outputs=[info_i2i]
    )
    # å±€éƒ¨é‡ç»˜
    gr.on(
        triggers=[generate_button_inp.click, prompt_inp.submit, negative_prompt_inp.submit],
        fn = generate_inp,
        inputs = [
            image_inp,
            prompt_inp,
            negative_prompt_inp,
            width_inp,
            height_inp,
            num_inference_steps_inp,
            strength_inp,
            batch_images_inp,
            true_cfg_scale_inp, 
            seed_param_inp,
            transformer_dropdown,
            lora_dropdown, 
            lora_weights,
            max_vram_tb,
        ],
        outputs = [image_output_inp, info_inp]
    )
    enhance_button_inp.click(
        fn=enhance_prompt, 
        inputs=[prompt_inp], 
        outputs=[prompt_inp, info_inp]
    )
    reverse_button_inp.click(
        fn=enhance_prompt, 
        inputs=[prompt_inp, image_inp], 
        outputs=[prompt_inp, info_inp]
    )
    exchange_button_inp.click(
        fn=exchange_width_height, 
        inputs=[width_inp, height_inp], 
        outputs=[width_inp, height_inp, info_inp]
    )
    adjust_button_inp.click(
        fn=adjust_width_height, 
        inputs=[image_inp], 
        outputs=[width_inp, height_inp, info_inp]
    )
    stop_button_inp.click(
        fn=stop_generate, 
        inputs=[], 
        outputs=[info_inp]
    )
    # ControlNet
    gr.on(
        triggers=[generate_button_con.click, prompt_con.submit, negative_prompt_con.submit],
        fn = generate_con,
        inputs = [
            image_con,
            prompt_con,
            negative_prompt_con,
            width_con,
            height_con,
            num_inference_steps_con,
            strength_con,
            batch_images_con,
            true_cfg_scale_con, 
            seed_param_con,
            transformer_dropdown,
            lora_dropdown, 
            lora_weights,
            max_vram_tb,
        ],
        outputs = [image_output_con, info_con]
    )
    enhance_button_con.click(
        fn=enhance_prompt, 
        inputs=[prompt_con], 
        outputs=[prompt_con, info_con]
    )
    reverse_button_con.click(
        fn=enhance_prompt, 
        inputs=[prompt_con, image_con], 
        outputs=[prompt_con, info_con]
    )
    exchange_button_con.click(
        fn=exchange_width_height, 
        inputs=[width_con, height_con], 
        outputs=[width_con, height_con, info_con]
    )
    adjust_button_con.click(
        fn=adjust_width_height, 
        inputs=[image_con], 
        outputs=[width_con, height_con, info_con]
    )
    stop_button_con.click(
        fn=stop_generate, 
        inputs=[], 
        outputs=[info_con]
    )
    # å›¾åƒç¼–è¾‘
    gr.on(
        triggers=[generate_button_edit.click, prompt_edit.submit, negative_prompt_edit.submit],
        fn = generate_edit,
        inputs = [
            image_edit,
            prompt_edit,
            negative_prompt_edit,
            num_inference_steps_edit,
            batch_images_edit,
            true_cfg_scale_edit, 
            seed_param_edit,
            transformer_dropdown2,
            lora_dropdown, 
            lora_weights,
            max_vram_tb,
        ],
        outputs = [image_output_edit, info_edit]
    )
    enhance_button_edit.click(
        fn=enhance_prompt_edit, 
        inputs=[prompt_edit, image_edit], 
        outputs=[prompt_edit, info_edit]
    )
    reverse_button_edit.click(
        fn=enhance_prompt, 
        inputs=[prompt_edit, image_edit], 
        outputs=[prompt_edit, info_edit]
    )
    stop_button_edit.click(
        fn=stop_generate, 
        inputs=[], 
        outputs=[info_edit]
    )
    # å›¾åƒç¼–è¾‘ï¼ˆåŒå›¾ï¼‰
    gr.on(
        triggers=[generate_button_edit2.click, prompt_edit2.submit, negative_prompt_edit2.submit],
        fn = generate_edit2,
        inputs = [
            image_edit2,
            image_edit3,
            prompt_edit2,
            negative_prompt_edit2,
            num_inference_steps_edit2,
            batch_images_edit2,
            true_cfg_scale_edit2, 
            seed_param_edit2,
            transformer_dropdown2,
            lora_dropdown, 
            lora_weights,
            max_vram_tb,
            size_edit2,
            reserve_edit2,
        ],
        outputs = [image_output_edit2, info_edit2]
    )
    enhance_button_edit2.click(
        fn=enhance_prompt_edit2, 
        inputs=[prompt_edit2, image_edit2, image_edit3], 
        outputs=[prompt_edit2, info_edit2]
    )
    reverse_button_edit2.click(
        fn=enhance_prompt, 
        inputs=[prompt_edit2, image_edit2], 
        outputs=[prompt_edit2, info_edit2]
    )
    stop_button_edit2.click(
        fn=stop_generate, 
        inputs=[], 
        outputs=[info_edit2]
    )
    # å±€éƒ¨ç¼–è¾‘
    gr.on(
        triggers=[generate_button_editinp.click, prompt_editinp.submit, negative_prompt_editinp.submit],
        fn = generate_editinp,
        inputs = [
            image_editinp,
            prompt_editinp,
            negative_prompt_editinp,
            num_inference_steps_editinp,
            strength_editinp,
            batch_images_editinp,
            true_cfg_scale_editinp, 
            seed_param_editinp,
            transformer_dropdown2,
            lora_dropdown, 
            lora_weights,
            max_vram_tb,
        ],
        outputs = [image_output_editinp, info_editinp]
    )
    enhance_button_editinp.click(
        fn=enhance_prompt_edit, 
        inputs=[prompt_editinp, image_editinp], 
        outputs=[prompt_editinp, info_editinp]
    )
    reverse_button_editinp.click(
        fn=enhance_prompt, 
        inputs=[prompt_editinp, image_editinp], 
        outputs=[prompt_editinp, info_editinp]
    )
    stop_button_editinp.click(
        fn=stop_generate, 
        inputs=[], 
        outputs=[info_editinp]
    )
    # å›¾åƒç¼–è¾‘PLUS
    gr.on(
        triggers=[generate_button_editplus.click, prompt_editplus.submit, negative_prompt_editplus.submit],
        fn = generate_editplus,
        inputs = [
            image_editplus,
            prompt_editplus,
            negative_prompt_editplus,
            num_inference_steps_editplus,
            batch_images_editplus,
            true_cfg_scale_editplus, 
            seed_param_editplus,
            transformer_dropdown3,
            lora_dropdown, 
            lora_weights,
            max_vram_tb,
        ],
        outputs = [image_output_editplus, info_editplus]
    )
    enhance_button_editplus.click(
        fn=enhance_prompt_edit, 
        inputs=[prompt_editplus, image_editplus], 
        outputs=[prompt_editplus, info_editplus]
    )
    reverse_button_editplus.click(
        fn=enhance_prompt, 
        inputs=[prompt_editplus, image_editplus], 
        outputs=[prompt_editplus, info_editplus]
    )
    stop_button_editplus.click(
        fn=stop_generate, 
        inputs=[], 
        outputs=[info_editplus]
    )
    # è½¬æ¢lora
    convert_button.click(
        fn=convert_lora,
        inputs = [lora_in],
        outputs = [lora_out, info_lora]
    )
    # å›¾ç‰‡ä¿¡æ¯
    image_info.upload(
        fn=load_image_info,
        inputs=[image_info],
        outputs=[info_info]
    )
    # è®¾ç½®
    save_button.click(
        fn=save_openai_config,
        inputs=[transformer_dropdown, transformer_dropdown2, transformer_dropdown3, max_vram_tb, openai_base_url_tb, openai_api_key_tb, model_name_tb, temperature_tb, top_p_tb, max_tokens_tb, modelscope_api_key_tb],
        outputs=[info_config],
    )


if __name__ == "__main__": 
    demo.launch(
        server_name=args.server_name, 
        server_port=args.server_port,
        share=args.share, 
        mcp_server=args.mcp_server,
        inbrowser=True,
    )