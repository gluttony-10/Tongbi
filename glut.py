import gradio as gr
import numpy as np
import torch
import random

from diffusers import FluxKontextPipeline, GGUFQuantizationConfig, FluxTransformer2DModel, FluxPipeline
from transformers import T5EncoderModel
import psutil
import argparse
from openai import OpenAI
import os

from nunchaku import NunchakuFluxTransformer2dModel
from nunchaku.caching.diffusers_adapters import apply_cache_on_pipe
from nunchaku.utils import get_precision

import datetime
from PIL import Image

from image_gen_aux import DepthPreprocessor

parser = argparse.ArgumentParser() 
parser.add_argument("--flux_text_encoder_2", type=str, default="t5-v1_1-xxl-encoder-Q8_0.gguf", help="fluxçš„text_encoder_2æ¨¡å‹å¯¹åº”çš„ggufæ–‡ä»¶")
parser.add_argument("--server_name", type=str, default="127.0.0.1", help="IPåœ°å€ï¼Œå±€åŸŸç½‘è®¿é—®æ”¹ä¸º0.0.0.0")
parser.add_argument("--server_port", type=int, default=7890, help="ä½¿ç”¨ç«¯å£")
parser.add_argument("--share", action="store_true", help="æ˜¯å¦å¯ç”¨gradioå…±äº«")
parser.add_argument("--mcp_server", action="store_true", help="æ˜¯å¦å¯ç”¨mcpæœåŠ¡")
parser.add_argument("--afbc", type=float, default=0, help="ç¬¬ä¸€å—ç¼“å­˜ï¼Œç”¨æ¥åŠ é€Ÿç”Ÿæˆï¼Œ0ä¸ºå…³é—­ï¼Œ0.12ä¸ºå€é€Ÿ")
parser.add_argument("--vram", type=str, default="low", choices=['low', 'mid', 'high'], help="è°ƒæ•´æ˜¾å­˜å ç”¨ï¼Œhighå ç”¨22Gï¼Œmidå ç”¨10Gï¼Œlowå ç”¨3.5G")
args = parser.parse_args()

MODEL_NAME = os.environ.get("MODEL_NAME", "glm-4-flash-250414")

print(" å¯åŠ¨ä¸­ï¼Œè¯·è€å¿ƒç­‰å¾… bilibili@åå­—é±¼ https://space.bilibili.com/893892")
print(f'\033[32mPytorchç‰ˆæœ¬ï¼š{torch.__version__}\033[0m')
if torch.cuda.is_available():
    device = "cuda" 
    print(f'\033[32mæ˜¾å¡å‹å·ï¼š{torch.cuda.get_device_name()}\033[0m')
    total_vram_in_gb = torch.cuda.get_device_properties(0).total_memory / 1073741824
    print(f'\033[32mæ˜¾å­˜å¤§å°ï¼š{total_vram_in_gb:.2f}GB\033[0m')
    mem = psutil.virtual_memory()
    print(f'\033[32må†…å­˜å¤§å°ï¼š{mem.total/1073741824:.2f}GB\033[0m')
    if torch.cuda.get_device_capability()[0] >= 8:
        print(f'\033[32mæ”¯æŒBF16\033[0m')
        dtype = torch.bfloat16
    else:
        print(f'\033[32mä¸æ”¯æŒBF16ï¼Œä»…æ”¯æŒFP16\033[0m')
        dtype = torch.float16
else:
    print(f'\033[32mCUDAä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥\033[0m')
    device = "cpu"
precision = get_precision()
print(f'\033[32mä½¿ç”¨{precision}æ¨¡å‹\033[0m')

MAX_SEED = np.iinfo(np.int32).max

transformer = NunchakuFluxTransformer2dModel.from_pretrained(
    f"mit-han-lab/nunchaku-flux.1-dev/svdq-{precision}_r32-flux.1-dev.safetensors",
    offload=True if args.vram == "low" else False,
    torch_dtype=dtype
)
transformer.set_attention_impl("nunchaku-fp16")
pipe = FluxPipeline.from_pretrained(
    "models/FLUX.1-dev", 
    transformer=transformer, 
    torch_dtype=dtype
)
if args.afbc!=0:
    apply_cache_on_pipe(
        pipe,
        residual_diff_threshold=args.afbc,
    )
if args.vram == "high":
    pipe.to(device)
else:
    pipe.enable_sequential_cpu_offload()

PREFERED_KONTEXT_RESOLUTIONS = [
    (672, 1568),
    (688, 1504),
    (720, 1456),
    (752, 1392),
    (800, 1328),
    (832, 1248),
    (880, 1184),
    (944, 1104),
    (1024, 1024),
    (1104, 944),
    (1184, 880),
    (1248, 832),
    (1328, 800),
    (1392, 752),
    (1456, 720),
    (1504, 688),
    (1568, 672),
]

# æ·»åŠ æœ¬åœ° LoRA æ‰«æåŠŸèƒ½
LORA_DIR = "models/lora"
os.makedirs(LORA_DIR, exist_ok=True)
flux_loras_raw = []
current_lora = None 

if os.path.exists(LORA_DIR) and os.path.isdir(LORA_DIR):
    for file in os.listdir(LORA_DIR):
        if file.endswith(".safetensors"):
            file_path = os.path.join(LORA_DIR, file)
            base_name = os.path.splitext(file)[0]
            
            # æŸ¥æ‰¾åŒåå›¾ç‰‡æ–‡ä»¶
            image_path = None
            for ext in ['.jpg', '.jpeg', '.png']:
                test_path = os.path.join(LORA_DIR, base_name + ext)
                if os.path.isfile(test_path):
                    image_path = test_path
                    break
            
            # æŸ¥æ‰¾å¹¶è¯»å–åŒåtxtæ–‡ä»¶ä½œä¸ºtrigger_word
            trigger_word = ""
            txt_path = os.path.join(LORA_DIR, base_name + ".txt")
            if os.path.isfile(txt_path):
                try:
                    with open(txt_path, 'r', encoding='utf-8') as f:
                        trigger_word = f.read().strip()
                except Exception as e:
                    print(f"è¯»å– trigger_word æ–‡ä»¶å¤±è´¥: {e}")
            
            flux_loras_raw.append({
                "image": image_path or "models/lora/Gluttony10.png",  # è®¾ç½®é»˜è®¤å›¾ç‰‡è·¯å¾„
                "title": base_name,
                "path": file_path,  # å­˜å‚¨æœ¬åœ°æ–‡ä»¶è·¯å¾„
                "trigger_word": trigger_word,  # ä½¿ç”¨ä»txtè¯»å–çš„å†…å®¹
            })
    print(f"ä»æœ¬åœ°ç›®å½•åŠ è½½äº† {len(flux_loras_raw)} ä¸ª LoRA")
else:
    print(f"è­¦å‘Šï¼šLoRA ç›®å½• {LORA_DIR} ä¸å­˜åœ¨")


def load_lora_weights(lora_path):
    """ç›´æ¥åŠ è½½æœ¬åœ° LoRA æ–‡ä»¶"""
    if os.path.isfile(lora_path):
        return lora_path
    print(f"LoRA æ–‡ä»¶ä¸å­˜åœ¨: {lora_path}")
    return None


def update_selection(selected_state: gr.SelectData, flux_loras):
    if selected_state.index >= len(flux_loras):
        return "### æœªé€‰æ‹© LoRA", gr.update(), None
    
    lora_item = flux_loras[selected_state.index]
    updated_text = f"### å·²é€‰æ‹©: {lora_item['title']}"
    
    return updated_text, lora_item["trigger_word"], selected_state.index, gr.update(visible=True)


def remove_custom_lora():
    """Remove custom LoRA"""
    return gr.update(visible=False), None, None, gr.Gallery(selected_index=None), "### Click on a LoRA in the gallery to select it"


def classify_gallery(flux_loras):
    """Sort gallery by likes"""
    sorted_gallery = sorted(flux_loras, key=lambda x: x.get("likes", 0), reverse=True)
    return [(item["image"], item["title"]) for item in sorted_gallery], sorted_gallery


def convert_prompt(prompt: str, retry_times: int = 3) -> str:
    if not os.environ.get("OPENAI_API_KEY"):
        return prompt, "æœªè®¾ç½® OPENAI_API_KEY"
    client = OpenAI()
    text = prompt.strip()

    for i in range(retry_times):
        response = client.chat.completions.create(
            messages=[{"role": "system", "content": """
                ç¿»è¯‘æˆè‹±æ–‡
            """
            },
            {
                "role": "user",
                "content": text.strip()
            },
            ],
            model=MODEL_NAME,
            temperature=0.95,
            top_p=0.7,
            stream=False,
            max_tokens=1024,
        )
        if response.choices:
            return response.choices[0].message.content.replace('"', ''), "æç¤ºè¯å¢å¼ºå®Œæ¯•"
    return prompt.replace('"', ''), "æç¤ºè¯å¢å¼ºå®Œæ¯•"


def infer_with_lora(input_images, prompt, selected_index, custom_lora, seed=42, randomize_seed=False, steps=20, guidance_scale=2.5, lora_scale=1.0, flux_loras=None):
    global current_lora, pipe
    print(f"Received {len(input_images) if input_images else 0} images")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = f"outputs/{timestamp}"
    os.makedirs(output_dir, exist_ok=True)
    
    # ç¡®å®šåŸºç¡€ç§å­
    if randomize_seed:
        base_seed = random.randint(0, MAX_SEED)
    else:
        base_seed = seed

    all_results = []  # å­˜å‚¨æ‰€æœ‰ç”Ÿæˆç»“æœ
    
    lora_to_use = custom_lora or (
        flux_loras[selected_index] 
        if selected_index is not None and flux_loras and selected_index < len(flux_loras) 
        else None
    )
    
    if lora_to_use and lora_to_use != current_lora:
        try:
            # ç›´æ¥ä½¿ç”¨æœ¬åœ°æ–‡ä»¶è·¯å¾„
            lora_path = lora_to_use["path"]
            if os.path.isfile(lora_path):
                pipe.transformer.update_lora_params(lora_path)
                pipe.transformer.set_lora_strength(lora_scale)
                print(f"Loaded LoRA: {lora_path} with scale {lora_scale}")
                current_lora = lora_to_use
        except Exception as e:
            print(f"Error loading LoRA: {e}")
    elif not lora_to_use:
        pipe.transformer.set_lora_strength(0)
        
    # é€æ­¥ç”Ÿæˆå¹¶è¾“å‡ºæ¯å¼ å›¾ç‰‡
    if input_images:
        # å¤„ç†æ¯å¼ è¾“å…¥å›¾åƒ
        for index, img_path in enumerate(input_images):
            # ä»æ–‡ä»¶è·¯å¾„åŠ è½½å›¾åƒ
            try:
                # æå–å…ƒç»„ä¸­çš„å®é™…æ–‡ä»¶è·¯å¾„
                actual_path = img_path[0] if isinstance(img_path, tuple) else img_path
                img = Image.open(actual_path).convert("RGB")
            except Exception as e:
                print(f"æ— æ³•åŠ è½½å›¾åƒ {img_path}: {e}")
                continue
                
            # ä¸ºæ¯å¼ å›¾ç¡®å®šç‹¬ç«‹ç§å­
            if randomize_seed:
                current_seed = base_seed + index
            else:
                current_seed = seed + index
                
            width, height = img.size
            aspect_ratio = width / height
            
            # é€‰æ‹©æœ€æ¥è¿‘çš„åˆ†è¾¨ç‡
            _, target_width, target_height = min(
                (abs(aspect_ratio - w / h), w, h) 
                for w, h in PREFERED_KONTEXT_RESOLUTIONS
            )
            
            # ç”Ÿæˆå•å¼ å›¾åƒ
            generator = torch.Generator().manual_seed(current_seed)
            result_img = pipe(
                image=img,
                prompt=prompt,
                guidance_scale=guidance_scale,
                num_inference_steps=steps,
                height=target_height,
                width=target_width,
                generator=generator,
            ).images[0]
            
            # ä¿å­˜å›¾ç‰‡åˆ°è¾“å‡ºç›®å½•
            output_path = os.path.join(output_dir, f"result_{index+1}.png")
            result_img.save(output_path)
            
            # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
            all_results.append(result_img)
            
            # å®æ—¶è¾“å‡ºå½“å‰ç»“æœ
            size_info = f"å›¾{index+1}: {target_width}Ã—{target_height} (å·²ä¿å­˜åˆ° {output_path})"
            yield all_results, current_seed, f"æ­£åœ¨ç”Ÿæˆ... ({index+1}/{len(input_images)}) {size_info}"
        
        # æ‰€æœ‰å›¾ç‰‡ç”Ÿæˆå®Œæˆåè¾“å‡ºæœ€ç»ˆç»“æœ
        if not all_results:
            size_info = "é”™è¯¯: æ²¡æœ‰ç”Ÿæˆä»»ä½•æœ‰æ•ˆå›¾ç‰‡"
            yield [], base_seed, size_info
        else:
            size_info = f"å®Œæˆ! å…±ç”Ÿæˆ {len(all_results)} å¼ å›¾ç‰‡ï¼Œå·²ä¿å­˜åˆ° {output_dir}"
            yield all_results, base_seed, size_info
    else:
        # æ²¡æœ‰ä¸Šä¼ å›¾åƒæ—¶ç”Ÿæˆå•å¼ å›¾
        generator = torch.Generator().manual_seed(base_seed)
        result_img = pipe(
            prompt=prompt,
            guidance_scale=guidance_scale,
            num_inference_steps=steps,
            generator=generator,
        ).images[0]
        
        # ä¿å­˜å•å¼ å›¾ç‰‡
        output_path = os.path.join(output_dir, "result.png")
        result_img.save(output_path)
        
        yield [result_img], base_seed, f"ç”Ÿæˆå°ºå¯¸: 1024Ã—1024 (å·²ä¿å­˜åˆ° {output_path})"


with gr.Blocks(theme=gr.themes.Base()) as demo:
    with gr.Column():
        gr.Markdown("""
                <div>
                    <h2 style="font-size: 30px;text-align: center;">é€šè‡‚ Tongbi</h2>
                </div>
                <div style="text-align: center;">
                    åå­—é±¼
                    <a href="https://space.bilibili.com/893892">ğŸŒbilibili</a> 
                    |FLUX
                    <a href="https://github.com/black-forest-labs/flux">ğŸŒgithub</a> 
                </div>
                <div style="text-align: center; font-weight: bold; color: red;">
                    âš ï¸ è¯¥æ¼”ç¤ºä»…ä¾›å­¦æœ¯ç ”ç©¶å’Œä½“éªŒä½¿ç”¨ã€‚
                </div>
                """)
        with gr.Tabs():
            with gr.TabItem("FLUX.1-dev"):
                with gr.Row():
                    with gr.Column():
                        gallery = gr.Gallery(
                            label="é€‰æ‹©LoRA",
                            allow_preview=False,
                            columns=3,
                            show_share_button=False,
                        )
                        lora_prompt = gr.Textbox(
                            label="LoRAæç¤ºè¯ç¤ºä¾‹",
                            value="",
                            interactive=False, 
                        )
                        lora_scale = gr.Slider(
                            label="LoRAå¼ºåº¦",
                            minimum=0,
                            maximum=2,
                            step=0.1,
                            value=1.0,
                        )
                        custom_model_button = gr.Button("å–æ¶ˆé€‰æ‹©çš„LoRA", visible=False)
                    with gr.Column():
                        prompt = gr.Text(
                            label='æç¤ºè¯ï¼ˆä¾‹å¦‚ï¼š"Change the car color to red"ï¼Œ"Transform to Bauhaus art style"ï¼‰',
                            placeholder="è¾“å…¥æç¤ºè¯ï¼Œè‡ªç„¶è¯­è¨€ï¼Œè‹±æ–‡æè¿°",
                        )
                        with gr.Row():
                            randomize_seed = gr.Checkbox(label="éšæœºç§å­", value=True, scale=1)
                            seed = gr.Slider(
                                label="ç§å­",
                                minimum=0,
                                maximum=MAX_SEED,
                                step=1,
                                value=0,
                                scale=2,
                            )
                        guidance_scale = gr.Slider(
                            label="æŒ‡å¯¼é‡è¡¨",
                            minimum=1,
                            maximum=10,
                            step=0.1,
                            value=2.5,
                        )       
                        steps = gr.Slider(
                            label="æ¨ç†æ­¥æ•°",
                            minimum=1,
                            maximum=50,
                            value=20,
                            step=1
                        )

                        with gr.Row():
                            run_button = gr.Button("å¼€å§‹ç”Ÿæˆ", variant="primary")
                            enhance_button = gr.Button("æç¤ºè¯å¢å¼º", variant="secondary")
                        input_image = gr.Gallery(label="ä¸Šä¼ å›¾åƒè¿›è¡Œç¼–è¾‘ï¼ˆå¯å¤šé€‰ï¼‰", type="filepath", file_types=["image"])
                    with gr.Column():
                        state = gr.Textbox(value="è¯·ä¿®æ”¹å‚æ•°ã€å¡«å†™æç¤ºè¯å¹¶ä¸Šä¼ å›¾åƒ", interactive=False, show_label=False)
                        result = gr.Gallery(label="ç”Ÿæˆç»“æœ", interactive=False)
                        prompt_title = gr.Markdown(
                            value="### Click on a LoRA in the gallery to select it",
                            visible=True,
                        )
                        gr_flux_loras = gr.State(value=flux_loras_raw)
                        selected_state = gr.State(value=None)
                        custom_loaded_lora = gr.State(value=None)
            with gr.TabItem("æ–°æ ‡ç­¾é¡µ"):
                gr.Markdown("## æ–°æ ‡ç­¾é¡µå†…å®¹")
                
    custom_model_button.click(
        fn=remove_custom_lora,
        outputs=[custom_model_button, custom_loaded_lora, selected_state, gallery, prompt_title]
    )
    gallery.select(
        fn=update_selection,
        inputs=[gr_flux_loras],
        outputs=[prompt_title, lora_prompt, selected_state, custom_model_button],
        show_progress=False
    )
    gr.on(
        triggers=[run_button.click, prompt.submit],
        fn = infer_with_lora,
        inputs = [input_image, prompt, selected_state, custom_loaded_lora, seed, randomize_seed, steps, guidance_scale, lora_scale, gr_flux_loras],
        outputs = [result, seed, state]
    )
    enhance_button.click(
        fn = convert_prompt,
        inputs = [prompt],
        outputs = [prompt, state]
    )
    demo.load(
        fn=classify_gallery, 
        inputs=[gr_flux_loras], 
        outputs=[gallery, gr_flux_loras]
    )

if __name__ == "__main__": 
    demo.launch(
        server_name=args.server_name, 
        server_port=args.server_port,
        share=args.share, 
        mcp_server=args.mcp_server,
        inbrowser=True,
    )